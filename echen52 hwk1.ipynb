{"cells":[{"cell_type":"markdown","metadata":{"id":"AL8T2iZUN2Qj"},"source":["# CS 447 Homework 1 $-$ Tokenization & Language Models\n","In this homework we will study **tokenization** and **language modeling**. In particular, you will build a WordPiece tokenizer and some n-gram language models on a corpus of Wikipedia articles.\n","\n","This notebook is designed to be run in Google Colab. Navigate to <TT>colab.research.google.com</TT> and upload this notebook. Then follow the instructions in the notebook to do the assignent.\n","\n","To run the notebook, you will need to connect to a Runtime. For this homework, all you need is a CPU. You can change the runtime by going to <TT>Runtime > Change runtime type</TT> and selecting <TT>None</TT> in the <TT>Hardware Accelerator</TT> field. We encourage you to disconnect from the runtime when you are not using it, as Google Colab can limit your resources if you overuse them. You can read more about Google Colab at https://research.google.com/colaboratory/faq.html.\n","\n","We have imported all the libraries you need to do this homework. <b>You should not import any extra libraries.</b> If you do, the autograder will fail to run your code.\n","\n","**Reminder: The course policy of this class prohibits the use of AI tools to help with coding, such as Chat-GPT or other code completion software. Further, you may not look at or copy from code repositories online; and while you may discuss the homework with your classmates, you may *not* share code with each other.** You are of course welcome to look at general Python materials (such as Python or Pytorch tutorials and documentation)."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3033,"status":"ok","timestamp":1740095486339,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"6zXwUyLZogP_","outputId":"3dea8f08-29aa-4586-919f-edad2aede02a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"]}],"source":["!pip install datasets"]},{"cell_type":"markdown","metadata":{"id":"FCTcbDLsh-nS"},"source":["# Part 1: WordPiece Tokenization [28 points]\n","\n","Here you will implement WordPiece tokenization (you can read more about WordPiece in this [paper](https://arxiv.org/pdf/1609.08144))."]},{"cell_type":"markdown","metadata":{"id":"UU08lg2JoX9A"},"source":["## Download & preprocess the data\n","\n","We use a subset of the FineWeb BBC News dataset. You can see the Huggingface dataset card [here](https://huggingface.co/datasets/permutans/fineweb-bbc-news) (including a discussion of potential limitations or biases), and the corresponding research paper [here](https://arxiv.org/pdf/2406.17557)."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1740095488755,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"LwpobI7Moeo-","outputId":"27cd9221-e006-497f-e57d-d3448082d545"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," NewsDataset({'train': 7948, 'test': 883}) \n","\n","Example sentences:\n","\t the three are suspected of aggravated attempts to avoid tax controls, according to swedish prosecutor olof sahlgren.\n","\t missing killer alan john giles seen in worcestershire village a convicted killer who absconded from prison has possibly been seen in worcestershire, police said.\n","\t it will be a lot tighter than people are expecting, said the three-times league cup winner.\n","\t they look cold, half of them.\n","\t but it could take a decade to build a base for the submarines elsewhere in the uk, it said.\n","\t hes a proven premier league performer who brings quality to our squad, boss gary megson told the club website.\n","\t but the industry is still so new there are many different approaches.\n","\t un chief ban ki-moon is sending two top aides to the country to help investigate the alleged assaults in the countrys volatile eastern region.\n","\t cheng cheng ensured that his classmates shouted down xu xiaofei before she had even started to speak, and she found it difficult to recover.\n","\t the report in beijings official english-language china daily notes that the country has suffered a series of knife attacks in schools and day-care centres in recent months.\n"]}],"source":["### DO NOT EDIT ###\n","\n","import os\n","import pandas as pd\n","from datasets import load_dataset\n","import random\n","from tqdm.notebook import tqdm\n","\n","class NewsDataset():\n","    \"\"\"\n","    A class to manage and preprocess the News dataset for this homework.\n","\n","    Attributes:\n","        train (list[str]): The training dataset as a list of sentences.\n","        test (list[str]): The testing dataset as a list of sentences.\n","    \"\"\"\n","    def __init__(self, redownload_dataset=False):\n","        \"\"\"\n","        Initializes the NewsDataset object. If the dataset files do not exist\n","        or redownload_dataset is set to True, it downloads the dataset.\n","\n","        Args:\n","            redownload_dataset (bool): If True, redownloads the dataset.\n","        \"\"\"\n","        self.train, self.test = [], []\n","        self.load_data(redownload_dataset)\n","\n","    def __str__(self):\n","        \"\"\"\n","        Returns a string representation of the dataset, including the number of\n","        training and testing examples.\n","\n","        Returns:\n","            str: Information about the dataset size\n","        \"\"\"\n","        repr = {}\n","        repr[\"train\"] = len(self.train) if self.train else 0\n","        repr[\"test\"] = len(self.test) if self.test else 0\n","        return f\"{type(self).__name__}({repr})\"\n","\n","    @staticmethod\n","    def isValid(s: str) -> bool:\n","        \"\"\"\n","        Checks whether a given string is valid for inclusion in the dataset.\n","        A string is considered valid if it contains more than five words and\n","        at least one alphabetic character.\n","\n","        Args:\n","            s (str): The string to validate.\n","\n","        Returns:\n","            bool: True if the string is valid, False otherwise.\n","        \"\"\"\n","        return s and (len(s.strip().split()) > 5) and any(c.isalpha() for c in s)\n","\n","    @staticmethod\n","    def preprocess(paragraph: str):\n","        paragraph = paragraph.translate(str.maketrans({'?': '.', '!': '.'}))\n","        sentences = paragraph.split('.')\n","        sentences = [s.replace(\"\\\"\", \"\").replace(\"\\'\", \"\").strip().replace(\"\\n\", \" \") + '.' for s in sentences]\n","        sentences = [s.lower() for s in sentences if '  ' not in s]\n","        sentences = [s for s in sentences if not any(char.isdigit() for char in s)]\n","        return sentences\n","\n","    def preprocessWP(self, data):\n","        \"\"\"\n","        Preprocesses a dataset by tokenizing them into\n","        sentences and filtering out invalid ones.\n","\n","        Args:\n","            data: An iterable containing the text of articles.\n","\n","        Returns:\n","            list[str]: A list of valid sentences.\n","        \"\"\"\n","        dataset = []\n","        for item in data:\n","            sentences = self.preprocess(item[\"text\"])\n","            sentences = [sent for sent in sentences if self.isValid(sent)]\n","            dataset += sentences\n","        return dataset\n","\n","    def downloadDatasetWP(self):\n","        \"\"\"\n","        Downloads and preprocesses the News dataset.\n","\n","        Returns:\n","            list[str]: A list of preprocessed sentences from the dataset.\n","        \"\"\"\n","        ds = load_dataset(\"permutans/fineweb-bbc-news\", \"sample-10BT\")\n","        ds = self.preprocessWP(ds[\"train\"])\n","        return ds\n","\n","    def generateDatasetWP(self):\n","        \"\"\"\n","        Downloads, preprocesses, and splits the dataset into training and\n","        testing sets.\n","        The processed data is saved as parquet files.\n","        \"\"\"\n","        ds = self.downloadDatasetWP()\n","        data = {\"text\": ds}\n","        df = pd.DataFrame(data)\n","        train = df.sample(frac=0.05,random_state=200)\n","        test = train.sample(frac=0.1,random_state=200)\n","        train = train.drop(test.index)\n","        train.to_parquet('train.parquet', index=False)\n","        test.to_parquet('test.parquet', index=False)\n","\n","    def load_data(self, redownload_dataset):\n","        \"\"\"\n","        Loads the dataset from parquet files. If the files do not exist or\n","        redownload_dataset is set to True, it downloads the dataset.\n","        \"\"\"\n","        if (redownload_dataset or\n","                not os.path.exists('train.parquet') or\n","                not os.path.exists('test.parquet')):\n","            self.generateDatasetWP()\n","        self.train, self.test = (pd.read_parquet('train.parquet')[\"text\"].tolist(),\n","                                 pd.read_parquet('test.parquet')[\"text\"].tolist())\n","\n","\n","if __name__ == '__main__':\n","    dataset = NewsDataset()\n","\n","    print('\\n', dataset, '\\n\\nExample sentences:')\n","\n","    for sentence in dataset.train[:10]:\n","        print('\\t', sentence)"]},{"cell_type":"markdown","metadata":{"id":"NqinUgNann-8"},"source":["## <font color='red'>TODO:</font> Tokenizer\n","\n","First, you will write a training (learning) algorithm, which takes in a corpus (list of sentences), and returns a token vocabulary. Then, you will write the tokenization algorithm. This takes in a sentence and returns its tokenization based on the vocabulary.\n","\n","Your code for this section will all be in the `WordPieceTokenizer` class, where you will implement several functions.\n","\n","### Training algorithm\n","\n","WordPiece learns a token vocabulary in an iterative fashion. Starting with an initial token vocabulary (the set of characters in the training data), each step merges a pair of consecutive tokens, where the pair of tokens is selected according to a scoring function (described below). The process is continued until a desired `vocab_size` is reached.\n","\n","You will complete this algorithm by implementing several functions in the `WordPieceTokenizer` class:\n","\n","1. **`initialize(self, train_corpus)`: [6 points]** This returns the initial vocabulary, the initial tokenization of each word, the word frequncies, and a mapping from each token to the words containing that token. First, split each sentence in `train_corpus` on the space character. The initial vocabulary is the set of all characters that occur in any word; but characters occuring in the middle of a word should be prepended with the special sequence `##`. For example, if your corpus is the sentence `the old man the boat.`, your initial vocabulary would be:\n","<div align=\"center\"><code>{'t', '##h', '##e', 'o', '##l', '##d', 'm', '##a', '##n', 't', 'b', '##o', '##t', '##.'}</code></div>\n","This function should also compute the initial tokenization of each word, storing them in a dictionary from word to tokenization. For example:\n","<div align=\"center\"><code>{'the': ('t', '##h', '##e;), 'old': ('o', '##l', '##d'),                \n","    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}\n","</code></div>\n","The function should also count the frequency of each word in the training corpus, and return a mapping from token to the words that use that token. For example:\n","<div align=\"center\"><code>word_freqs = {'the': 100, 'old': 34, 'man': 76, 'boat.': 18}</code></div>\n","    <div align=\"center\"><code>tokens2word = {'t': {'the'}, '##h': {'the'}, '##e': {'the'}, ...}</code></div>\n","\n","2. **`find_best_pair(self, word_tokenizations, word_freqs, vocab)`: [6 points]** This function computes a score for each *consecutive pair* $(t_1, t_2)$ of tokens, and returns the pair of tokens with highest score. The scoring function is $$\\frac{c(t_1, t_2) \\cdot |V|}{c(t_1)\\cdot c(t_2)}$$where $c(t_1, t_2)$ is the number of times $t_1$ and $t_2$ occur consecutively in the corpus, $c(t_i)$ is the nubmer of times token $t_i$ occurs, and $|V|$ is the size of the current token vocabulary. The function will return the pair with highest score (ties broken alphabetically).\n","\n","    The idea behind the scoring function is that a pair of tokens will have high score if they occur frequently together *and* each token does not occur frequently on its own. Factoring in the size of the vocabulary gives a (slight) preference to longer subword units as the vocabulary size grows.\n","    \n","\n","3. **`merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word)`: [6 points]** This function updates your current vocabulary (`vocab`), tokenizations (`word_tokenizations`), and token-to-word mapping (`tokens2word`) based on the pair of tokens to merge (`best_pair`). The `vocab` should simply be updated to include the new token, and any word in `word_tokenizations` that contains the consecutive pair of tokens should be updated to merge that pair. For example, suppose the pair of tokens to merge is `(##h, ##e)` and `word_tokenizations` is\n","<div align=\"center\"><code>  {'the': ('t', '##h', '##e;), 'old': ('o', '##l', '##d'),                \n","    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}</code></div>\n","Then <code>word_tokenizations</code> should be updated to\n","<div align=\"center\"><code>  {'the': ('t', '##he'), 'old': ('o', '##l', '##d'),                      \n","    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}</code></div>\n","Similarly, <code>tokens2word</code> should be updated to include the new token:\n","<div align=\"center\"><code>tokens2word['##he'] = {'the'}</code></div>\n","<i>Note:</i> In order to pass the autograder, you should <i>not</i> remove <code>'the'</code> from <code>tokens2word['##h']</code> or <code>tokens2word['##e']</code>, despite the fact that these tokens are no longer used for that word.\n","\n","We provide you the function `train(self, train_corpus, vocab_size)` which iteratively calls these functions until the desired `vocab_size` is hit. You should NOT modify this function, but you should look at it to understand how it works.\n","\n","### Tokenization algorithm\n","\n","Finally, you will write the `tokenize` and `detokenize` functions, which use the token vocabulary you learned to tokenize and detokenize arbitrary sentences:\n","\n","1. **`tokenize(self, sentence)`: [5 points]** This function takes a sentence and returns its tokenization. First, split on the space character. For each word, find the longest substring starting at the beginning of the word that is in the token vocabulary, and choose that token. Then, iteratively repeat this procedure on the remainder of the word, until the entire word is tokenized. For example, consider the word `swiftly`:\n","    * If `s` and `sw` are tokens but `swi` is not, then choose `sw`.\n","    * Then, proceed with `##iftly`. If `##i`, `##if`, and `##ift` are tokens, but `##iftl` is not, choose `##ift`.\n","    * Then, proceed with `##ly`. If both `##l` and `##ly` are tokens, choose `##ly`.\n","\n","  After tokenizing each word in this way, return a list containing the tokenization of each word separated by space tokens. For the sentence `he is swift.`, for example, you might return `['he', ' ', 'is', ' ', 'sw', '##ift', '##.']`\n","\n","  **Note:** If at any step during the tokenization of a word, you cannot find a matching token in your vocabulary, then the *entire* word should be tokenized as `<UNK>`. For example, if you are considering `##mat` but `##m`, `##ma`, and `##mat` are not in the vocabulary, the entire word (including the part of the word preceeding `##mat`) should be tokenized as `<UNK>`.\n","\n","\n","2. **`detokenize(self, tokens)`: [5 points]** This takes a list of tokens and returns its corresponding sentence. Simply copy tokens into a string from left to right, being careful to remove `##` at the beginning of tokens."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"RWRZljnLnMWu","executionInfo":{"status":"ok","timestamp":1740095496011,"user_tz":360,"elapsed":125,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"}}},"outputs":[],"source":["class WordPieceTokenizer(object):\n","    def __init__(self, train_corpus, vocab_size, do_print=False, do_tqdm=True):\n","        '''\n","        ### DO NOT EDIT ###\n","\n","        Trains a WordPiece tokenizer.\n","        Args:\n","            train_corpus: List of strings (each string is a sentence)\n","            vocab_size: Integer indicating how big the vocabulary should be\n","            do_print: Whether or not to print merges\n","        '''\n","        self.train_corpus = train_corpus\n","        self.vocab_size = vocab_size\n","        self.do_print = do_print\n","        self.do_tqdm = do_tqdm\n","        for sent in train_corpus: assert '##' not in sent, sent\n","        self.vocab = None\n","\n","    def initialize(self, train_corpus):\n","        '''\n","        Initialize token vocabulary, word frequencies, and token-to-word dictionaries.\n","        Args:\n","            train_corpus: List of strings, where each string is a sentence\n","        Returns:\n","            vocab: A set containing the initial token vocabulary (each token is a string)\n","            word_tokenizations: A dictionary of word to its tokenization (a tuple of strings)\n","            word_freqs: A dictionary that maps each word to its frequency in the training data\n","            tokens2word: A dictionary that maps each token to the set of words with that token\n","        '''\n","\n","        vocab = {'<UNK>', ' '} # Initialize vocab with special tokens (don't change this)\n","        word_tokenizations, word_freqs, tokens2word = {}, {}, {}\n","\n","        ### TODO ###\n","        for string in train_corpus:\n","          tc = string.split(\" \")\n","\n","          #get counts of each word\n","          for item in tc:\n","\n","            if item in word_tokenizations:\n","              word_freqs[item] +=1\n","              continue\n","            token_list =[]\n","            if item not in word_freqs:\n","              word_freqs[item] = 1\n","            else:\n","              word_freqs[item] +=1\n","\n","            for i in range(len(item)):\n","\n","              #item not in vocab, and its the first element\n","              if item[i] not in vocab and i==0:\n","                vocab.add(item[i])\n","                token_list.append(item[i])\n","              #item not in vocab, not first element\n","              elif item[i] not in vocab:\n","                vocab.add(\"##\"+item[i])\n","                token_list.append(\"##\"+item[i])\n","              #item in vocab, but its the first element\n","              elif item[i] in vocab and i==0:\n","                token_list.append(item[i])\n","              #item not in vocab, not first element\n","              else:\n","                token_list.append(\"##\"+item[i])\n","                vocab.add(\"##\"+item[i])\n","\n","            word_tokenizations[item] =tuple(token_list)\n","\n","            for j in range(len(item)):\n","              if item[j] not in tokens2word.keys() and j==0:\n","                a = set()\n","                a.add(item)\n","                tokens2word[item[j]] = a\n","              elif item[j] in tokens2word.keys() and j==0:\n","                if item not in tokens2word[item[j]]:\n","                  tokens2word[item[j]].add(item)\n","              elif (\"##\"+item[j]) not in tokens2word.keys() and j>0:\n","                a = set()\n","                a.add(item)\n","                tokens2word[\"##\"+ item[j]] = a\n","              elif (\"##\"+item[j]) in tokens2word.keys() and j>0:\n","                if item not in tokens2word[\"##\"+item[j]]:\n","                  tokens2word[\"##\"+ item[j]].add(item)\n","\n","\n","\n","        return vocab, word_tokenizations, word_freqs, tokens2word\n","\n","    def find_best_pair(self, word_tokenizations, word_freqs, vocab):\n","        '''\n","        Score all pairs of consecutive tokens (bigrams) in train_corpus, and return the pair with\n","          highest score. If there is a tie, choose the pair that is first alphabetically.\n","        Args:\n","            word_freqs: Dictionary of word to frqeuency\n","            word_tokenizations: Dictionary of word to its tokenization (a tuple of strings)\n","            vocab: A set of tokens (strings)\n","        Returns:\n","            best_pair: The pair (tuple) of tokens (t1, t2) that has highest score\n","            scores: Dictionary of tokens (t1, t2) to its score\n","        '''\n","\n","        best_pair, scores = None, {}\n","        unicountdict ={}\n","        bincountdict ={}\n","\n","        for word, tokens in word_tokenizations.items():\n","          for i in range(0, len(tokens)):\n","            if tokens[i] not in unicountdict:\n","              unicountdict[tokens[i]]=word_freqs[word]\n","            else:\n","              unicountdict[tokens[i]]+=word_freqs[word]\n","\n","        for word, tokens in word_tokenizations.items():\n","          for i in range(1, len(tokens)):\n","            if (tokens[i-1], tokens[i]) not in bincountdict:\n","              bincountdict[(tokens[i-1], tokens[i])] = word_freqs[word]\n","            else:\n","              bincountdict[(tokens[i-1], tokens[i])]+=word_freqs[word]\n","\n","        for tup, ct1t2 in bincountdict.items():\n","          ct1 = unicountdict[tup[0]]\n","          ct2 = unicountdict[tup[1]]\n","          scores[tup] = ct1t2*len(vocab)/(ct1*ct2)\n","\n","        max_value = max(scores.values())\n","        max_keys = [k for k, v in scores.items() if v == max_value]\n","        best_pair = min(max_keys)\n","\n","\n","        ### TODO ###\n","\n","        return best_pair, scores\n","\n","    def merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word):\n","        '''\n","        Update vocab, word_tokenizations, and tokens2word based on pair to merge.\n","        Args:\n","            best_pair: The pair of tokens that had highest score; that is, the pair to be merged\n","            vocab: The token vocabulary (set of strings)\n","            word_tokenizations: Dictionary from word to tokenization\n","            tokens2word: A dictionary from token to the set of words with that token\n","        Returns: Nothing\n","        Modifies:\n","            vocab: The new token should be added to the vocab set.\n","            word_tokenizations: Any word that contains the consecutive pair of tokens in best_pair\n","              should be re-tokenized such that that pair of tokens has been merged.\n","            tokens2word: The new token should be mapped to the set of words that use this token;\n","              do NOT otherwise modify this dictionary.\n","        Hint: When looking for words that contain the new token, look ONLY at words that use either of\n","          the tokens in best_pair (by using tokens2word).\n","        '''\n","\n","        ### TODO ###\n","        merged_pair = self.pair_concat(best_pair)\n","        if merged_pair not in vocab:\n","            vocab.add(merged_pair)\n","\n","        words1 = tokens2word[best_pair[0]]\n","\n","        words2 = tokens2word[best_pair[1]]\n","        tokens2word[merged_pair] = set()\n","\n","\n","        # tokens2word[merged_pair] = set()\n","        for word in (words1 | words2):\n","            if word in words1 and word in words2:\n","\n","                tokenlist = list(word_tokenizations[word])\n","\n","                for i in range(0, len(tokenlist)-1):\n","                  if i ==len(tokenlist)-1:\n","                    if best_pair[0] == tokenlist[i] and best_pair[1] == tokenlist[i+1]:\n","                      tokenlist[i+1]=merged_pair\n","                      tokenlist.remove(tokenlist[i])\n","                      tokens2word[merged_pair].add(word)\n","                      word_tokenizations[word] = tuple(tokenlist)\n","                      break\n","                    # else:\n","                    #   del tokens2word[merged_pair]\n","                    #   break\n","                  if best_pair[0] == tokenlist[i] and best_pair[1] == tokenlist[i+1]:\n","                    tokenlist[i+1]=merged_pair\n","                    tokenlist.remove(tokenlist[i])\n","                    tokens2word[merged_pair].add(word)\n","                    word_tokenizations[word] = tuple(tokenlist)\n","                    break\n","\n","\n","\n","\n","\n","    def pair_concat(self,pair):\n","      if pair[1].startswith('##'):\n","        return pair[0] + pair[1][2:]\n","      else:\n","        return pair[0] + pair[1]\n","\n","\n","\n","\n","    def train(self):\n","        '''\n","        ### DO NOT EDIT ###\n","\n","        Trains the WordPiece tokenization algorithm by calling iteratively merging tokens until\n","          vocab_size is reached.\n","        Args:\n","            train_corpus: List of strings, where each string is a sentence\n","            vocab_size: The desired vocabulary size\n","        Returns:\n","            vocab: The set of tokens in the vocabulary, returned as a sorted list\n","        '''\n","        vocab, word_tokenizations, word_freqs, tokens2word = self.initialize(self.train_corpus)\n","        init_vocab_size = len(vocab)\n","        if self.do_print: print(\"Initial vocab size:\", init_vocab_size)\n","        assert self.vocab_size >= len(vocab), 'Cannot give a vocab size smaller than initial vocab size'\n","\n","        itr = tqdm(range(self.vocab_size - len(vocab))) if self.do_tqdm else range(self.vocab_size - len(vocab))\n","        for i in itr:\n","            best_pair, scores = self.find_best_pair(word_tokenizations, word_freqs, vocab)\n","            self.merge_best_pair(best_pair, vocab, word_tokenizations, tokens2word)\n","            outputs = (vocab, word_tokenizations, tokens2word)\n","            print(\"\\tMerging \", best_pair) # UNCOMMENT IF YOU WANT TO SEE THE MERGES\n","            assert len(vocab) == init_vocab_size + i + 1, str(len(vocab)) + ' '+ str(init_vocab_size) + ' ' + str(i)\n","            if all(len(word_tokenizations[x]) == 1 for x in word_tokenizations):\n","                print(\"All words have been maximally merged at vocab_size=\" +str(len(vocab)) + \" â€“ breaking.\")\n","                break\n","        if self.do_print: print(\"Done!\")\n","        self.vocab = sorted(vocab)\n","\n","    def longest_matching_substring(self, word, string_set):\n","      longest_substring = \"\"\n","      current_substring = \"\"\n","      for char in word:\n","          current_substring += char\n","          if current_substring in string_set:\n","            if len(current_substring) > len(longest_substring):\n","              longest_substring = current_substring\n","\n","      return longest_substring\n","\n","\n","    def tokenize(self, sentence):\n","        '''\n","        Tokenizes a sentence.\n","        Args:\n","            sentence: A string representing a sentence\n","        Returns:\n","            tokens: A list containing the tokens of the sentence\n","        '''\n","        assert type(sentence) == str\n","        tokens = []\n","\n","        ### TODO ###\n","        sentence_split = sentence.split(\" \")\n","        listtokens=[]\n","        for word in sentence_split:\n","          i = 0\n","          wordremain = word\n","          int_tokens=[]\n","          while i <len(word):\n","            if i ==0:\n","              token = self.longest_matching_substring(wordremain,self.vocab)\n","            else:\n","              token = self.longest_matching_substring((\"##\" +wordremain),self.vocab)\n","\n","            if len(token) == 0:\n","              int_tokens.append('<UNK>')\n","              break\n","\n","            elif (i==0 and len(token)>0):\n","              tokenlen = len(token)\n","              wordremain = wordremain[tokenlen:]\n","              int_tokens.append(token)\n","            elif (i>0 and len(token)>2):\n","              tokenlen = len(token)-2\n","              wordremain = wordremain[tokenlen:]\n","              int_tokens.append(token)\n","            i += tokenlen\n","          listtokens.append(int_tokens)\n","\n","        newlist = []\n","        for word in listtokens:\n","          if '<UNK>' in word:\n","            newlist.append(['<UNK>'])\n","          else:\n","            newlist.append(word)\n","\n","        for word in newlist:\n","          for token in word:\n","            tokens.append(token)\n","          tokens.append(\" \")\n","\n","        tokens = tokens[:-1]\n","\n","\n","\n","        return tokens\n","\n","    def detokenize(self, tokens):\n","        '''\n","        Detokenizes a sentence.\n","        Args:\n","            tokens: A list of tokens representing a sentence\n","        Returns:\n","            sentence: A string representing the sentence\n","        '''\n","        assert type(tokens) == list and len(tokens) > 0 and type(tokens[0]) == str\n","        sentence = ''\n","\n","        for token in tokens:\n","          if token[0:2] == \"##\":\n","            sentence += str(token[2:])\n","          else:\n","            sentence += str(token)\n","\n","        return sentence"]},{"cell_type":"markdown","metadata":{"id":"nXCkjvBaLp_B"},"source":["##Sanity Check: Tokenizer Class\n","\n","The code below runs a sanity check for your `WordPieceTokenizer` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."]},{"cell_type":"code","execution_count":19,"metadata":{"cellView":"form","id":"1h6jCy1WL9WG","executionInfo":{"status":"ok","timestamp":1740095501188,"user_tz":360,"elapsed":635,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"}}},"outputs":[],"source":["# @title Sanity Check Code\n","### DO NOT EDIT ###\n","\n","def check_dictionary(test_dict, correct_dict):\n","    if not isinstance(test_dict, dict) or not isinstance(correct_dict, dict):\n","        return \"INCORRECT\", \"Is not a dictionary.\"\n","    if set(test_dict.keys()) != set(correct_dict.keys()):\n","        return \"INCORRECT\", f\"Key mismatch: Missing {set(correct_dict.keys()) - set(test_dict.keys())}, Extra {set(test_dict.keys()) - set(correct_dict.keys())}\"\n","    for key in correct_dict:\n","        if test_dict[key] != correct_dict[key]:\n","            return \"INCORRECT\", f\"Value mismatch for key '{key}': Expected {correct_dict[key]}, Got {test_dict[key]}\"\n","    return \"CORRECT\", ''\n","\n","def check_set(test_set, correct_set):\n","    if not isinstance(test_set, set) or not isinstance(correct_set, set):\n","        return \"INCORRECT\", \"Is not a set.\"\n","    if test_set != correct_set:\n","        return \"INCORRECT\", f\"Set mismatch: Missing {correct_set - test_set}, Extra {test_set - correct_set}\"\n","    return \"CORRECT\", \"\"\n","\n","def sanityCheckInitialize(train):\n","    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n","\n","    print('\\n\\n--- TEST: initialize(self, train_corpus) ---')\n","    res = tokenizer.initialize(train)\n","    if len(res) != 4:\n","        print('FAILED\\ninitialize(self, train_corpus) must return 4 items.')\n","        return\n","    vocab, word_tokenizations, word_freqs, tokens2word = res\n","\n","    correct_vocab = {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}\n","    correct_word_tokenizations = {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\n","    correct_word_freqs = {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\n","    correct_tokens2word = {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'that', 'banana.', 'boat.', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'that', 'boat.', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'lied.', 'like'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}\n","\n","    pass1, msg1 = check_set(vocab, correct_vocab)\n","    pass2, msg2 = check_dictionary(word_tokenizations, correct_word_tokenizations)\n","    pass3, msg3 = check_dictionary(word_freqs, correct_word_freqs)\n","    pass4, msg4 = check_dictionary(tokens2word, correct_tokens2word)\n","\n","    print('\\tvocab:\\t\\t\\t'+pass1+'\\t'+msg1 + '\\tYour vocab: ' +str(vocab) + '\\tCorrect vocab:' + str(correct_vocab))\n","    print('\\tword_tokenizations:\\t'+pass2+'\\t'+msg2 + '\\tYour word_tokenizations: ' +str(word_tokenizations) + '\\tCorrect word_tokenizations: ' + str(correct_word_tokenizations))\n","    print('\\tword_freqs:\\t\\t'+pass3+'\\t'+msg3 + '\\tYour word_freqs: ' +str(word_freqs) + '\\tCorrect word_freqs: ' + str(correct_word_freqs))\n","    print('\\ttokens2word:\\t\\t'+pass4+'\\t'+msg4 + '\\tYour tokens2word: ' +str(tokens2word) + '\\tCorrect tokens2word: ' + str(correct_tokens2word))\n","\n","    if len({pass1, pass2, pass3, pass4}) == 1 and pass1 == 'CORRECT':\n","        print('\\n  Passed!')\n","    else: print('  Failed.')\n","\n","def sanityCheckFindBestPair(train):\n","    print('\\n\\n--- TEST: find_best_pair(self, word_tokenizations, word_freqs, vocab) ---') # max_len does not matter for this test\n","    test_cases = ({'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}, [(({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##w', '##;'), {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##o', '##w;'), {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##r', '##ow;'), {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##r', '##row;'), {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##r', '##u'), {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('a', '##rrow;'), {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('f', '##ru'), {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('b', '##o'), {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##o', '##l'), {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##ol', '##d'), {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('f', '##l'), {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('o', '##l'), {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667})), (({'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('ol', '##d'), {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##d', '##.'), {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('k', '##n'), {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}), (('s', '##h'), {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}))])\n","\n","    word_freqs, test_cases = test_cases[0], test_cases[1]\n","    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n","\n","    overall_pass = True\n","    for i in range(len(test_cases)):\n","        res = tokenizer.find_best_pair(test_cases[i][0][0], word_freqs, test_cases[i][0][1])\n","        if len(res) != 2:\n","            pass1, msg1 = 'INCORRECT', 'Did not return 2 items.'\n","        else:\n","            best_pair, scores = res\n","            bp = best_pair == test_cases[i][1][0]\n","            sc, scc = check_dictionary(scores, test_cases[i][1][1])\n","            if bp and sc == 'CORRECT':\n","                pass1, msg1 = 'CORRECT', ''\n","            else:\n","                overall_pass = False\n","                pass1 = 'INCORRECT'\n","                msg1 = ''\n","                if not bp: msg1 += 'best_pair is incorrect. '\n","                if sc == 'INCORRECT': msg1 += 'scores is incorrect (' + scc + ')'\n","        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tword_tokenizations: ' + str(test_cases[i][0][0]) + '\\tword_freqs: ' + str(word_freqs) + '\\tvocab: ' + str(test_cases[i][0][1]) +\n","              '\\tYour best_pair: ' + str(best_pair) + '\\tCorrect best_pair: ' + str(test_cases[i][1][0]) + '\\tYour scores: ' + str(scores) + '\\tCorrect scores: ' + str(test_cases[i][1][1]))\n","    if overall_pass: print('\\n  Passed!')\n","    else: print('  Failed.')\n","\n","def sanityCheckMergeBestPair(train):\n","    print('\\n\\n--- TEST: merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word) ---') # max_len does not matter for this test\n","    test_cases = [((('##w', '##;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}})), ((('##o', '##w;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}})), ((('##r', '##ow;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}})), ((('##r', '##row;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}})), ((('##r', '##u'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}})), ((('a', '##rrow;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}})), ((('f', '##ru'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}})), ((('b', '##o'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}})), ((('##o', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}})), ((('##ol', '##d'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}})), ((('f', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}})), ((('o', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}})), ((('ol', '##d'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}})), ((('##d', '##.'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}})), ((('k', '##n'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}})), ((('s', '##h'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}), ({'old', '<UNK>', 'sh', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}))]\n","\n","    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n","\n","    overall_pass = True\n","    for i in range(len(test_cases)):\n","        from copy import deepcopy\n","        vocab, word_tokenizations, tokens2word = deepcopy(test_cases[i][0][1]), deepcopy(test_cases[i][0][2]), deepcopy(test_cases[i][0][3])\n","        vocabo, word_tokenizationso, tokens2wordo = deepcopy(vocab), deepcopy(word_tokenizations), deepcopy(tokens2word)\n","        tokenizer.merge_best_pair(test_cases[i][0][0], vocab, word_tokenizations, tokens2word)\n","\n","        pass1, msg0 = check_set(vocab, test_cases[i][1][0])\n","        pass2, msg2 = check_dictionary(word_tokenizations, test_cases[i][1][1])\n","        pass3, msg3 = check_dictionary(tokens2word, test_cases[i][1][2])\n","        if pass1 == 'CORRECT' and pass2 == 'CORRECT' and pass3 == 'CORRECT':\n","            pass1, msg1 = 'CORRECT', ''\n","        else:\n","            overall_pass = False\n","            pass1 = 'INCORRECT'\n","            msg1 = ''\n","            if pass1 != 'CORRECT': msg1 += 'vocab is incorrect. (' + msg0 + ') '\n","            if pass2 != 'CORRECT': msg1 += 'word_tokenizations is incorrect. (' + msg2 + ') '\n","            if pass3 != 'CORRECT': msg1 += 'tokens2word is incorrect. (' + msg3 + ') '\n","        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tbest_pair: ' + str(test_cases[i][0][0]) + '\\tvocab: ' + str(vocabo) + '\\tword_tokenizations: ' + str(word_tokenizationso) + '\\ttokens2word: ' + str(tokens2wordo) +\n","              '\\tYour vocab: ' + str(vocab) + '\\tCorrect vocab: ' + str(test_cases[i][1][0]) + '\\tYour word_tokenizations: ' + str(word_tokenizations) + '\\tCorrect word_tokenizations: ' + str(test_cases[i][1][1])+\n","              '\\tYour tokens2word: ' + str(tokens2word) + '\\tCorrect tokens2word: ' + str(test_cases[i][1][2]))\n","    if overall_pass: print('\\n  Passed!')\n","    else: print('  Failed.')\n","\n","\n","def sanityCheckTokenize(train, test):\n","\n","    print('\\n\\n--- TEST: tokenize(self, sentence) ---')\n","\n","    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n","    tokenizer.vocab = [' ', '##.', '##;', '##a', '##d', '##d.', '##e', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##ol', '##old', '##ow;', '##r', '##row;', '##rrow;', '##ru', '##s', '##t', '##u', '##w', '##w;', '<UNK>', 'a', 'arrow;', 'b', 'bo', 'f', 'fl', 'fru', 'h', 'k', 'kn', 'l', 'm', 'o', 'ol', 'old', 's', 'sh', 't']\n","    correct_ans = [['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>'], ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']]\n","\n","    overall_pass = True\n","    for i in range(len(test)):\n","        toks = tokenizer.tokenize(test[i])\n","        if toks == correct_ans[i]: pass1, msg1 = 'CORRECT', ''\n","        else:\n","            pass1, msg1 = 'INCORRECT', 'Tokenization is incorrect.'\n","            overall_pass = False\n","        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tsentence: ' + str(test[i]) +\n","              '\\tYour tokenization: ' + str(toks) + '\\tCorrect tokenization: ' + str(correct_ans[i]))\n","\n","    if overall_pass: print('\\n  Passed!')\n","    else: print('  Failed.')\n","\n","def sanityCheckDetokenize(train):\n","\n","    print('\\n\\n--- TEST: detokenize(self, tokens) ---')\n","\n","    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n","    tokenizer.vocab = [' ', '##.', '##;', '##a', '##d', '##d.', '##e', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##ol', '##old', '##ow;', '##r', '##row;', '##rrow;', '##ru', '##s', '##t', '##u', '##w', '##w;', '<UNK>', 'a', 'arrow;', 'b', 'bo', 'f', 'fl', 'fru', 'h', 'k', 'kn', 'l', 'm', 'o', 'ol', 'old', 's', 'sh', 't']\n","    inputs = [['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>'], ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']]\n","    correct_ans = ['she <UNK> the book <UNK>', 'time <UNK> for <UNK> one.']\n","\n","    overall_pass = True\n","    for i in range(len(inputs)):\n","        sent = tokenizer.detokenize(inputs[i])\n","        if sent == correct_ans[i]: pass1, msg1 = 'CORRECT', ''\n","        else:\n","            pass1, msg1 = 'INCORRECT', 'Sentence is incorrect.'\n","            overall_pass = False\n","        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\ttokens: ' + str(inputs[i]) +\n","              '\\tYour sentence: ' + str(sent) + '\\tCorrect sentence: ' + str(correct_ans[i]))\n","\n","    if overall_pass: print('\\n  Passed!')\n","    else: print('  Failed.')\n","\n","\n","def sanityCheckTokenizer():\n","    sample_train_corpus = [\"the old man the boat.\",\n","                   \"time flies like an arrow; fruit flies like a banana.\",\n","                   \"she told him that she knew that he lied.\"]\n","    sample_test_corpus = [\"she read the book 1984.\",\n","                          \"time waits for no one.\"]\n","\n","    print(\"Sample train corpus:\", sample_train_corpus)\n","    print(\"Sample test corpus:\", sample_test_corpus)\n","    sanityCheckInitialize(sample_train_corpus)\n","    sanityCheckFindBestPair(sample_train_corpus)\n","    sanityCheckMergeBestPair(sample_train_corpus)\n","    sanityCheckTokenize(sample_train_corpus, sample_test_corpus)\n","    sanityCheckDetokenize(sample_train_corpus)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266,"status":"ok","timestamp":1740095501456,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"cQvj7i7S9_Ru","outputId":"d324d4ec-537d-490e-aac1-1236cd7687a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sample train corpus: ['the old man the boat.', 'time flies like an arrow; fruit flies like a banana.', 'she told him that she knew that he lied.']\n","Sample test corpus: ['she read the book 1984.', 'time waits for no one.']\n","\n","\n","--- TEST: initialize(self, train_corpus) ---\n","\tvocab:\t\t\tCORRECT\t\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', '##d', 'h', 'l', '##k', 'b', '##.', '##u', 'f', 'k', '##s', '##a', '##;', '##w', '##m', '##e', 't', 'o', 'm', '##r', 's', '##l', '##o', '##i', '##t'}\tCorrect vocab:{'a', '##n', 't', ' ', '##h', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\n","\tword_tokenizations:\tCORRECT\t\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\n","\tword_freqs:\t\tCORRECT\t\tYour word_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tCorrect word_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\n","\ttokens2word:\t\tCORRECT\t\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'man', 'that'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'lied.', 'like'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}}\n","\n","  Passed!\n","\n","\n","--- TEST: find_best_pair(self, word_tokenizations, word_freqs, vocab) ---\n","\tCase  0:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', 't', ' ', '##h', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour best_pair: ('##w', '##;')\tCorrect best_pair: ('##w', '##;')\tYour scores: {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223}\tCorrect scores: {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223}\n","\tCase  1:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', 't', ' ', '##h', '##w;', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour best_pair: ('##o', '##w;')\tCorrect best_pair: ('##o', '##w;')\tYour scores: {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335}\tCorrect scores: {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335}\n","\tCase  2:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', 't', ' ', '##h', '##w;', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour best_pair: ('##r', '##ow;')\tCorrect best_pair: ('##r', '##ow;')\tYour scores: {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446}\tCorrect scores: {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446}\n","\tCase  3:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##row;', '##t'}\tYour best_pair: ('##r', '##row;')\tCorrect best_pair: ('##r', '##row;')\tYour scores: {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554}\tCorrect scores: {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554}\n","\tCase  4:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('##r', '##u')\tCorrect best_pair: ('##r', '##u')\tYour scores: {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665}\tCorrect scores: {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665}\n","\tCase  5:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('a', '##rrow;')\tCorrect best_pair: ('a', '##rrow;')\tYour scores: {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777}\tCorrect scores: {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777}\n","\tCase  6:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('f', '##ru')\tCorrect best_pair: ('f', '##ru')\tYour scores: {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889}\tCorrect scores: {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889}\n","\tCase  7:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('b', '##o')\tCorrect best_pair: ('b', '##o')\tYour scores: {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0}\tCorrect scores: {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0}\n","\tCase  8:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('##o', '##l')\tCorrect best_pair: ('##o', '##l')\tYour scores: {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111}\tCorrect scores: {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111}\n","\tCase  9:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('##ol', '##d')\tCorrect best_pair: ('##ol', '##d')\tYour scores: {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222}\tCorrect scores: {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222}\n","\tCase 10:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('f', '##l')\tCorrect best_pair: ('f', '##l')\tYour scores: {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5}\tCorrect scores: {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5}\n","\tCase 11:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('o', '##l')\tCorrect best_pair: ('o', '##l')\tYour scores: {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667}\tCorrect scores: {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667}\n","\tCase 12:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('ol', '##d')\tCorrect best_pair: ('ol', '##d')\tYour scores: {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333}\tCorrect scores: {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333}\n","\tCase 13:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('##d', '##.')\tCorrect best_pair: ('##d', '##.')\tYour scores: {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0}\tCorrect scores: {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0}\n","\tCase 14:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('k', '##n')\tCorrect best_pair: ('k', '##n')\tYour scores: {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335}\tCorrect scores: {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335}\n","\tCase 15:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('s', '##h')\tCorrect best_pair: ('s', '##h')\tYour scores: {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}\tCorrect scores: {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}\n","\n","  Passed!\n","\n","\n","--- TEST: merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word) ---\n","\tCase  0:\tCORRECT\t\tbest_pair: ('##w', '##;')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}}\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tCorrect vocab: {'a', '##n', 't', ' ', '##h', '##w;', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}}\n","\tCase  1:\tCORRECT\t\tbest_pair: ('##o', '##w;')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}}\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tCorrect vocab: {'a', '##n', 't', ' ', '##h', '##w;', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}\n","\tCase  2:\tCORRECT\t\tbest_pair: ('##r', '##ow;')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##row;', '##t'}\tCorrect vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}\n","\tCase  3:\tCORRECT\t\tbest_pair: ('##r', '##row;')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}\n","\tCase  4:\tCORRECT\t\tbest_pair: ('##r', '##u')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}\n","\tCase  5:\tCORRECT\t\tbest_pair: ('a', '##rrow;')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}\n","\tCase  6:\tCORRECT\t\tbest_pair: ('f', '##ru')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}\n","\tCase  7:\tCORRECT\t\tbest_pair: ('b', '##o')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}\n","\tCase  8:\tCORRECT\t\tbest_pair: ('##o', '##l')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}\n","\tCase  9:\tCORRECT\t\tbest_pair: ('##ol', '##d')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##old', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}\n","\tCase 10:\tCORRECT\t\tbest_pair: ('f', '##l')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}\tYour vocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}\n","\tCase 11:\tCORRECT\t\tbest_pair: ('o', '##l')\tvocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}\tYour vocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}\n","\tCase 12:\tCORRECT\t\tbest_pair: ('ol', '##d')\tvocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}\tYour vocab: {'old', 'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}\n","\tCase 13:\tCORRECT\t\tbest_pair: ('##d', '##.')\tvocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}\tYour vocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}\n","\tCase 14:\tCORRECT\t\tbest_pair: ('k', '##n')\tvocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}\tYour vocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}\n","\tCase 15:\tCORRECT\t\tbest_pair: ('s', '##h')\tvocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}\tYour vocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t', 'sh'}\tCorrect vocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t', 'sh'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}\n","\n","  Passed!\n","\n","\n","--- TEST: tokenize(self, sentence) ---\n","\tCase  0:\tCORRECT\t\tsentence: she read the book 1984.\tYour tokenization: ['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>']\tCorrect tokenization: ['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>']\n","\tCase  1:\tCORRECT\t\tsentence: time waits for no one.\tYour tokenization: ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']\tCorrect tokenization: ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']\n","\n","  Passed!\n","\n","\n","--- TEST: detokenize(self, tokens) ---\n","\tCase  0:\tCORRECT\t\ttokens: ['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>']\tYour sentence: she <UNK> the book <UNK>\tCorrect sentence: she <UNK> the book <UNK>\n","\tCase  1:\tCORRECT\t\ttokens: ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']\tYour sentence: time <UNK> for <UNK> one.\tCorrect sentence: time <UNK> for <UNK> one.\n","\n","  Passed!\n"]}],"source":["if __name__=='__main__':\n","    sanityCheckTokenizer()"]},{"cell_type":"markdown","metadata":{"id":"bCbUEgXfLz81"},"source":["## Run your tokenizer\n","\n","Now that you have written your tokenizer, we can run it on real data. If implemented correctly. your tokenizer should train in **less than 4 minutes**. If your code takes too long, make sure your `merge_best_pair(best_pair, ...)` function uses the `tokens2word` dictionary to only iterate over the words that use one of the tokens in `best_pair`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6161ccd5dcc14353884876f717a012b7","7ae74e8391d44f4baca20f5225eea4d6","2497a2bc0a6e4cbc846fcf248537f273","36eb1bd6761543daa53a69043fe0aa23","f39ecda8be43422f84defba1ec85278f","7e8b5a7771574e23b3984fb1618c3e5e","2b9e219db59143ce8d0ae7ebed8f16b2","7e33fe9337d949dda61beb09aee747d7","7d21c21222c34c5e923d075e9aa65b7f","2628e2b9c132450db5ff97763465c684","bb798ecc9b5046079944a3c5ac6de113"]},"executionInfo":{"elapsed":53556,"status":"ok","timestamp":1739577279044,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"lPdN2aTVvTu9","outputId":"ad6b82c4-b912-4579-9321-0b1fe4f15efd"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/603 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6161ccd5dcc14353884876f717a012b7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\tMerging  ('##^', '##^')\n","\tMerging  ('>', '##>')\n","\tMerging  ('##>', '##>')\n","\tMerging  ('|', '##^^')\n","\tMerging  ('|', '##Â©')\n","\tMerging  ('m', '##Ã³')\n","\tMerging  ('e', '##x')\n","\tMerging  ('x', '##-')\n","\tMerging  ('b', '##Ã¤')\n","\tMerging  ('#', '##b')\n","\tMerging  ('d', '##Ã­')\n","\tMerging  ('x', '##z')\n","\tMerging  ('|Â©', '##m')\n","\tMerging  ('|Â©m', '##m')\n","\tMerging  ('|Â©mm', '##v')\n","\tMerging  ('f', '##Ã©')\n","\tMerging  ('bÃ¤', '##c')\n","\tMerging  ('bÃ¤c', '##k')\n","\tMerging  ('xz', '##c')\n","\tMerging  ('(', '##j')\n","\tMerging  ('##b', '(j')\n","\tMerging  ('##b(j', '##p')\n","\tMerging  ('##b(jp', '##)')\n","\tMerging  ('r', '##&')\n","\tMerging  ('r&', '##b')\n","\tMerging  ('##b', '##&')\n","\tMerging  ('b', '##b&')\n","\tMerging  ('d', '##&')\n","\tMerging  ('##z', '##z')\n","\tMerging  ('a', '##&')\n","\tMerging  ('a', '##*')\n","\tMerging  ('##u', '##Ã­')\n","\tMerging  ('##q', '##uÃ­')\n","\tMerging  ('##(', '##p')\n","\tMerging  ('##(', '##m')\n","\tMerging  ('##f', '##Ã©')\n","\tMerging  ('o', '##f')\n","\tMerging  ('q', '##u')\n","\tMerging  ('q', '##:')\n","\tMerging  ('ex', '##p')\n","\tMerging  ('â€œ', '##w')\n","\tMerging  ('d&', '##d')\n","\tMerging  ('##q', '##u')\n","\tMerging  ('##-', '##q')\n","\tMerging  ('e', '##qu')\n","\tMerging  ('##m', '##â”')\n","\tMerging  ('##â”', '##.')\n","\tMerging  ('##l', '##-q')\n","\tMerging  ('a', '##l-q')\n","\tMerging  ('`', '##l')\n","\tMerging  ('##Â¿', '##h')\n","\tMerging  ('bÃ¤ck', '##h')\n","\tMerging  ('(j', '##v')\n","\tMerging  ('##p', '##|')\n","\tMerging  ('##mâ”', '##s')\n","\tMerging  ('##s', '##(m')\n","\tMerging  ('##y', '##s(m')\n","\tMerging  ('##â”', '##s')\n","\tMerging  ('(jv', '##s')\n","\tMerging  ('(jvs', '##)')\n","\tMerging  ('a*', '##s')\n","\tMerging  ('##f', '##f')\n","\tMerging  ('e', '##ff')\n","\tMerging  ('of', '##f')\n","\tMerging  ('e', '##v')\n","\tMerging  ('##t', '##ys(m')\n","\tMerging  ('bb&', '##t')\n","\tMerging  ('dÃ­', '##t')\n","\tMerging  ('dÃ­t', '##r')\n","\tMerging  ('mÃ³', '##r')\n","\tMerging  ('j', '##u')\n","\tMerging  ('##i', '##Å†')\n","\tMerging  ('n', '##iÅ†')\n","\tMerging  ('fÃ©', '##i')\n","\tMerging  ('|Â©mmv', '##i')\n","\tMerging  ('|Â©mmvi', '##i')\n","\tMerging  ('|Â©mmvii', '##i')\n","\tMerging  ('u', '##p')\n","\tMerging  ('up', '##-')\n","\tMerging  ('##x', 'up')\n","\tMerging  ('u', '##k')\n","\tMerging  ('uk', '##â€™')\n","\tMerging  ('ukâ€™', '##s')\n","\tMerging  ('##a', '##fÃ©')\n","\tMerging  ('c', '##afÃ©')\n","\tMerging  ('##a', '##mâ”s')\n","\tMerging  ('##a', '##amâ”s')\n","\tMerging  ('##d', '##aamâ”s')\n","\tMerging  ('##d', '##daamâ”s')\n","\tMerging  ('s', '##ddaamâ”s')\n","\tMerging  ('##a', '##quÃ­')\n","\tMerging  ('##Â¿h', '##a')\n","\tMerging  ('##Â¿ha', '##v')\n","\tMerging  ('al-q', '##a')\n","\tMerging  ('niÅ†', '##a')\n","\tMerging  ('xzc', '##a')\n","\tMerging  ('â€œw', '##a')\n","\tMerging  ('â”', '##a')\n","\tMerging  ('â”a', '##c')\n","\tMerging  ('â”ac', '##h')\n","\tMerging  ('â”ach', '##i')\n","\tMerging  ('t', '##h')\n","\tMerging  ('##(p', '##o')\n","\tMerging  ('##(po', '##r')\n","\tMerging  ('##(por', '##t')\n","\tMerging  ('##(port', '##h')\n","\tMerging  ('##(porth', '##m')\n","\tMerging  ('##(porthm', '##a')\n","\tMerging  ('##(porthma', '##d')\n","\tMerging  ('##(porthmad', '##o')\n","\tMerging  ('##(porthmado', '##g')\n","\tMerging  ('##(porthmadog', '##,')\n","\tMerging  ('##o', '##aquÃ­')\n","\tMerging  ('j', '##oaquÃ­')\n","\tMerging  ('##o', '##p|')\n","\tMerging  ('t', '##op|')\n","\tMerging  ('##tys(m', '##o')\n","\tMerging  ('`l', '##o')\n","\tMerging  ('uk', '##)')\n","\tMerging  ('##|', '##.')\n","\tMerging  ('##d', '##|')\n","\tMerging  ('##d', '##:')\n","\tMerging  ('@', '##d')\n","\tMerging  ('##d:', '@d')\n","\tMerging  ('##k', '##d:@d')\n","\tMerging  ('##r', '##kd:@d')\n","\tMerging  ('##r', '##rkd:@d')\n","\tMerging  ('##o', '##rrkd:@d')\n","\tMerging  ('##w', '##orrkd:@d')\n","\tMerging  ('##w', '##worrkd:@d')\n","\tMerging  ('##wworrkd:@d', '##o')\n","\tMerging  ('##-', '##j')\n","\tMerging  ('##-j', '##j')\n","\tMerging  ('##-jj', '##a')\n","\tMerging  ('##-jja', '##r')\n","\tMerging  ('##-jjar', '##,')\n","\tMerging  ('##a', '##-jjar,')\n","\tMerging  ('##b', '##j')\n","\tMerging  ('o', '##bj')\n","\tMerging  ('##u', '##bj')\n","\tMerging  ('s', '##ubj')\n","\tMerging  ('##bj', '##k')\n","\tMerging  ('##bjk', '##s')\n","\tMerging  ('##n', '##a-jjar,')\n","\tMerging  ('##l', '##na-jjar,')\n","\tMerging  ('a', '##lna-jjar,')\n","\tMerging  ('##n', '##d|')\n","\tMerging  ('##n', '##tys(mo')\n","\tMerging  ('##o', '##ntys(mo')\n","\tMerging  ('m', '##ontys(mo')\n","\tMerging  ('##n', '##Ã©')\n","\tMerging  ('##n', '##nÃ©')\n","\tMerging  ('##n', '##â”.')\n","\tMerging  ('##o', '##nâ”.')\n","\tMerging  ('##i', '##onâ”.')\n","\tMerging  ('##i', '##ionâ”.')\n","\tMerging  ('##s', '##iionâ”.')\n","\tMerging  ('##s', '##siionâ”.')\n","\tMerging  ('m', '##ssiionâ”.')\n","\tMerging  ('`lo', '##n')\n","\tMerging  ('`lon', '##g')\n","\tMerging  ('fÃ©i', '##n')\n","\tMerging  ('joaquÃ­', '##n')\n","\tMerging  ('joaquÃ­n', '##,')\n","\tMerging  ('montys(mo', '##n')\n","\tMerging  ('montys(mon', '##t')\n","\tMerging  ('montys(mont', '##g')\n","\tMerging  ('montys(montg', '##o')\n","\tMerging  ('montys(montgo', '##m')\n","\tMerging  ('â€œwa', '##n')\n","\tMerging  ('â€œwan', '##t')\n","\tMerging  ('up-', '##t')\n","\tMerging  ('up-', '##c')\n","\tMerging  ('ex', '##-')\n","\tMerging  ('ex-', '##b')\n","\tMerging  ('ex-b', '##r')\n","\tMerging  ('ex-br', '##i')\n","\tMerging  ('ex-bri', '##t')\n","\tMerging  ('ex-brit', '##i')\n","\tMerging  ('ex-briti', '##s')\n","\tMerging  ('ex-britis', '##h')\n","\tMerging  ('up-t', '##o')\n","\tMerging  ('up-to', '##-')\n","\tMerging  ('up-to', '##,')\n","\tMerging  ('##p', 'up-to,')\n","\tMerging  ('##m', '##pup-to,')\n","\tMerging  ('up-to-', '##d')\n","\tMerging  ('up-to-d', '##a')\n","\tMerging  ('up-to-da', '##t')\n","\tMerging  ('##n', 'up-t')\n","\tMerging  ('##o', '##nup-t')\n","\tMerging  ('##r', '##onup-t')\n","\tMerging  ('##f', '##ronup-t')\n","\tMerging  ('x-', '##r')\n","\tMerging  ('x-', '##t')\n","\tMerging  ('x-t', '##b')\n","\tMerging  ('##r', 'x-tb')\n","\tMerging  ('##d', '##rx-tb')\n","\tMerging  ('x-r', '##a')\n","\tMerging  ('x-ra', '##y')\n","\tMerging  ('x-ra', '##t')\n","\tMerging  ('##a', '##q')\n","\tMerging  ('##q', '##x')\n","\tMerging  ('u', '##q')\n","\tMerging  ('uq', '##r')\n","\tMerging  ('uqr', '##d')\n","\tMerging  ('##r', '##aq')\n","\tMerging  ('##f', '##aq')\n","\tMerging  ('w', '##aq')\n","\tMerging  ('i', '##raq')\n","\tMerging  ('##h', '##faq')\n","\tMerging  ('##s', '##hfaq')\n","\tMerging  ('i', '##shfaq')\n","\tMerging  ('a', '##shfaq')\n","\tMerging  ('waq', '##t')\n","\tMerging  ('##o', '##qx')\n","\tMerging  ('##l', '##oqx')\n","\tMerging  ('ex-', '##v')\n","\tMerging  ('ex-', '##c')\n","\tMerging  ('ex-c', '##h')\n","\tMerging  ('ex-', '##g')\n","\tMerging  ('ex-g', '##u')\n","\tMerging  ('ex-c', '##i')\n","\tMerging  ('ex-v', '##i')\n","\tMerging  ('ex-ch', '##a')\n","\tMerging  ('ex-ci', '##a')\n","\tMerging  ('ex-vi', '##c')\n","\tMerging  ('##o', 'ex-vi')\n","\tMerging  ('##s', '##oex-vi')\n","\tMerging  ('ex-cha', '##n')\n","\tMerging  ('ex-chan', '##c')\n","\tMerging  ('##]', '##.')\n","\tMerging  ('##k', '##].')\n","\tMerging  ('##c', '##k].')\n","\tMerging  ('##a', '##ck].')\n","\tMerging  ('##b', '##ack].')\n","\tMerging  ('[', '##back].')\n","\tMerging  ('##q', '##b')\n","\tMerging  ('i', '##qb')\n","\tMerging  ('iqb', '##a')\n","\tMerging  ('iqba', '##l')\n","\tMerging  ('up-c', '##l')\n","\tMerging  ('##r', 'up-cl')\n","\tMerging  ('##rup-cl', '##a')\n","\tMerging  ('##rup-cla', '##s')\n","\tMerging  ('##rup-clas', '##s')\n","\tMerging  ('up-c', '##o')\n","\tMerging  ('up-co', '##m')\n","\tMerging  ('up-com', '##i')\n","\tMerging  ('up-comi', '##n')\n","\tMerging  ('up-comin', '##g')\n","\tMerging  ('##/', '##h')\n","\tMerging  ('##/', '##f')\n","\tMerging  ('##s', '##/f')\n","\tMerging  ('##s', '##s/f')\n","\tMerging  ('##t', '##ss/f')\n","\tMerging  ('##l', '##tss/f')\n","\tMerging  ('##u', '##ltss/f')\n","\tMerging  ('##ultss/f', '##i')\n","\tMerging  ('##ultss/fi', '##x')\n","\tMerging  ('##ultss/fix', '##t')\n","\tMerging  ('##ultss/fixt', '##u')\n","\tMerging  ('##ultss/fixtu', '##r')\n","\tMerging  ('##k', '##/')\n","\tMerging  ('##k/', '##c')\n","\tMerging  ('##k/', '##u')\n","\tMerging  ('##u', '##k/u')\n","\tMerging  ('(', '##uk/u')\n","\tMerging  ('##s', '##k/c')\n","\tMerging  ('##l', '##sk/c')\n","\tMerging  ('##l', '##lsk/c')\n","\tMerging  ('(uk/u', '##s')\n","\tMerging  ('##i', '##llsk/c')\n","\tMerging  ('s', '##illsk/c')\n","\tMerging  ('(uk/us', '##a')\n","\tMerging  ('(uk/usa', '##)')\n","\tMerging  ('sillsk/c', '##o')\n","\tMerging  ('sillsk/co', '##n')\n","\tMerging  ('sillsk/con', '##n')\n","\tMerging  ('##-', '##qu')\n","\tMerging  ('##-qu', '##a')\n","\tMerging  ('##-qua', '##k')\n","\tMerging  ('##t', '##-quak')\n","\tMerging  ('##s', '##t-quak')\n","\tMerging  ('##o', '##st-quak')\n","\tMerging  ('p', '##ost-quak')\n","\tMerging  ('##-qua', '##l')\n","\tMerging  ('##-qua', '##r')\n","\tMerging  ('##-quar', '##t')\n","\tMerging  ('##-qual', '##i')\n","\tMerging  ('##-quali', '##t')\n","\tMerging  ('##-qualit', '##y')\n","\tMerging  ('##y', '##].')\n","\tMerging  ('##g', '##y].')\n","\tMerging  ('##a', '##y].')\n","\tMerging  ('##a', '##ay].')\n","\tMerging  ('##d', '##aay].')\n","\tMerging  ('##r', '##daay].')\n","\tMerging  ('##u', '##rdaay].')\n","\tMerging  ('##t', '##urdaay].')\n","\tMerging  ('s', '##turdaay].')\n","\tMerging  ('##n', '##gy].')\n","\tMerging  ('##i', '##ngy].')\n","\tMerging  ('##l', '##ingy].')\n","\tMerging  ('##p', '##lingy].')\n","\tMerging  ('##p', '##plingy].')\n","\tMerging  ('##u', '##pplingy].')\n","\tMerging  ('s', '##upplingy].')\n","\tMerging  ('##v', '##/')\n","\tMerging  ('##p', '##/')\n","\tMerging  ('##p/', '##r')\n","\tMerging  ('##o', '##p/r')\n","\tMerging  ('p', '##op/r')\n","\tMerging  ('pop/r', '##o')\n","\tMerging  ('pop/ro', '##c')\n","\tMerging  ('pop/roc', '##k')\n","\tMerging  ('[', '##w')\n","\tMerging  ('##)', '##,')\n","\tMerging  ('##)', '##.')\n","\tMerging  ('##x', '##).')\n","\tMerging  ('##i', '##x).')\n","\tMerging  ('m', '##ix).')\n","\tMerging  ('(', '##f')\n","\tMerging  ('(f', '##),')\n","\tMerging  ('(f', '##)')\n","\tMerging  ('##w', '(f),')\n","\tMerging  ('##w', '(f)')\n","\tMerging  ('##g', '##w(f)')\n","\tMerging  ('##p', '(f),')\n","\tMerging  ('##u', '(f),')\n","\tMerging  ('##d', '(f),')\n","\tMerging  ('##d', '##d(f),')\n","\tMerging  ('##a', '##w(f),')\n","\tMerging  ('##n', '##u(f),')\n","\tMerging  ('iraq', '##.')\n","\tMerging  ('##â€™', '##s')\n","\tMerging  ('i', '##â€™')\n","\tMerging  ('iâ€™', '##m')\n","\tMerging  ('##i', '##d:')\n","\tMerging  ('##d', '##d:')\n","\tMerging  ('##d', '##dd:')\n","\tMerging  ('##a', '##id:')\n","\tMerging  ('s', '##aid:')\n","\tMerging  ('##g', '##id:')\n","\tMerging  ('##n', '##gid:')\n","\tMerging  ('##d', '##ngid:')\n","\tMerging  ('a', '##dngid:')\n","\tMerging  ('##n', '##ngid:')\n","\tMerging  ('##u', '##nngid:')\n","\tMerging  ('##l', '##unngid:')\n","\tMerging  ('##c', '##lunngid:')\n","\tMerging  ('i', '##clunngid:')\n","\tMerging  ('w', '##h')\n","\tMerging  ('##â€™', '##t')\n","\tMerging  ('##â€™', '##d')\n","\tMerging  ('##n', '##â€™t')\n","\tMerging  ('##d', '##nâ€™t')\n","\tMerging  ('##i', '##dnâ€™t')\n","\tMerging  ('d', '##idnâ€™t')\n","\tMerging  ('##o', '##nâ€™t')\n","\tMerging  ('d', '##onâ€™t')\n","\tMerging  ('##c', '##k')\n","\tMerging  ('(', '##c')\n","\tMerging  ('(c', '##)')\n","\tMerging  ('(c', '##ck')\n","\tMerging  ('(c', '##h')\n","\tMerging  ('(c', '##p')\n","\tMerging  ('##ff', '##)')\n","\tMerging  ('##a', '##ff)')\n","\tMerging  ('(', '##aff)')\n","\tMerging  ('##a', '(cp')\n","\tMerging  ('##a(cp', '##o')\n","\tMerging  ('##a(cpo', '##),')\n","\tMerging  ('##s', '(c)')\n","\tMerging  ('##s', '##s(c)')\n","\tMerging  ('##p', '##s(c)')\n","\tMerging  ('##m', '##s(c)')\n","\tMerging  ('##i', '(ch')\n","\tMerging  ('##h', '##i(ch')\n","\tMerging  ('##w', '##hi(ch')\n","\tMerging  ('##i(ch', '##i')\n","\tMerging  ('##o', '##i(chi')\n","\tMerging  ('##k', '##oi(chi')\n","\tMerging  ('(ch', '##n')\n","\tMerging  ('(chn', '##o')\n","\tMerging  ('(chno', '##l')\n","\tMerging  ('(chnol', '##o')\n","\tMerging  ('(chnolo', '##g')\n","\tMerging  ('(chnolog', '##y')\n","\tMerging  ('(chnology', '##,')\n","\tMerging  ('##s', '##dd:')\n","\tMerging  ('a', '##sdd:')\n","\tMerging  ('##n', '##sdd:')\n","\tMerging  ('##n', '##nsdd:')\n","\tMerging  ('##a', '##nnsdd:')\n","\tMerging  ('##t', '##annsdd:')\n","\tMerging  ('##r', '##tannsdd:')\n","\tMerging  ('##zz', '##y')\n","\tMerging  ('##i', '##zzy')\n","\tMerging  ('f', '##izzy')\n","\tMerging  ('t', '##izzy')\n","\tMerging  ('(', '##w')\n","\tMerging  ('(w', '##h')\n","\tMerging  ('##b', '(w')\n","\tMerging  ('(w', '##b')\n","\tMerging  ('(wb', '##)')\n","\tMerging  ('##u', '(wb)')\n","\tMerging  ('##b(w', '##r')\n","\tMerging  ('##b(wr', '##)')\n","\tMerging  ('##-', '##b')\n","\tMerging  ('##-', '##-b')\n","\tMerging  ('##f', '##--b')\n","\tMerging  ('uk', '##-b')\n","\tMerging  ('uk-b', '##a')\n","\tMerging  ('uk-ba', '##s')\n","\tMerging  ('##o', '##f--b')\n","\tMerging  ('e', '##-b')\n","\tMerging  ('##m', 'e-b')\n","\tMerging  ('e-b', '##o')\n","\tMerging  ('e-b', '##a')\n","\tMerging  ('e-ba', '##s')\n","\tMerging  ('##n', 'e-bas')\n","\tMerging  ('##o', '##ne-bas')\n","\tMerging  ('##i', '##one-bas')\n","\tMerging  ('##t', '##ione-bas')\n","\tMerging  ('##a', '##tione-bas')\n","\tMerging  ('##c', '##atione-bas')\n","\tMerging  ('##u', '##catione-bas')\n","\tMerging  ('##d', '##ucatione-bas')\n","\tMerging  ('##-', '##f')\n","\tMerging  ('##ff', '##-f')\n","\tMerging  ('##ff-f', '##,')\n","\tMerging  ('##u', '##ff-f')\n","\tMerging  ('##t', '##uff-f')\n","\tMerging  ('s', '##tuff-f')\n","\tMerging  ('stuff-f', '##i')\n","\tMerging  ('stuff-fi', '##l')\n","\tMerging  ('stuff-fil', '##l')\n","\tMerging  ('##x', '##-f')\n","\tMerging  ('##x-f', '##r')\n","\tMerging  ('##a', '##x-fr')\n","\tMerging  ('t', '##ax-fr')\n","\tMerging  ('off', '##-')\n","\tMerging  ('off-', '##p')\n","\tMerging  ('off-p', '##u')\n","\tMerging  ('##s', 'off-p')\n","\tMerging  ('off-pu', '##t')\n","\tMerging  ('off-put', '##t')\n","\tMerging  ('##soff-p', '##i')\n","\tMerging  ('off-putt', '##i')\n","\tMerging  ('##soff-pi', '##n')\n","\tMerging  ('##soff-pin', '##n')\n","\tMerging  ('off-putti', '##n')\n","\tMerging  ('off-puttin', '##g')\n","\tMerging  ('off-putting', '##,')\n","\tMerging  ('off-', '##c')\n","\tMerging  ('##x', '##-')\n","\tMerging  ('##x-', '##m')\n","\tMerging  ('##x-', '##w')\n","\tMerging  ('##x-', '##y')\n","\tMerging  ('##x-', '##h')\n","\tMerging  ('##i', '##x-h')\n","\tMerging  ('s', '##ix-h')\n","\tMerging  ('##i', '##x-w')\n","\tMerging  ('s', '##ix-w')\n","\tMerging  ('##i', '##x-y')\n","\tMerging  ('s', '##ix-y')\n","\tMerging  ('##x-m', '##o')\n","\tMerging  ('six-h', '##o')\n","\tMerging  ('six-ho', '##u')\n","\tMerging  ('six-hou', '##r')\n","\tMerging  ('##i', '##x-mo')\n","\tMerging  ('s', '##ix-mo')\n","\tMerging  ('##l', '##x-mo')\n","\tMerging  ('##l', '##lx-mo')\n","\tMerging  ('##llx-mo', '##r')\n","\tMerging  ('##llx-mor', '##r')\n","\tMerging  ('##llx-morr', '##i')\n","\tMerging  ('##llx-morri', '##s')\n","\tMerging  ('six-mo', '##n')\n","\tMerging  ('six-mon', '##t')\n","\tMerging  ('six-mont', '##h')\n","\tMerging  ('##g', '##q')\n","\tMerging  ('##gq', '##v')\n","\tMerging  ('##r', '##gqv')\n","\tMerging  ('##gq', '##i')\n","\tMerging  ('##rgqv', '##i')\n","\tMerging  ('##rgqvi', '##s')\n","\tMerging  ('##rgqvis', '##t')\n","\tMerging  ('##gqi', '##n')\n","\tMerging  ('##gqin', '##g')\n","\tMerging  ('##n', '##gqing')\n","\tMerging  ('##o', '##ngqing')\n","\tMerging  ('##h', '##ongqing')\n","\tMerging  ('c', '##hongqing')\n","\tMerging  ('e-bo', '##o')\n","\tMerging  ('e-boo', '##k')\n","\tMerging  ('e-bo', '##r')\n","\tMerging  ('e-bor', '##d')\n","\tMerging  ('##c', '##/h')\n","\tMerging  ('##c', '##c/h')\n","\tMerging  ('t', '##o')\n","\tMerging  ('y', '##o')\n","\tMerging  ('yo', '##u')\n","\tMerging  ('##ck', 'yo')\n","\tMerging  ('##ckyo', '##,')\n","\tMerging  ('yo', '##w')\n","\tMerging  ('##l', 'yow')\n","\tMerging  ('##l', '##lyow')\n","\tMerging  ('##-', 'yo')\n","\tMerging  ('##-yo', '##m')\n","\tMerging  ('##-yom', '##o')\n","\tMerging  ('##-yomo', '##s')\n","\tMerging  ('##-yomos', '##o')\n","\tMerging  ('##-yomoso', '##m')\n","\tMerging  ('##r', '##-yomosom')\n","\tMerging  ('##h', '##r-yomosom')\n","\tMerging  ('##c', '##hr-yomosom')\n","\tMerging  ('##-yo', '##n')\n","\tMerging  ('##-yon', '##-')\n","\tMerging  ('##-yon-', '##y')\n","\tMerging  ('##-yon', '##g')\n","\tMerging  ('##l', '##-yong')\n","\tMerging  ('##r', '##-yon-y')\n","\tMerging  ('##r', '##l-yong')\n","\tMerging  ('##a', '##r-yon-y')\n","\tMerging  ('##a', '##rl-yong')\n","\tMerging  ('##n', '##ckyo')\n","\tMerging  ('##n', '##ckyo,')\n","\tMerging  ('yo', '##r')\n","\tMerging  ('yor', '##k')\n","\tMerging  ('yo', '##v')\n","\tMerging  ('##k', 'yov')\n","\tMerging  ('##kyov', '##y')\n","\tMerging  ('##kyovy', '##c')\n","\tMerging  ('##kyovyc', '##h')\n","\tMerging  ('yo', '##b')\n","\tMerging  ('yob', '##b')\n","\tMerging  ('##u', '##kyovych')\n","\tMerging  ('yo', '##k')\n","\tMerging  ('yov', '##,')\n","\tMerging  ('##p', 'yov')\n","\tMerging  ('##u', '##pyov')\n","\tMerging  ('##u', '##upyov')\n","\tMerging  ('##v', 'yo')\n","\tMerging  ('york', '##,')\n","\tMerging  ('##h', 'yo')\n","\tMerging  ('##hyo', '##y')\n","\tMerging  ('##d', '##hyoy')\n","\tMerging  ('##u', '##dhyoy')\n","\tMerging  ('##s', '##uupyov')\n","\tMerging  ('##udhyoy', '##o')\n","\tMerging  ('yob', '##o')\n","\tMerging  ('yok', '##o')\n","\tMerging  ('yoko', '##h')\n","\tMerging  ('yobb', '##i')\n","\tMerging  ('yobbi', '##s')\n","\tMerging  ('yobbis', '##h')\n","\tMerging  ('yov', '##i')\n","\tMerging  ('##a', 'yovi')\n","\tMerging  ('##l', '##ayovi')\n","\tMerging  ('##s', '##layovi')\n","\tMerging  ('##g', '##slayovi')\n","\tMerging  ('##u', '##gslayovi')\n","\tMerging  ('##ugslayovi', '##a')\n","\tMerging  ('##ugslayovia', '##.')\n","\tMerging  ('yokoh', '##a')\n","\tMerging  ('yokoha', '##m')\n","\tMerging  ('yokoham', '##a')\n","\tMerging  ('yokohama', '##,')\n","\tMerging  ('##n', '##ukyovych')\n","\tMerging  ('##a', '##nukyovych')\n","\tMerging  ('##udhyoyo', '##n')\n","\tMerging  ('##udhyoyon', '##o')\n","\tMerging  ('##udhyoyono', '##,')\n","\tMerging  ('##vyo', '##n')\n","\tMerging  ('##vyon', '##n')\n","\tMerging  ('##l', 'yov,')\n","\tMerging  ('##u', '##lyov,')\n","\tMerging  ('##g', '##ulyov,')\n","\tMerging  ('##a', '##gulyov,')\n","\tMerging  ('##z', '##agulyov,')\n","\tMerging  ('##r', '##zagulyov,')\n","\tMerging  ('##a', '##agulyov,')\n","\tMerging  ('##i', '##aagulyov,')\n","\tMerging  ('##s', '##iaagulyov,')\n","\tMerging  ('york', '##.')\n","\tMerging  ('(wh', '##o')\n","\tMerging  ('(who', '##)')\n","\tMerging  ('(who', '##u')\n","\tMerging  ('##t', '(whou')\n","\tMerging  ('##t(whou', '##t')\n","\tMerging  ('##i', '##t(whout')\n","\tMerging  ('##y', '##/')\n","\tMerging  ('##y/', '##u')\n","\tMerging  ('##l', '##y/u')\n","\tMerging  ('##o', '##ly/u')\n","\tMerging  ('p', '##oly/u')\n","\tMerging  ('poly/u', '##n')\n","\tMerging  ('poly/un', '##i')\n","\tMerging  ('poly/uni', '##v')\n","\tMerging  ('##d', '##/')\n","\tMerging  ('##d', '##d/')\n","Vocab: [' ', '#', '##&', '##(', '##(m', '##(p', '##(po', '##(por', '##(port', '##(porth', '##(porthm', '##(porthma', '##(porthmad', '##(porthmado', '##(porthmadog', '##(porthmadog,', '##)', '##),', '##).', '##*', '##,', '##-', '##--b', '##-b', '##-f', '##-j', '##-jj', '##-jja', '##-jjar', '##-jjar,', '##-q', '##-qu', '##-qua', '##-quak', '##-qual', '##-quali', '##-qualit', '##-quality', '##-quar', '##-quart', '##-yo', '##-yom', '##-yomo', '##-yomos', '##-yomoso', '##-yomosom', '##-yon', '##-yon-', '##-yon-y', '##-yong', '##.', '##/', '##/f', '##/h', '##:', '##;', '##>', '##>>', '##]', '##].', '##^', '##^^', '##a', '##a(cp', '##a(cpo', '##a(cpo),', '##a-jjar,', '##aagulyov,', '##aamâ”s', '##aay].', '##ack].', '##aff)', '##afÃ©', '##agulyov,', '##aid:', '##amâ”s', '##annsdd:', '##anukyovych', '##aq', '##aquÃ­', '##ar-yon-y', '##arl-yong', '##atione-bas', '##aw(f),', '##ax-fr', '##ay].', '##ayovi', '##b', '##b&', '##b(j', '##b(jp', '##b(jp)', '##b(w', '##b(wr', '##b(wr)', '##back].', '##bj', '##bjk', '##bjks', '##c', '##c/h', '##catione-bas', '##cc/h', '##chr-yomosom', '##ck', '##ck].', '##ckyo', '##ckyo,', '##clunngid:', '##d', '##d(f),', '##d/', '##d:', '##d:@d', '##daamâ”s', '##daay].', '##dd(f),', '##dd/', '##dd:', '##ddaamâ”s', '##ddd:', '##dhyoy', '##dngid:', '##dnâ€™t', '##drx-tb', '##ducatione-bas', '##d|', '##e', '##f', '##f--b', '##faq', '##ff', '##ff)', '##ff-f', '##ff-f,', '##fronup-t', '##fÃ©', '##g', '##gid:', '##gq', '##gqi', '##gqin', '##gqing', '##gqv', '##gslayovi', '##gulyov,', '##gw(f)', '##gy].', '##h', '##hfaq', '##hi(ch', '##hongqing', '##hr-yomosom', '##hyo', '##hyoy', '##i', '##i(ch', '##i(chi', '##iaagulyov,', '##id:', '##idnâ€™t', '##iionâ”.', '##illsk/c', '##ingy].', '##ione-bas', '##ionâ”.', '##it(whout', '##ix).', '##ix-h', '##ix-mo', '##ix-w', '##ix-y', '##izzy', '##iÅ†', '##j', '##k', '##k/', '##k/c', '##k/u', '##k].', '##kd:@d', '##koi(chi', '##kyov', '##kyovy', '##kyovyc', '##kyovych', '##l', '##l-q', '##l-yong', '##layovi', '##lingy].', '##llsk/c', '##llx-mo', '##llx-mor', '##llx-morr', '##llx-morri', '##llx-morris', '##llyow', '##lna-jjar,', '##loqx', '##lsk/c', '##ltss/f', '##lunngid:', '##lx-mo', '##ly/u', '##lyov,', '##lyow', '##m', '##me-b', '##mpup-to,', '##ms(c)', '##mâ”', '##mâ”s', '##n', '##na-jjar,', '##nckyo', '##nckyo,', '##nd|', '##ne-bas', '##ngid:', '##ngqing', '##ngy].', '##nngid:', '##nnsdd:', '##nnÃ©', '##nsdd:', '##ntys(mo', '##nu(f),', '##nukyovych', '##nup-t', '##nÃ©', '##nâ€™t', '##nâ”.', '##o', '##oaquÃ­', '##oex-vi', '##of--b', '##oi(chi', '##oly/u', '##one-bas', '##ongqing', '##ontys(mo', '##onup-t', '##onâ€™t', '##onâ”.', '##op/r', '##op|', '##oqx', '##orrkd:@d', '##ost-quak', '##p', '##p(f),', '##p/', '##p/r', '##plingy].', '##pplingy].', '##ps(c)', '##pup-to,', '##pyov', '##p|', '##q', '##qb', '##qu', '##quÃ­', '##qx', '##r', '##r-yomosom', '##r-yon-y', '##raq', '##rdaay].', '##rgqv', '##rgqvi', '##rgqvis', '##rgqvist', '##rkd:@d', '##rl-yong', '##ronup-t', '##rrkd:@d', '##rtannsdd:', '##rup-cl', '##rup-cla', '##rup-clas', '##rup-class', '##rx-tb', '##rzagulyov,', '##s', '##s(c)', '##s(m', '##s/f', '##sdd:', '##shfaq', '##siaagulyov,', '##siionâ”.', '##sk/c', '##slayovi', '##soex-vi', '##soff-p', '##soff-pi', '##soff-pin', '##soff-pinn', '##ss(c)', '##ss/f', '##ssiionâ”.', '##st-quak', '##suupyov', '##t', '##t(whou', '##t(whout', '##t-quak', '##tannsdd:', '##tione-bas', '##tss/f', '##tuff-f', '##turdaay].', '##tys(m', '##tys(mo', '##u', '##u(f),', '##u(wb)', '##ubj', '##ucatione-bas', '##udhyoy', '##udhyoyo', '##udhyoyon', '##udhyoyono', '##udhyoyono,', '##uff-f', '##ugslayovi', '##ugslayovia', '##ugslayovia.', '##uk/u', '##ukyovych', '##ultss/f', '##ultss/fi', '##ultss/fix', '##ultss/fixt', '##ultss/fixtu', '##ultss/fixtur', '##ulyov,', '##unngid:', '##upplingy].', '##upyov', '##urdaay].', '##uupyov', '##uÃ­', '##v', '##v/', '##vyo', '##vyon', '##vyonn', '##w', '##w(f)', '##w(f),', '##whi(ch', '##worrkd:@d', '##wworrkd:@d', '##wworrkd:@do', '##x', '##x).', '##x-', '##x-f', '##x-fr', '##x-h', '##x-m', '##x-mo', '##x-w', '##x-y', '##xup', '##y', '##y/', '##y/u', '##y].', '##ys(m', '##z', '##zagulyov,', '##zz', '##zzy', '##|', '##|.', '##Â©', '##Â¿', '##Â¿h', '##Â¿ha', '##Â¿hav', '##Ã¤', '##Ã©', '##Ã­', '##Ã³', '##Å†', '##â€™', '##â€™d', '##â€™s', '##â€™t', '##â€', '##â€¦', '##â”', '##â”.', '##â”s', '#b', '&', '(', '(aff)', '(c', '(c)', '(cck', '(ch', '(chn', '(chno', '(chnol', '(chnolo', '(chnolog', '(chnology', '(chnology,', '(cp', '(f', '(f)', '(f),', '(j', '(jv', '(jvs', '(jvs)', '(uk/u', '(uk/us', '(uk/usa', '(uk/usa)', '(w', '(wb', '(wb)', '(wh', '(who', '(who)', '(whou', ')', ',', '-', ':', '<UNK>', '>', '>>', '@', '@d', '[', '[back].', '[w', '`', '`l', '`lo', '`lon', '`long', 'a', 'a&', 'a*', 'a*s', 'adngid:', 'al-q', 'al-qa', 'alna-jjar,', 'asdd:', 'ashfaq', 'b', 'bb&', 'bb&t', 'bÃ¤', 'bÃ¤c', 'bÃ¤ck', 'bÃ¤ckh', 'c', 'cafÃ©', 'chongqing', 'd', 'd&', 'd&d', 'didnâ€™t', 'donâ€™t', 'dÃ­', 'dÃ­t', 'dÃ­tr', 'e', 'e-b', 'e-ba', 'e-bas', 'e-bo', 'e-boo', 'e-book', 'e-bor', 'e-bord', 'eff', 'equ', 'ev', 'ex', 'ex-', 'ex-b', 'ex-br', 'ex-bri', 'ex-brit', 'ex-briti', 'ex-britis', 'ex-british', 'ex-c', 'ex-ch', 'ex-cha', 'ex-chan', 'ex-chanc', 'ex-ci', 'ex-cia', 'ex-g', 'ex-gu', 'ex-v', 'ex-vi', 'ex-vic', 'exp', 'f', 'fizzy', 'fÃ©', 'fÃ©i', 'fÃ©in', 'g', 'h', 'i', 'iclunngid:', 'iqb', 'iqba', 'iqbal', 'iraq', 'iraq.', 'ishfaq', 'iâ€™', 'iâ€™m', 'j', 'joaquÃ­', 'joaquÃ­n', 'joaquÃ­n,', 'ju', 'k', 'l', 'm', 'mix).', 'montys(mo', 'montys(mon', 'montys(mont', 'montys(montg', 'montys(montgo', 'montys(montgom', 'mssiionâ”.', 'mÃ³', 'mÃ³r', 'n', 'niÅ†', 'niÅ†a', 'o', 'obj', 'of', 'off', 'off-', 'off-c', 'off-p', 'off-pu', 'off-put', 'off-putt', 'off-putti', 'off-puttin', 'off-putting', 'off-putting,', 'p', 'poly/u', 'poly/un', 'poly/uni', 'poly/univ', 'pop/r', 'pop/ro', 'pop/roc', 'pop/rock', 'post-quak', 'q', 'q:', 'qu', 'r', 'r&', 'r&b', 's', 'said:', 'sddaamâ”s', 'sillsk/c', 'sillsk/co', 'sillsk/con', 'sillsk/conn', 'six-h', 'six-ho', 'six-hou', 'six-hour', 'six-mo', 'six-mon', 'six-mont', 'six-month', 'six-w', 'six-y', 'stuff-f', 'stuff-fi', 'stuff-fil', 'stuff-fill', 'sturdaay].', 'subj', 'supplingy].', 't', 'tax-fr', 'th', 'tizzy', 'to', 'top|', 'u', 'uk', 'uk)', 'uk-b', 'uk-ba', 'uk-bas', 'ukâ€™', 'ukâ€™s', 'up', 'up-', 'up-c', 'up-cl', 'up-co', 'up-com', 'up-comi', 'up-comin', 'up-coming', 'up-t', 'up-to', 'up-to,', 'up-to-', 'up-to-d', 'up-to-da', 'up-to-dat', 'uq', 'uqr', 'uqrd', 'v', 'w', 'waq', 'waqt', 'wh', 'x', 'x-', 'x-r', 'x-ra', 'x-rat', 'x-ray', 'x-t', 'x-tb', 'xz', 'xzc', 'xzca', 'y', 'yo', 'yob', 'yobb', 'yobbi', 'yobbis', 'yobbish', 'yobo', 'yok', 'yoko', 'yokoh', 'yokoha', 'yokoham', 'yokohama', 'yokohama,', 'yor', 'york', 'york,', 'york.', 'you', 'yov', 'yov,', 'yovi', 'yow', 'z', '|', '|^^', '|Â©', '|Â©m', '|Â©mm', '|Â©mmv', '|Â©mmvi', '|Â©mmvii', '|Â©mmviii', 'Â©', 'Ã³', 'â€œ', 'â€œw', 'â€œwa', 'â€œwan', 'â€œwant', 'â€', 'â€¢', 'â”', 'â”a', 'â”ac', 'â”ach', 'â”achi']\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    tokenizer = WordPieceTokenizer(dataset.train, 700, do_tqdm = True, do_print=False)\n","    tokenizer.train()\n","    print(\"Vocab:\", tokenizer.vocab)"]},{"cell_type":"markdown","metadata":{"id":"sCR5Iq6oo6wd"},"source":["Now we can look at some tokenizations on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1739576045147,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"csdUptctgyVu","outputId":"7a8dd883-3315-487a-8be1-2766fdf5392d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: five killed in new mumbai building collapse at least five people have been killed in a building collapse in the indian city of mumbai - the second such incident in as many days.\n","Tokenization: ['f', '##i', '##v', '##e', ' ', 'k', '##i', '##l', '##l', '##e', '##d', ' ', 'i', '##n', ' ', 'n', '##e', '##w', ' ', 'm', '##u', '##m', '##b', '##a', '##i', ' ', 'b', '##u', '##i', '##l', '##d', '##i', '##n', '##g', ' ', 'c', '##o', '##l', '##l', '##a', '##p', '##s', '##e', ' ', 'a', '##t', ' ', 'l', '##e', '##a', '##s', '##t', ' ', 'f', '##i', '##v', '##e', ' ', 'p', '##e', '##o', '##p', '##l', '##e', ' ', 'h', '##a', '##v', '##e', ' ', 'b', '##e', '##e', '##n', ' ', 'k', '##i', '##l', '##l', '##e', '##d', ' ', 'i', '##n', ' ', 'a', ' ', 'b', '##u', '##i', '##l', '##d', '##i', '##n', '##g', ' ', 'c', '##o', '##l', '##l', '##a', '##p', '##s', '##e', ' ', 'i', '##n', ' ', 'th', '##e', ' ', 'i', '##n', '##d', '##i', '##a', '##n', ' ', 'c', '##i', '##t', '##y', ' ', 'of', ' ', 'm', '##u', '##m', '##b', '##a', '##i', ' ', '-', ' ', 'th', '##e', ' ', 's', '##e', '##c', '##o', '##n', '##d', ' ', 's', '##u', '##c', '##h', ' ', 'i', '##n', '##c', '##i', '##d', '##e', '##n', '##t', ' ', 'i', '##n', ' ', 'a', '##s', ' ', 'm', '##a', '##n', '##y', ' ', 'd', '##a', '##y', '##s', '##.']\n","Detokenization: five killed in new mumbai building collapse at least five people have been killed in a building collapse in the indian city of mumbai - the second such incident in as many days.\n","\n","Sentence: thank you and all best wishes.\n","Tokenization: ['th', '##a', '##n', '##k', ' ', 'you', ' ', 'a', '##n', '##d', ' ', 'a', '##l', '##l', ' ', 'b', '##e', '##s', '##t', ' ', 'w', '##i', '##s', '##h', '##e', '##s', '##.']\n","Detokenization: thank you and all best wishes.\n","\n","Sentence: indeed, ive only just married my british-born pakistani girlfriend (im white), and have found her family and culture to be very open and welcoming.\n","Tokenization: ['i', '##n', '##d', '##e', '##e', '##d', '##,', ' ', 'i', '##v', '##e', ' ', 'o', '##n', '##l', '##y', ' ', 'ju', '##s', '##t', ' ', 'm', '##a', '##r', '##r', '##i', '##e', '##d', ' ', 'm', '##y', ' ', 'b', '##r', '##i', '##t', '##i', '##s', '##h', '##-b', '##o', '##r', '##n', ' ', 'p', '##a', '##k', '##i', '##s', '##t', '##a', '##n', '##i', ' ', 'g', '##i', '##r', '##l', '##f', '##r', '##i', '##e', '##n', '##d', ' ', '(', '##i', '##m', ' ', 'wh', '##i', '##t', '##e', '##),', ' ', 'a', '##n', '##d', ' ', 'h', '##a', '##v', '##e', ' ', 'f', '##o', '##u', '##n', '##d', ' ', 'h', '##e', '##r', ' ', 'f', '##a', '##m', '##i', '##l', '##y', ' ', 'a', '##n', '##d', ' ', 'c', '##u', '##l', '##t', '##u', '##r', '##e', ' ', 'to', ' ', 'b', '##e', ' ', 'v', '##e', '##r', '##y', ' ', 'o', '##p', '##e', '##n', ' ', 'a', '##n', '##d', ' ', 'w', '##e', '##l', '##c', '##o', '##m', '##i', '##n', '##g', '##.']\n","Detokenization: indeed, ive only just married my british-born pakistani girlfriend (im white), and have found her family and culture to be very open and welcoming.\n","\n","Sentence: unfortunately it is extremely hard to hear such news although there are thousands of such cases in pakistan.\n","Tokenization: ['u', '##n', '##f', '##o', '##r', '##t', '##u', '##n', '##a', '##t', '##e', '##l', '##y', ' ', 'i', '##t', ' ', 'i', '##s', ' ', 'ex', '##t', '##r', '##e', '##m', '##e', '##l', '##y', ' ', 'h', '##a', '##r', '##d', ' ', 'to', ' ', 'h', '##e', '##a', '##r', ' ', 's', '##u', '##c', '##h', ' ', 'n', '##e', '##w', '##s', ' ', 'a', '##l', '##t', '##h', '##o', '##u', '##g', '##h', ' ', 'th', '##e', '##r', '##e', ' ', 'a', '##r', '##e', ' ', 'th', '##o', '##u', '##s', '##a', '##n', '##d', '##s', ' ', 'of', ' ', 's', '##u', '##c', '##h', ' ', 'c', '##a', '##s', '##e', '##s', ' ', 'i', '##n', ' ', 'p', '##a', '##k', '##i', '##s', '##t', '##a', '##n', '##.']\n","Detokenization: unfortunately it is extremely hard to hear such news although there are thousands of such cases in pakistan.\n","\n","Sentence: the buy-to-let market is pressurising the housing market and new development does not appear to be the best of options in this crowded country with poor road infrastructure and public transport.\n","Tokenization: ['th', '##e', ' ', 'b', '##u', '##y', '##-', '##t', '##o', '##-', '##l', '##e', '##t', ' ', 'm', '##a', '##r', '##k', '##e', '##t', ' ', 'i', '##s', ' ', 'p', '##r', '##e', '##s', '##s', '##u', '##r', '##i', '##s', '##i', '##n', '##g', ' ', 'th', '##e', ' ', 'h', '##o', '##u', '##s', '##i', '##n', '##g', ' ', 'm', '##a', '##r', '##k', '##e', '##t', ' ', 'a', '##n', '##d', ' ', 'n', '##e', '##w', ' ', 'd', '##e', '##v', '##e', '##l', '##o', '##p', '##m', '##e', '##n', '##t', ' ', 'd', '##o', '##e', '##s', ' ', 'n', '##o', '##t', ' ', 'a', '##p', '##p', '##e', '##a', '##r', ' ', 'to', ' ', 'b', '##e', ' ', 'th', '##e', ' ', 'b', '##e', '##s', '##t', ' ', 'of', ' ', 'o', '##p', '##t', '##i', '##o', '##n', '##s', ' ', 'i', '##n', ' ', 'th', '##i', '##s', ' ', 'c', '##r', '##o', '##w', '##d', '##e', '##d', ' ', 'c', '##o', '##u', '##n', '##t', '##r', '##y', ' ', 'w', '##i', '##t', '##h', ' ', 'p', '##o', '##o', '##r', ' ', 'r', '##o', '##a', '##d', ' ', 'i', '##n', '##f', '##r', '##a', '##s', '##t', '##r', '##u', '##c', '##t', '##u', '##r', '##e', ' ', 'a', '##n', '##d', ' ', 'p', '##u', '##b', '##l', '##i', '##c', ' ', 't', '##r', '##a', '##n', '##s', '##p', '##o', '##r', '##t', '##.']\n","Detokenization: the buy-to-let market is pressurising the housing market and new development does not appear to be the best of options in this crowded country with poor road infrastructure and public transport.\n","\n","Sentence: the sbvt ad is yet another example of what the bush camp has been doing to denigrate veterans who oppose george bush - like senator mccain for instance.\n","Tokenization: ['th', '##e', ' ', 's', '##b', '##v', '##t', ' ', 'a', '##d', ' ', 'i', '##s', ' ', 'y', '##e', '##t', ' ', 'a', '##n', '##o', '##t', '##h', '##e', '##r', ' ', 'ex', '##a', '##m', '##p', '##l', '##e', ' ', 'of', ' ', 'wh', '##a', '##t', ' ', 'th', '##e', ' ', 'b', '##u', '##s', '##h', ' ', 'c', '##a', '##m', '##p', ' ', 'h', '##a', '##s', ' ', 'b', '##e', '##e', '##n', ' ', 'd', '##o', '##i', '##n', '##g', ' ', 'to', ' ', 'd', '##e', '##n', '##i', '##g', '##r', '##a', '##t', '##e', ' ', 'v', '##e', '##t', '##e', '##r', '##a', '##n', '##s', ' ', 'wh', '##o', ' ', 'o', '##p', '##p', '##o', '##s', '##e', ' ', 'g', '##e', '##o', '##r', '##g', '##e', ' ', 'b', '##u', '##s', '##h', ' ', '-', ' ', 'l', '##i', '##k', '##e', ' ', 's', '##e', '##n', '##a', '##t', '##o', '##r', ' ', 'm', '##c', '##c', '##a', '##i', '##n', ' ', 'f', '##o', '##r', ' ', 'i', '##n', '##s', '##t', '##a', '##n', '##c', '##e', '##.']\n","Detokenization: the sbvt ad is yet another example of what the bush camp has been doing to denigrate veterans who oppose george bush - like senator mccain for instance.\n","\n","Sentence: those are all people that have money, mr schwarzenegger said when asked about the strike.\n","Tokenization: ['th', '##o', '##s', '##e', ' ', 'a', '##r', '##e', ' ', 'a', '##l', '##l', ' ', 'p', '##e', '##o', '##p', '##l', '##e', ' ', 'th', '##a', '##t', ' ', 'h', '##a', '##v', '##e', ' ', 'm', '##o', '##n', '##e', '##y', '##,', ' ', 'm', '##r', ' ', 's', '##c', '##h', '##w', '##a', '##r', '##z', '##e', '##n', '##e', '##g', '##g', '##e', '##r', ' ', 's', '##a', '##i', '##d', ' ', 'wh', '##e', '##n', ' ', 'a', '##s', '##k', '##e', '##d', ' ', 'a', '##b', '##o', '##u', '##t', ' ', 'th', '##e', ' ', 's', '##t', '##r', '##i', '##k', '##e', '##.']\n","Detokenization: those are all people that have money, mr schwarzenegger said when asked about the strike.\n","\n","Sentence: lead research professor allen wilcox said: there apparently are biological factors promoting intercourse during a womans six fertile days, whether she wants a baby or not.\n","Tokenization: ['l', '##e', '##a', '##d', ' ', 'r', '##e', '##s', '##e', '##a', '##r', '##c', '##h', ' ', 'p', '##r', '##o', '##f', '##e', '##s', '##s', '##o', '##r', ' ', 'a', '##l', '##l', '##e', '##n', ' ', 'w', '##i', '##l', '##c', '##o', '##x', ' ', 'said:', ' ', 'th', '##e', '##r', '##e', ' ', 'a', '##p', '##p', '##a', '##r', '##e', '##n', '##t', '##l', '##y', ' ', 'a', '##r', '##e', ' ', 'b', '##i', '##o', '##l', '##o', '##g', '##i', '##c', '##a', '##l', ' ', 'f', '##a', '##c', '##t', '##o', '##r', '##s', ' ', 'p', '##r', '##o', '##m', '##o', '##t', '##i', '##n', '##g', ' ', 'i', '##n', '##t', '##e', '##r', '##c', '##o', '##u', '##r', '##s', '##e', ' ', 'd', '##u', '##r', '##i', '##n', '##g', ' ', 'a', ' ', 'w', '##o', '##m', '##a', '##n', '##s', ' ', 's', '##i', '##x', ' ', 'f', '##e', '##r', '##t', '##i', '##l', '##e', ' ', 'd', '##a', '##y', '##s', '##,', ' ', 'wh', '##e', '##t', '##h', '##e', '##r', ' ', 's', '##h', '##e', ' ', 'w', '##a', '##n', '##t', '##s', ' ', 'a', ' ', 'b', '##a', '##b', '##y', ' ', 'o', '##r', ' ', 'n', '##o', '##t', '##.']\n","Detokenization: lead research professor allen wilcox said: there apparently are biological factors promoting intercourse during a womans six fertile days, whether she wants a baby or not.\n","\n","Sentence: put offshore skills to work on marine energy.\n","Tokenization: ['p', '##u', '##t', ' ', 'of', '##f', '##s', '##h', '##o', '##r', '##e', ' ', 's', '##k', '##i', '##l', '##l', '##s', ' ', 'to', ' ', 'w', '##o', '##r', '##k', ' ', 'o', '##n', ' ', 'm', '##a', '##r', '##i', '##n', '##e', ' ', 'e', '##n', '##e', '##r', '##g', '##y', '##.']\n","Detokenization: put offshore skills to work on marine energy.\n","\n","Sentence: jan peter balkenende (left) has been out trying to persuade voters the trouble is, said the woman standing next to him, were trying to persuade people who dont want to be persuaded.\n","Tokenization: ['j', '##a', '##n', ' ', 'p', '##e', '##t', '##e', '##r', ' ', 'b', '##a', '##l', '##k', '##e', '##n', '##e', '##n', '##d', '##e', ' ', '(', '##l', '##e', '##f', '##t', '##)', ' ', 'h', '##a', '##s', ' ', 'b', '##e', '##e', '##n', ' ', 'o', '##u', '##t', ' ', 't', '##r', '##y', '##i', '##n', '##g', ' ', 'to', ' ', 'p', '##e', '##r', '##s', '##u', '##a', '##d', '##e', ' ', 'v', '##o', '##t', '##e', '##r', '##s', ' ', 'th', '##e', ' ', 't', '##r', '##o', '##u', '##b', '##l', '##e', ' ', 'i', '##s', '##,', ' ', 's', '##a', '##i', '##d', ' ', 'th', '##e', ' ', 'w', '##o', '##m', '##a', '##n', ' ', 's', '##t', '##a', '##n', '##d', '##i', '##n', '##g', ' ', 'n', '##e', '##x', '##t', ' ', 'to', ' ', 'h', '##i', '##m', '##,', ' ', 'w', '##e', '##r', '##e', ' ', 't', '##r', '##y', '##i', '##n', '##g', ' ', 'to', ' ', 'p', '##e', '##r', '##s', '##u', '##a', '##d', '##e', ' ', 'p', '##e', '##o', '##p', '##l', '##e', ' ', 'wh', '##o', ' ', 'd', '##o', '##n', '##t', ' ', 'w', '##a', '##n', '##t', ' ', 'to', ' ', 'b', '##e', ' ', 'p', '##e', '##r', '##s', '##u', '##a', '##d', '##e', '##d', '##.']\n","Detokenization: jan peter balkenende (left) has been out trying to persuade voters the trouble is, said the woman standing next to him, were trying to persuade people who dont want to be persuaded.\n","\n"]}],"source":["### DO NOT EDIT ###\n","\n","if __name__=='__main__':\n","    for sent in random.sample(dataset.test, 10):\n","        print('Sentence:', sent)\n","        toks = tokenizer.tokenize(sent)\n","        print('Tokenization:', toks)\n","        print('Detokenization:', tokenizer.detokenize(toks))\n","        print()"]},{"cell_type":"markdown","metadata":{"id":"ZKt-WVr6OtP4"},"source":["# Part 2: Language Models [72 points]\n","\n","Here, you will train some <b>n-gram language models</b> on WikiText-2, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following paper: https://arxiv.org/pdf/1609.07843v1.pdf. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2."]},{"cell_type":"markdown","metadata":{"id":"cN2Ja8MNP4qS"},"source":["## Download & preprocess the data\n","\n","To make your models more robust, it is necessary to perform some basic preprocessing on the corpora. <i>You do not need to edit this code.</i>\n","\n","* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp;In this homework, we are interested in modeling individual sentences, rather than longer chunks of text such as paragraphs or documents. The WikiTest dataset provides paragraphs; thus, we provide a simple method to identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n","\n","* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). These markers will allow your models to generate sentences that have realistic beginnings and endings.\n","\n","* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown words in the test corpora, all words that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`) before estimating your models. The WikiText dataset has already done this, and you can read about the method in the paper above. When unknown words are encountered in the test corpus, they should be treated as that special token instead.\n","\n","We provide you with preprocessing code here, and you should not modify it.\n","\n","After the preprocessing, you may assume that all words in the test set appear in the training set, as this code has already replaced the unseen tokens with `<UNK>`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCkrUjKEBrNp"},"outputs":[],"source":["### DO NOT EDIT ###\n","\n","# Constants (feel free to use these in your code, but do not change them)\n","START = \"<s>\"   # Start-of-sentence token\n","END = \"</s>\"    # End-of-sentence-token\n","UNK = \"<UNK>\"   # Unknown word token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUdZstjH30DL"},"outputs":[],"source":["### DO NOT EDIT ###\n","import os\n","import random\n","import sys\n","from urllib.request import urlretrieve\n","\n","def preprocess(data, vocab=None):\n","    final_data = []\n","    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n","    for paragraph in data:\n","        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()]\n","        if vocab is not None:\n","            paragraph = [x if x in vocab else UNK for x in paragraph]\n","        if paragraph == [] or paragraph.count('=') >= 2: continue\n","        sen = []\n","        prev_punct, prev_quot = False, False\n","        for word in paragraph:\n","            if prev_quot:\n","                if word[0] not in lowercase:\n","                    final_data.append(sen)\n","                    sen = []\n","                    prev_punct, prev_quot = False, False\n","            if prev_punct:\n","                if word == '\"':\n","                    prev_punct, prev_quot = False, True\n","                else:\n","                    if word[0] not in lowercase:\n","                        final_data.append(sen)\n","                        sen = []\n","                        prev_punct, prev_quot = False, False\n","            if word in {'.', '?', '!'}: prev_punct = True\n","            sen += [word]\n","        if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n","        final_data.append(sen)\n","    vocab_was_none = vocab is None\n","    if vocab is None:\n","        vocab = set()\n","    for i in range(len(final_data)):\n","        final_data[i] = [START] + final_data[i] + [END]\n","        if vocab_was_none:\n","            for word in final_data[i]:\n","                vocab.add(word)\n","    return final_data, vocab\n","\n","def getDataset():\n","    splits = ['train', 'valid']\n","    datasets = []\n","    path = './{}.txt'\n","    url = 'https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/{}.txt'\n","    for split in splits:\n","        if os.path.exists(path.format(split)):\n","            print(f\"{split} dataset already downloaded\")\n","        else:\n","            filename = f'{split}.txt'\n","            urlretrieve(url.format(split), filename)\n","        datasets.append(open(f'{split}.txt').read().split('\\n'))\n","    train_dataset, vocab = preprocess(datasets[0])\n","    test_dataset, _ = preprocess(datasets[1], vocab)\n","    return train_dataset, test_dataset\n","\n","if __name__ == '__main__':\n","    train_dataset, test_dataset = getDataset()"]},{"cell_type":"markdown","metadata":{"id":"mSFJ07ELGUMh"},"source":["Run the next cell to see 10 random sentences of the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1739826490516,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"swPwiHBHDDkT","outputId":"6eb2f277-b47e-40e9-9686-304f9e7ca0de"},"outputs":[{"output_type":"stream","name":"stdout","text":["['<s>', 'Philadelphia', ':', 'University', 'of', 'Pennsylvania', 'Press', '.', '</s>']\n","['<s>', 'It', 'is', 'written', 'as', '<UNK>', '(', '<UNK>', ')', 'in', 'Biblical', 'Aramaic', 'and', '<UNK>', '(', '<UNK>', ')', 'in', 'Syriac', 'as', 'used', 'by', 'the', 'Assyrian', 'Church', ',', 'both', 'meaning', 'simply', '\"', 'God', '\"', '.', '</s>']\n","['<s>', 'The', 'medley', 'starts', 'off', 'with', '\"', 'Vintage', 'Clothes', '\"', ',', 'which', 'McCartney', '\"', 'sat', 'down', 'one', 'day', '\"', 'to', 'write', ',', 'that', 'was', '\"', 'looking', 'back', ',', '[', 'and', ']', 'looking', 'back', '.', '\"', '</s>']\n","['<s>', 'Robin', '<UNK>', ',', 'a', 'professor', 'of', 'strategic', 'management', ',', 'believes', 'that', 'Isabella', \"'s\", 'advice', 'and', 'guidance', 'on', 'household', 'management', 'can', 'also', 'be', 'applied', 'to', 'business', 'management', ',', 'and', 'her', 'lessons', 'on', 'the', 'subject', 'have', 'stood', 'the', 'test', 'of', 'time', 'better', 'than', 'some', 'of', 'her', 'advice', 'on', 'cooking', 'or', '<UNK>', '.', '</s>']\n","['<s>', 'It', 'was', 'rare', 'for', 'contemporary', 'portraits', 'to', 'show', 'women', 'reading', ',', 'and', 'if', 'the', 'model', 'herself', 'could', 'read', 'then', 'she', 'was', 'likely', 'from', 'a', 'noble', 'family', '.', '</s>']\n","['<s>', 'Across', 'southern', 'China', ',', 'the', 'typhoon', 'damaged', 'over', '10', 'million', 'hectares', '(', '25', 'million', 'acres', ')', 'of', 'crop', 'fields', '.', '</s>']\n","['<s>', 'There', 'are', '13', 'â€“', '18', '<UNK>', 'and', '16', 'â€“', '22', '<UNK>', '.', '</s>']\n","['<s>', 'Bishop', 'Fulton', 'J.', 'Sheen', \"'s\", 'devotional', 'program', 'Life', 'Is', 'Worth', 'Living', 'went', 'up', 'against', 'Milton', 'Berle', 'in', 'many', 'cities', ',', 'and', 'was', 'the', 'first', 'show', 'to', 'compete', 'successfully', 'in', 'the', 'ratings', 'against', '\"', 'Mr.', 'Television', '\"', '.', '</s>']\n","['<s>', 'In', 'the', 'months', 'before', 'Domoina', 'struck', ',', 'dry', 'conditions', 'persisted', 'across', 'southeastern', 'Africa', '.', '</s>']\n","['<s>', 'After', 'the', 'Fifth', 'Crusade', 'ended', '\"', 'in', 'colossal', 'and', '<UNK>', 'failure', '\"', ',', 'John', 'returned', 'to', 'his', 'kingdom', '.', '</s>']\n"]}],"source":["if __name__ == '__main__':\n","    for x in random.sample(train_dataset, 10):\n","        print (x)"]},{"cell_type":"markdown","metadata":{"id":"YM6hNHMqTMt2"},"source":["## The LanguageModel Class\n","\n","You will implement 4 types of language models: a <b>unigram</b> model, a <b>smoothed unigram</b> model, a <b>bigram</b> model, and a <b>smoothed bigram</b> model. Each of the models is worth 25 points and extends the following base class. <b>You do not need to implement anything in this class</b>; you will instead implement each of the following methods in the relevant subclass:\n","\n","* <b>`__init__(self, trainCorpus)`</b>: Train the language model on `trainCorpus`. This will involve calculating relative frequency estimates according to the type of model you're implementing.\n","\n","* <b>`generateSentence(self)`</b>: <b>[4 points]</b> Return a sentence that is generated by the language model. It should be a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>, where each <TT>w<sup>(i)</sup></TT> is a word in your vocabulary (including <TT>&lt;UNK&gt;</TT> but exlcuding <TT>&lt;s&gt;</TT> and <TT>&lt;&sol;s&gt;</TT>). You may assume that <TT>&lt;s&gt;</TT> starts each sentence (with probability $1$). The following words <TT>w<sup>(1)</sup></TT>, ... , <TT>w<sup>(n)</sup></TT>, <TT>&lt;&sol;s&gt;</TT> are generated according to your language model's distribution. Note that the number of words <TT>n</TT> is not fixed; instead, you should stop the sentence as soon as you generate the stop token <TT>&lt;&sol;s&gt;</TT>.\n","\n","* <b>`getSentenceLogProbability(self, sentence)`</b>: <b>[7 points]</b> Return the <em> logarithm of the probability</em> of <TT>sentence</TT>, which is again a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>. You should use the natural logarithm $-$ that is, the base-<em>e</em> logarithm. See the note below about performing your calculations in log space.\n","\n","* <b>`getCorpusPerplexity(self, testCorpus)`</b>: <b>[7 points]</b> You need to compute the perplexity (normalized inverse log probability) of `testCorpus` according to your model. For a corpus $W$ with $N$ words and a bigram model, Jurafsky and Martin tells you to compute perplexity as follows:\n","\n","$$Perplexity(W) = \\Big [ \\prod_{i=1}^N \\frac{1}{P(w^{(i)}|w^{(i-1)})} \\Big ]^{1/N}$$\n","\n","<b>Implementation Hint:</b> In order to avoid underflow, you will likely need to do all of your calculations in log-space. That is, instead of multiplying probabilities, you should add the logarithms of the probabilities and exponentiate the result:\n","\n","$$\\prod_{i=1}^N P(w^{(i)}|w^{(i-1)}) = \\exp\\Big (\\sum_{i=1}^N \\log P(w^{(i)}|w^{(i-1)}) \\Big ) $$\n","\n","Using this property should help you in your implementation of `generateSentence(self)` and `getCorpusPerplexity(self, testCorpus)`.\n","\n","Feel free to implement helper methods as you wish (either in the base class or in the subclases). <b>But be sure not to change the function signatures of the provided methods</b> (i.e. the function and argument names), or else the autograder will fail."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"code","id":"uKO6dHNSS45P"},"outputs":[],"source":["import math\n","import random\n","from collections import defaultdict\n","\n","class LanguageModel(object):\n","    def naturalLog(self, prob):\n","      if not prob:\n","        return []\n","\n","      log_data = []\n","      for x in prob:\n","        if x <= 0:\n","          return [] # Return empty list if non-positive number is found\n","        log_data.append(math.log(x))\n","      return log_data\n","\n","    def unigram_count(self, d, word):\n","      if word not in d.keys():\n","          d[word] = 1\n","      else:\n","          d[word] += 1\n","\n","\n","    def bigram_count(self, d, word1, word2):\n","      if word1 not in d.keys():\n","        d[word1] = {word2: 1}\n","      else:\n","        if word2 in d[word1].keys():\n","          d[word1][word2] += 1\n","        else:\n","          d[word1][word2] = 1\n","\n","\n","    def __init__(self, trainCorpus):\n","        '''\n","        Initialize and train the model (i.e. estimate the model's underlying probability\n","        distribution from the training corpus.)\n","        '''\n","\n","        return\n","\n","    def generateSentence(self):\n","        '''\n","        Generate a sentence by drawing words according to the model's probability distribution.\n","        Note: Think about how to set the length of the sentence in a principled way.\n","        '''\n","\n","        raise NotImplementedError(\"Implement generateSentence in each subclass.\")\n","\n","    def getSentenceLogProbability(self, sentence):\n","        '''\n","        Calculate the log probability of the sentence provided.\n","        '''\n","\n","        raise NotImplementedError(\"Implement getSentenceProbability in each subclass.\")\n","\n","    def getCorpusPerplexity(self, testCorpus):\n","        '''\n","        Calculate the perplexity of the corpus provided.\n","        '''\n","\n","        raise NotImplementedError(\"Implement getCorpusPerplexity in each subclass.\")\n","\n","    def printSentences(self, n):\n","        '''\n","        Prints n sentences generated by your model.\n","        '''\n","\n","        ### DO NOT EDIT ###\n","        for i in range(n):\n","            sent = self.generateSentence()\n","            prob = self.getSentenceLogProbability(sent)\n","            print('Log Probability:', prob , '\\tSentence:',sent)"]},{"cell_type":"markdown","metadata":{"id":"Bf15l6f3etMV"},"source":["## <font color='red'>TODO:</font> Unigram Model [18 points]\n","\n","Here, you will implement each of the 4 functions described above for an <b>unsmoothed unigram</b> model. The probability distribution of a word is given by $\\hat P(w)$.\n","\n","<font color='green'><b>Hints:</b></font>\n","* <font color='green'>You should use a <b>dictionary</b> to map tokens to their unigram counts.</font>\n","* <font color='green'>Since you never want to generate the start-of-sentence token `<s>`, you should <b>not</b> include it in your counts.</font>\n","* <font color='green'>In general, avoid checking for membership in a list (i.e. avoid `x in lst`). Instead, use sets or dictionaries for this purpose $-$ membership checks are much faster on these data structures.</font>\n","* <font color='green'>Do <b>not</b> modify the training or test corpora by using `.append(...)` or `.pop(...)` on them. This will cause unexpected behavior in the autograder tests, which do not expect you to be changing the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2uZdMsqeuf2"},"outputs":[],"source":["class UnigramModel(LanguageModel):\n","    def __init__(self, trainCorpus):\n","\n","        ### TODO ###\n","        self.train_corpus = trainCorpus\n","        token_dict = self.token_dict()\n","        self.p_w = self.token_prob(token_dict)\n","\n","\n","    def token_dict(self):\n","        t_d = {}\n","\n","        for sentence in self.train_corpus:\n","            for word in sentence[1:]:\n","                super().unigram_count(t_d, word)\n","\n","        return t_d\n","\n","    def token_prob(self, token_dict):\n","\n","        N = sum(token_dict.values())\n","        p_w ={}\n","        for key, value in token_dict.items():\n","          p_w[key] = value/N\n","\n","        return p_w\n","\n","    def generateSentence(self):\n","\n","        sentence = []\n","        sentence.append(START)\n","\n","        word = ''\n","        while word != END:\n","          word = random.choices(list(self.p_w.keys()), weights = list(self.p_w.values()), k=1)[0]\n","          sentence.append(word)\n","\n","\n","        return sentence\n","\n","\n","\n","\n","    def getSentenceLogProbability(self, sentence):\n","\n","        probarray =[]\n","        for word in sentence[1:]:\n","          #print(word)\n","          probarray.append(self.p_w[word])\n","\n","        #print(probarray)\n","        logarray = super().naturalLog(probarray)\n","\n","        ### TODO ###\n","        return sum(logarray)\n","\n","    def getCorpusPerplexity(self, testCorpus):\n","\n","        ### TODO ###\n","        N = 0\n","        logprob = 0\n","\n","\n","        for sentence in testCorpus:\n","          #print(sentence)\n","          N += (len(sentence)-1)\n","          logprob += self.getSentenceLogProbability(sentence)\n","\n","        perplexity = math.exp(-1/N * logprob)\n","        return perplexity\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8c6zfDsT-GrU"},"source":["We provide you with a testing function that uses very simple training & test corpora (you could compute probability/perplexity by hand if you wanted to). This is just a <b>sanity check</b> $-$ passing this test does not guarantee you a perfect score in the autograder; this is simply to help you debug your model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1739826500505,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"o8uRRgLi6IVt","outputId":"eabe20af-8db3-458f-ac1d-fed5e5dbefd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- TEST: generateSentence() ---\n","Test generateSentence() passed!\n","\n","--- TEST: getSentenceLogProbability(...) ---\n","Correct log prob.: -19.08542845 \tYour log prob.: -19.08542845 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n","Correct log prob.: -114.5001481799 \tYour log prob.: -114.5001481799 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n","Correct log prob.: -108.7963657053 \tYour log prob.: -108.7963657053 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n","Correct log prob.: -53.6727664115 \tYour log prob.: -53.6727664115 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n","Correct log prob.: -55.4645258807 \tYour log prob.: -55.4645258807 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n","Test getSentenceProbability(...) passed!\n","\n","--- TEST: getCorpusPerplexity(...) ---\n","Correct train perp.: 41.3308239726 \tYour train perp.: 41.3308239726 \t PASSED\n","Correct test perp.: 38.0122981569 \tYour test perp.: 38.0122981569 \t PASSED\n","Test getCorpusPerplexity(...) passed!\n"]}],"source":["def sanityCheck(model_type):\n","    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n","\n","    #\tRead in the test corpus\n","    train_corpus = [\"By the Late Classic , a network of few <unk> ( few <unk> ) linked various parts of the city , running for several kilometres through its urban core .\",\n","    \"Few people realize how difficult it was to create Sonic 's graphics engine , which allowed for the incredible rate of speed the game 's known for .\"]\n","    test_corpus = [\"Classic few parts of the game allowed for few <unk> <unk> incredible city .\",\n","                   \"Few <unk> realize the difficult network , which linked the game to Sonic .\"]\n","    train_corpus, _ = preprocess(train_corpus)\n","    test_corpus, _ = preprocess(test_corpus)\n","    sentence = preprocess([\"Sonic was difficult .\"])[0][0]\n","\n","    # These are the correct answers (don't change them!)\n","    if model_type == \"unigram\":\n","       senprobs = [-19.08542845, -114.5001481799, -108.7963657053, -53.6727664115, -55.4645258807]\n","       trainPerp, testPerp = 41.3308239726, 38.0122981569\n","       model = UnigramModel(train_corpus)\n","    elif model_type == \"smoothed-unigram\":\n","       senprobs = [-19.0405293515, -115.3479413049, -108.9114348746, -54.8190029616, -55.8122547346]\n","       trainPerp, testPerp = 41.9994393615, 39.9531928383\n","       model = SmoothedUnigramModel(train_corpus)\n","    elif model_type == \"bigram\":\n","       senprobs = [-float('inf'), -10.3450917073, -9.2464794186, -float('inf'), -float('inf')]\n","       trainPerp, testPerp = 1.3861445461, float('inf')\n","       model = BigramModel(train_corpus)\n","    elif model_type == \"smoothed-bigram\":\n","       senprobs = [-16.355820202, -76.0026113319, -74.2346475108, -47.2885760372, -51.2730261907]\n","       trainPerp, testPerp = 12.2307627397, 26.7193157699\n","       model = SmoothedBigramModelAD(train_corpus)\n","    else: assert False, 'Invalid model_type'\n","\n","    print(\"--- TEST: generateSentence() ---\")\n","    modelSen = model.generateSentence()\n","    senTestPassed = isinstance(modelSen, list) and len(modelSen) > 1 and isinstance(modelSen[0], str)\n","    if senTestPassed:\n","        print (\"Test generateSentence() passed!\")\n","    else:\n","        print (\"Test generateSentence() failed; did not return a list of strings...\")\n","\n","    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n","    sentences = [sentence, *train_corpus, *test_corpus]\n","    failed = 0\n","    for i in range(len(sentences)):\n","        sen, correct_prob = sentences[i], senprobs[i]\n","        prob = round(model.getSentenceLogProbability(sen), 10)\n","        print(\"Correct log prob.:\", correct_prob, '\\tYour log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n","        if prob != correct_prob: failed+=1\n","\n","    if not failed:\n","        print (\"Test getSentenceProbability(...) passed!\")\n","    else:\n","        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n","\n","    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n","    train_perp = round(model.getCorpusPerplexity(train_corpus), 10)\n","    test_perp = round(model.getCorpusPerplexity(test_corpus), 10)\n","\n","    print(\"Correct train perp.:\", trainPerp, '\\tYour train perp.:', train_perp, '\\t', 'PASSED' if trainPerp == train_perp else 'FAILED')\n","    print(\"Correct test perp.:\", testPerp, '\\tYour test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n","    train_passed, test_passed = train_perp == trainPerp, test_perp == testPerp\n","    if train_passed and test_passed:\n","        print(\"Test getCorpusPerplexity(...) passed!\")\n","    else:\n","        print(\"Test getCorpusPerplexity(...) failed on\", \"the training corpus and the testing corpus...\" if not train_passed and not test_passed else \"the testing corpus...\" if not test_passed else \"the training corpus...\")\n","\n","if __name__=='__main__':\n","    sanityCheck('unigram')"]},{"cell_type":"markdown","metadata":{"id":"-Z8h9U63AkiG"},"source":["Next, we provide you with another <b>sanity check</b> that trains your model on the *entire* training set, and tests your functions on a small corpus (10 sentences) of *real* test data.\n","\n","If your code is inefficient, you will likely see that this cell is taking too long. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1650,"status":"ok","timestamp":1739826505356,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"_mCa6zasN-0j","outputId":"75df42cf-2580-4760-f9f8-db67714ccc76"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- TEST: getSentenceLogProbability(...) ---\n","Correct log prob.: -80.7782190984 \tYour log prob.: -80.7782190984 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n","Correct log prob.: -174.4769654449 \tYour log prob.: -174.4769654449 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n","Correct log prob.: -136.455148267 \tYour log prob.: -136.455148267 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n","Correct log prob.: -225.5890741503 \tYour log prob.: -225.5890741503 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n","Correct log prob.: -719.0142129846 \tYour log prob.: -719.0142129846 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n","Correct log prob.: -236.350443633 \tYour log prob.: -236.350443633 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n","Correct log prob.: -126.0056604204 \tYour log prob.: -126.0056604204 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n","Correct log prob.: -47.3424655612 \tYour log prob.: -47.3424655612 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n","Correct log prob.: -47.7775372096 \tYour log prob.: -47.7775372096 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n","Correct log prob.: -138.8159941929 \tYour log prob.: -138.8159941929 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n","Test getSentenceProbability(...) passed!\n","\n","--- TEST: getCorpusPerplexity(...) ---\n","Correct test perp.: 881.0132848704 \tYour test perp.: 881.0132848704 \t PASSED\n","Test getCorpusPerplexity(...) passed!\n"]}],"source":["def sanityCheckFullDataset(model_type):\n","    model = UnigramModel(train_dataset)\n","    idxes = list(range(75,7500, 800))\n","    small_test_corpus = [test_dataset[idx] for idx in idxes]\n","    if model_type == 'unigram':\n","        senprobs = [-80.7782190984, -174.4769654449, -136.455148267, -225.5890741503, -719.0142129846, -236.350443633, -126.0056604204, -47.3424655612, -47.7775372096, -138.8159941929]\n","        testPerp = 881.0132848704\n","        model = UnigramModel(train_dataset)\n","    elif model_type == 'smoothed-unigram':\n","        senprobs = [-80.8423009715, -174.5131424172, -136.3181234818, -225.357454098, -719.1543898871, -236.6682968913, -126.1965419509, -47.4369338195, -47.7692144935, -138.542462715]\n","        testPerp = 881.6105352831\n","        model = SmoothedUnigramModel(train_dataset)\n","    elif model_type == 'bigram':\n","        senprobs = [-float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -32.1502020637, -float('inf'), -float('inf')]\n","        testPerp = float ('inf')\n","        model = BigramModel(train_dataset)\n","    elif model_type == 'smoothed-bigram':\n","        senprobs = [-61.3754065648, -141.9754903887, -107.0849366076, -168.4944718788, -619.9409055374, -195.8159911677, -86.3762008156, -32.4764801981, -48.124714509, -124.687107856]\n","        testPerp = 261.4247123506\n","        model = SmoothedBigramModelAD(train_dataset)\n","    else: assert False, 'Invalid model_type'\n","    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n","    failed = 0\n","    for i in range(len(small_test_corpus)):\n","        sen, correct_prob = small_test_corpus[i], senprobs[i]\n","        prob = round(model.getSentenceLogProbability(sen), 10)\n","        print(\"Correct log prob.:\", correct_prob, '\\tYour log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n","        if prob != correct_prob: failed+=1\n","\n","    if not failed:\n","        print (\"Test getSentenceProbability(...) passed!\")\n","    else:\n","        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n","\n","    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n","    test_perp = round(model.getCorpusPerplexity(small_test_corpus), 10)\n","\n","    print(\"Correct test perp.:\", testPerp, '\\tYour test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n","    test_passed = test_perp == testPerp\n","    if test_passed:\n","        print(\"Test getCorpusPerplexity(...) passed!\")\n","    else:\n","        print(\"Test getCorpusPerplexity(...) failed on the testing corpus...\")\n","\n","if __name__=='__main__':\n","    sanityCheckFullDataset('unigram')"]},{"cell_type":"markdown","metadata":{"id":"lYJ0x5KpRrtU"},"source":["Finally, you can train your model on the full WikiText corpus, and evaluate it on the held-out test set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1920,"status":"ok","timestamp":1739826510055,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"s1XHIg0xrUIt","outputId":"b27d10da-af7a-4c37-aaea-9af9d4d0ff1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------- 5 sentences from your model ---------\n","Log Probability: -72.32620311491841 \tSentence: ['<s>', 'the', '<UNK>', 'sweep', 'at', 'bought', 'kind', 'Horus', 'find', 'by', '</s>']\n","Log Probability: -231.66380599171765 \tSentence: ['<s>', 'heavy', 'amino', 'his', 'the', 'India', 'with', 'agreed', 'to', 'being', 'chosen', 'excessive', 'and', 'rights', 'should', ',', '(', 'standard', 'the', 'together', ',', 'and', 'retro', 'more', 'states', 'Matt', 'goals', 'the', 'music', '\"', 'plates', 'grout', '</s>']\n","Log Probability: -90.95206888425095 \tSentence: ['<s>', 'eastward', 'she', 'locks', 'de', 'critical', 'somewhat', 'bound', 'martial', 'produced', '</s>']\n","Log Probability: -145.94142401575797 \tSentence: ['<s>', 'monastery', 'damaged', 'jubilee', 'for', 'equipped', 'Graham', '\"', 'were', ',', '.', 'drive', 'the', 'funerary', 'phosphorus', 'productions', 'specifically', 'and', 'is', '</s>']\n","Log Probability: -50.556990634399895 \tSentence: ['<s>', 'Composition', 'It', 'Portrait', 'of', 'sleeping', '</s>']\n","\n","--------- Corpus Perplexities ---------\n","Training Set: 1101.9435880266938\n","Testing Set: 912.1574385930448\n"]}],"source":["def runModel(model_type):\n","    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n","    # Read the corpora\n","    if model_type == 'unigram':\n","        model = UnigramModel(train_dataset)\n","    elif model_type == 'bigram':\n","        model = BigramModel(train_dataset)\n","    elif model_type == 'smoothed-unigram':\n","        model = SmoothedUnigramModel(train_dataset)\n","    else:\n","        model = SmoothedBigramModelAD(train_dataset)\n","\n","    print(\"--------- 5 sentences from your model ---------\")\n","    model.printSentences(5)\n","\n","    print (\"\\n--------- Corpus Perplexities ---------\")\n","    print (\"Training Set:\", model.getCorpusPerplexity(train_dataset))\n","    print (\"Testing Set:\", model.getCorpusPerplexity(test_dataset))\n","\n","if __name__=='__main__':\n","    runModel('unigram')"]},{"cell_type":"markdown","metadata":{"id":"2bGyA8vOfvRj"},"source":["## <font color='red'>TODO:</font> Smoothed Unigram Model [18 points]\n","\n","Here, you will implement each of the 4 functions described above for a <b>unigram</b> model with <b>Laplace (add-one) smoothing</b>. The probability distribution of a word is given by $P_L(w)$. This type of smoothing takes away some of the probability mass for observed events and assigns it to unseen events.\n","\n","In order to smooth your model, you will need the number of words in the corpus, $N$, and the number of word types, $S$. The distinction between these is meaningful: $N$ indicates the number of word instances, where $S$ refers to the size of our vocabulary. For example, the sentence <em>the cat saw the dog</em> has four word types (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>dog</em>), but five word tokens (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>the</em>, <em>dog</em>). The token <em>the</em> appears twice in the sentence, but they share the same type <em>the</em>.\n","\n","If $c(w)$ is the frequency of $w$ in the training data, you can compute $P_L(w)$ as follows:\n","\n","$$P_L(w)=\\frac{c(w)+1}{N+S}$$\n","\n","<font color='green'><b>Hints:</b></font>\n","* <font color='green'>You may find it convenient to make your `SmoothedUnigramModel` inherit your `UnigramModel`, and then override the function(s) that need to be changed.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzX-UZJPfvRn"},"outputs":[],"source":["class SmoothedUnigramModel(UnigramModel):\n","    def __init__(self, trainCorpus):\n","\n","        ### TODO ###\n","        super().__init__(trainCorpus)\n","        token_dict = super().token_dict()\n","        self.p_w = self.token_prob(token_dict)\n","\n","\n","    def token_prob(self, token_dict):\n","\n","        N = sum(token_dict.values())\n","        S= len(token_dict.keys())\n","        pl_w ={}\n","\n","        for key, value in token_dict.items():\n","          pl_w[key] = (value+1)/(N+S)\n","\n","        return pl_w\n","\n","\n","    def generateSentence(self):\n","\n","        ### TODO ###\n","        return super().generateSentence()\n","\n","    def getSentenceLogProbability(self, sentence):\n","\n","        ### TODO ###\n","        return super().getSentenceLogProbability(sentence)\n","\n","    def getCorpusPerplexity(self, testCorpus):\n","\n","        ### TODO ###\n","        return super().getCorpusPerplexity(testCorpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254,"status":"ok","timestamp":1739826517166,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"rBy9QaEdCUbc","outputId":"fac36bc3-cca0-4c98-a2ca-ce984064ade3"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- TEST: generateSentence() ---\n","Test generateSentence() passed!\n","\n","--- TEST: getSentenceLogProbability(...) ---\n","Correct log prob.: -19.0405293515 \tYour log prob.: -19.0405293515 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n","Correct log prob.: -115.3479413049 \tYour log prob.: -115.3479413049 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n","Correct log prob.: -108.9114348746 \tYour log prob.: -108.9114348746 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n","Correct log prob.: -54.8190029616 \tYour log prob.: -54.8190029616 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n","Correct log prob.: -55.8122547346 \tYour log prob.: -55.8122547346 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n","Test getSentenceProbability(...) passed!\n","\n","--- TEST: getCorpusPerplexity(...) ---\n","Correct train perp.: 41.9994393615 \tYour train perp.: 41.9994393615 \t PASSED\n","Correct test perp.: 39.9531928383 \tYour test perp.: 39.9531928383 \t PASSED\n","Test getCorpusPerplexity(...) passed!\n"]}],"source":["if __name__=='__main__':\n","    sanityCheck('smoothed-unigram')"]},{"cell_type":"markdown","metadata":{"id":"pxeKnWk1TTfF"},"source":["Since the next sanity check trains your model on the *entire* training set, you will likely see that it is taking too long if you have inefficiences in your code. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2776,"status":"ok","timestamp":1739826522257,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"210t2j1GS1Xd","outputId":"c8a6a7dc-1d45-4f26-d0fe-b1d50fb697dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- TEST: getSentenceLogProbability(...) ---\n","Correct log prob.: -80.8423009715 \tYour log prob.: -80.8423009715 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n","Correct log prob.: -174.5131424172 \tYour log prob.: -174.5131424172 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n","Correct log prob.: -136.3181234818 \tYour log prob.: -136.3181234818 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n","Correct log prob.: -225.357454098 \tYour log prob.: -225.357454098 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n","Correct log prob.: -719.1543898871 \tYour log prob.: -719.1543898871 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n","Correct log prob.: -236.6682968913 \tYour log prob.: -236.6682968913 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n","Correct log prob.: -126.1965419509 \tYour log prob.: -126.1965419509 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n","Correct log prob.: -47.4369338195 \tYour log prob.: -47.4369338195 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n","Correct log prob.: -47.7692144935 \tYour log prob.: -47.7692144935 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n","Correct log prob.: -138.542462715 \tYour log prob.: -138.542462715 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n","Test getSentenceProbability(...) passed!\n","\n","--- TEST: getCorpusPerplexity(...) ---\n","Correct test perp.: 881.6105352831 \tYour test perp.: 881.6105352831 \t PASSED\n","Test getCorpusPerplexity(...) passed!\n"]}],"source":["if __name__=='__main__':\n","    sanityCheckFullDataset('smoothed-unigram')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2864,"status":"ok","timestamp":1739561062581,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"6agWmpjdCWOt","outputId":"b3420b4c-3eaf-4698-d631-0189e70b8f9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------- 5 sentences from your model ---------\n","Log Probability: -79.34895362782214 \tSentence: ['<s>', 'that', 'oldest', 'auxiliary', 'major', 'approximately', 'of', 'career', 'a', '<UNK>', 'the', '.', '(', '</s>']\n","Log Probability: -106.40544725304825 \tSentence: ['<s>', 'throughout', 'at', 'from', 'but', 'the', ',', 'Goose', 'Importance', ',', 'week', 'wall', '.', ',', ',', 'criticized', 'in', 'of', '</s>']\n","Log Probability: -101.00499624055497 \tSentence: ['<s>', 'It', 'Chicago', 'the', 'Corps', 'and', ',', 'source', 'song', '(', 'severe', 'resumed', 'communists', 'place', '</s>']\n","Log Probability: -81.61720152578056 \tSentence: ['<s>', '.', 'final', 'of', 'the', 'Protoceratops', \"'s\", 'been', 'it', 'embark', 'England', 'under', 'the', '</s>']\n","Log Probability: -82.28262606133984 \tSentence: ['<s>', 'in', 'of', 'and', 'mobile', 'fictional', 'limited', 'July', 'kg', 'race', '.', 'years', '</s>']\n","\n","--------- Corpus Perplexities ---------\n","Training Set: 1103.0243317444865\n","Testing Set: 914.4724502283168\n"]}],"source":["if __name__=='__main__':\n","    runModel('smoothed-unigram')"]},{"cell_type":"markdown","metadata":{"id":"vGtcWVMGiEGw"},"source":["## <font color='red'>TODO:</font> Bigram Model [18 points]\n","\n","Here, you will implement each of the 4 functions described above for an <b>unsmoothed bigram</b> model. The probability distribution of a word is given by $\\hat P(w'|w)$. Thus, the probability of $w_i$ is conditioned on $w_{i-1}$.\n","\n","<font color='green'><b>Hints:</b></font>\n","* <font color='green'>You should use a dictionary of dictionaries to store your bigram counts. That is, the outer dictionary should map $w$ to another dictionary that maps $w'$ to the number of times $w'$ occurs after $w$.</font>\n","* <font color='green'>Do <b>not</b> attempt to iterate over all possible bigrams in your voabulary: <em>only store bigrams that actually occur in your training data.</em> You will run into timeout or out-of-memory issues if you attempt to enumerate all bigrams.</font>\n","* <font color='green'>Similarly, avoid nested loops over the training data.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ojk_q0YiEGx"},"outputs":[],"source":["class BigramModel(LanguageModel):\n","    def __init__(self, trainCorpus):\n","\n","        ### TODO ###\n","        self.train_corpus = trainCorpus\n","        self.tokens_unigrams = self.token_dict_unigrams()\n","        self.tokens_bigrams = self.token_dict_bigrams()\n","        self.p_ww = self.bigram_prob(self.tokens_unigrams, self.tokens_bigrams)\n","\n","\n","    def token_dict_unigrams(self):\n","      t_d_unigrams = {}\n","\n","      for sentence in self.train_corpus:\n","        for word1 in sentence[:-1]:\n","          super().unigram_count(t_d_unigrams, word1)\n","\n","      return t_d_unigrams\n","\n","    def token_dict_bigrams(self):\n","      t_d_bigrams = {}\n","\n","      for sentence in self.train_corpus:\n","        for word1, word2 in zip(sentence[:-1],sentence[1:]):\n","          super().bigram_count(t_d_bigrams, word1, word2)\n","\n","      return t_d_bigrams\n","\n","    def set_bigram_prob(self, d, word1, word2, prob):\n","      if word1 not in d.keys():\n","        d[word1] = {word2: prob}\n","      else:\n","        if word2 not in d[word1].keys():\n","                d[word1][word2] = prob\n","\n","\n","\n","    def bigram_prob(self, unigrams, bigrams):\n","        p_ww = {}\n","        for word1, word1dict in bigrams.items():\n","            for word2, numbigrams in word1dict.items():\n","              numunigrams = unigrams[word1]\n","              p_w = numbigrams / numunigrams\n","\n","              self.set_bigram_prob(p_ww, word1, word2, p_w)\n","        return p_ww\n","\n","\n","\n","    def generateSentence(self):\n","\n","        ### TODO ###\n","        sentence = []\n","        word1 = START\n","        word2 = ''\n","\n","        while word2 != END:\n","            possible_word2 = list(self.p_ww[word1].keys())\n","            word2_prob = list(self.p_ww[word1].values())\n","            word2 = random.choices(possible_word2, weights =word2_prob, k=1)[0]\n","            sentence.append(word1)\n","            word1 = word2\n","\n","        sentence.append(word2)\n","        return sentence\n","\n","    def getSentenceLogProbability(self, sentence):\n","       ### TODO ###\n","        probarray =[]\n","\n","        for word1, word2 in zip(sentence[:-1], sentence[1:]):\n","            if word1 in self.p_ww.keys() and word2 in self.p_ww[word1].keys():\n","              probarray.append(self.p_ww[word1][word2])\n","            else:\n","              return float('-inf')\n","\n","\n","        logarray = super().naturalLog(probarray)\n","\n","        ### TODO ###\n","        return sum(logarray)\n","\n","\n","\n","    def getCorpusPerplexity(self, testCorpus):\n","\n","        ### TODO ###\n","        logprob = 0\n","        N = 0\n","\n","        for sentence in testCorpus:\n","            N += len(sentence) - 1\n","            logprob += self.getSentenceLogProbability(sentence)\n","\n","        perplexity = math.exp(-1/N* logprob)\n","        return perplexity"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1739826534244,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"7J8O8mCICZO5","outputId":"ded05fd9-fe1b-4751-a557-19ef1bcb52dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- TEST: generateSentence() ---\n","Test generateSentence() passed!\n","\n","--- TEST: getSentenceLogProbability(...) ---\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n","Correct log prob.: -10.3450917073 \tYour log prob.: -10.3450917073 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n","Correct log prob.: -9.2464794186 \tYour log prob.: -9.2464794186 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n","Test getSentenceProbability(...) passed!\n","\n","--- TEST: getCorpusPerplexity(...) ---\n","Correct train perp.: 1.3861445461 \tYour train perp.: 1.3861445461 \t PASSED\n","Correct test perp.: inf \tYour test perp.: inf \t PASSED\n","Test getCorpusPerplexity(...) passed!\n"]}],"source":["if __name__=='__main__':\n","    sanityCheck('bigram')"]},{"cell_type":"markdown","metadata":{"id":"IMcAZYrpUE0W"},"source":["Since the next sanity check trains your model on the *entire* training set, you will likely see that it is taking too long if you have inefficiences in your code. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4107,"status":"ok","timestamp":1739826540131,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"DIolz_8sS3B9","outputId":"b66ad428-ec3c-470b-9c08-3b39d917974a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- TEST: getSentenceLogProbability(...) ---\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n","Correct log prob.: -32.1502020637 \tYour log prob.: -32.1502020637 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n","Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n","Test getSentenceProbability(...) passed!\n","\n","--- TEST: getCorpusPerplexity(...) ---\n","Correct test perp.: inf \tYour test perp.: inf \t PASSED\n","Test getCorpusPerplexity(...) passed!\n"]}],"source":["if __name__=='__main__':\n","    sanityCheckFullDataset('bigram')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5310,"status":"ok","timestamp":1739826548980,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"bKop-qYKCZO5","outputId":"e3eca36f-e082-4e55-cede-f54b827a88a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------- 5 sentences from your model ---------\n","Log Probability: -97.04902884021536 \tSentence: ['<s>', 'The', 'Cogan', 'Viaduct', ')', 'at', 'the', 'American', 'distribution', 'includes', 'the', '18th', 'century', 'also', 'be', 'told', 'Newsweek', ',', 'Krasnyi', 'Kavkaz', '(', 'Harry', '<UNK>', '.', '</s>']\n","Log Probability: -67.7940185916302 \tSentence: ['<s>', 'The', 'New', 'York', 'Times', 'critic', '<UNK>', 'Giles', ',', 'encouraging', 'him', 'that', 'Corbet', 'within', 'ten', 'games', '.', '</s>']\n","Log Probability: -83.13046918462703 \tSentence: ['<s>', 'He', 'maintained', 'by', 'two', 'guns', \"'\", 'of', 'Â£', '750', 'kg', '(', '75', 'the', 'reduced', 'the', 'United', 'States', '.', '</s>']\n","Log Probability: -469.87026404396573 \tSentence: ['<s>', 'The', 'newspaper', 'from', 'the', '<UNK>', 'do', 'not', 'as', 'his', 'first', ',', '<UNK>', ')', 'in', '2008', 'Italian', 'champions', 'of', 'the', 'aircraft', ',', 'but', 'Bart', 'is', 'known', 'as', 'a', 'large', 'Goose', 'Pagoda', 'and', 'relief', 'from', 'the', 'perfect', '41', 'â€“', '0', '@.@', '3', ',', 'which', 'is', 'inconclusive', ':', '\"', '...', ']', 'whole', 'of', 'Hellblazer', ',', 'in', 'the', 'BBC', 'anthology', 'series', 'to', '<UNK>', '<UNK>', 'or', 'destroyed', ',', 'and', 'recording', 'time', 'are', 'interviewed', 'by', 'a', 'photo', ',', 'while', 'underground', 'members', 'of', 'risky', '@-@', 'Air', 'Force', '(', 'translucent', ')', 'for', 'his', 'Eve', 'ball', 'to', 'completing', 'a', 'much', 'more', 'common', 'starlings', 'were', 'displaced', ',', 'the', 'Order', 'of', 'Ceres', 'in', 'the', 'end', 'it', 'was', '78', 'million', '.', '</s>']\n","Log Probability: -10.729337053631637 \tSentence: ['<s>', 'The', 'player', '.', '</s>']\n","\n","--------- Corpus Perplexities ---------\n","Training Set: 76.92394608735728\n","Testing Set: inf\n"]}],"source":["if __name__=='__main__':\n","    runModel('bigram')"]},{"cell_type":"markdown","metadata":{"id":"WPBeyKUsfnnW"},"source":["## <font color='red'>TODO:</font> Smoothed Bigram Model [18 points]\n","\n","Here, you will implement each of the 4 functions described above for a <b>bigram</b> model with <b>absolute discounting</b>. The probability distribution of a word is given by $P_{AD}(wâ€™|w)$.\n","\n","In order to smooth your model, you need to compute a discounting factor $D$. If $n_k$ is the number of bigrams $w_1w_2$ that appear exactly $k$ times, you can compute $D$ as:\n","\n","$$D=\\frac{n_1}{n_1+2n_2}$$\n","\n","For each word $w$, you then need to compute the number of bigram types $wwâ€™$ as follows:\n","\n","$$S(w)=|\\{wâ€™\\mid c(wwâ€™)>0\\}|$$\n","\n","where $c(wwâ€™)$ is the frequency of $wwâ€™$ in the training data. In other words, $S(w)$ is the number of unique words that follow $w$ at least once in the training data.\n","\n","Finally, you can compute $P_{AD}(wâ€™|w)$ as follows:\n","\n","$$P_{AD}(wâ€™|w)=\\frac{\\max \\big (c(wwâ€™)-D,0\\big )}{c(w)}+\\bigg (\\frac{D}{c(w)}\\cdot S(w) \\cdot P_L(wâ€™)\\bigg )$$\n","\n","where $c(w)$ is the frequency of $w$ in the training data and $P_L(wâ€™)$ is the Laplace-smoothed unigram probability of $wâ€™$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l1klb00wtVtS"},"outputs":[],"source":["class SmoothedBigramModelAD(BigramModel):\n","    def __init__(self, trainCorpus):\n","\n","        ### TODO ###\n","        super().__init__(trainCorpus)\n","        self.tokens_unigrams = self.completeUnigrams()\n","        n1 = self.n_k(1)\n","        n2 = self.n_k(2)\n","        self.pl_w = SmoothedUnigramModel(trainCorpus).p_w\n","        self.D = (n1)/(n1+2*n2)\n","        self.p_ww = self.smooth_bigram_prob(self.tokens_bigrams)\n","\n","\n","    def completeUnigrams(self):\n","      t_d_unigrams = {}\n","\n","      for sentence in self.train_corpus:\n","        for word1 in sentence:\n","          super().unigram_count(t_d_unigrams, word1)\n","\n","      return t_d_unigrams\n","\n","    def P_AD(self, word1, word2):\n","      c_ww = self.c_ww(word1, word2)\n","      c_w = self.c_w(word1)\n","      PL_W = self.pl_w[word2]\n","\n","      S_w = self.S_w(word1)\n","\n","      P_AD = (max(c_ww - self.D, 0) / c_w) + (self.D/c_w * S_w * PL_W)\n","      return P_AD\n","\n","\n","    def smooth_bigram_prob(self, bigrams):\n","      p_ww ={}\n","\n","      for word1, word1dict in bigrams.items():\n","          for word2, numbigrams in word1dict.items():\n","            P_AD = self.P_AD(word1, word2)\n","            self.set_bigram_prob(p_ww, word1, word2, P_AD)\n","      return p_ww\n","\n","\n","\n","    def n_k(self, k):\n","      n = 0\n","      for word1, word1dict in self.tokens_bigrams.items():\n","        for word2, bigramcount in word1dict.items():\n","          if bigramcount ==k:\n","            n+=1\n","      return n\n","\n","    def S_w(self, word1):\n","      word1dict = self.tokens_bigrams[word1].keys()\n","      return len(word1dict)\n","\n","\n","\n","    def c_ww(self, word1, word2):\n","      if word1 in self.tokens_bigrams.keys() and word2 in self.tokens_bigrams[word1].keys():\n","        return self.tokens_bigrams[word1][word2]\n","      else:\n","        return 0\n","\n","    def c_w(self, word1):\n","      if word1 in self.tokens_bigrams.keys():\n","        return self.tokens_unigrams[word1]\n","      else:\n","        return 0\n","\n","\n","\n","\n","    def generateSentence(self):\n","\n","        ### TODO ###\n","        #print(super().generateSentence())\n","        return super().generateSentence()\n","\n","    def getSentenceLogProbability(self, sentence):\n","\n","        ### TODO ###\n","        probarray =[]\n","\n","        for word1, word2 in zip(sentence[:-1], sentence[1:]):\n","            if word1 in self.p_ww.keys() and word2 in self.p_ww[word1].keys():\n","              probarray.append(self.p_ww[word1][word2])\n","            else:\n","              #print('word probability DNE')\n","              P_AD = self.P_AD(word1, word2)\n","              probarray.append(P_AD)\n","\n","\n","        logarray = super(BigramModel, self).naturalLog(probarray)\n","\n","        ### TODO ###\n","        return sum(logarray)\n","\n","    def getCorpusPerplexity(self, testCorpus):\n","\n","        ### TODO ###\n","        return super().getCorpusPerplexity(testCorpus)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1739826645532,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"-BLplBkmtfWG","outputId":"ec886574-295f-4d2f-f8b2-60c111d64b47"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- TEST: generateSentence() ---\n","Test generateSentence() passed!\n","\n","--- TEST: getSentenceLogProbability(...) ---\n","Correct log prob.: -16.355820202 \tYour log prob.: -16.355820202 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n","Correct log prob.: -76.0026113319 \tYour log prob.: -76.0026113319 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n","Correct log prob.: -74.2346475108 \tYour log prob.: -74.2346475108 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n","Correct log prob.: -47.2885760372 \tYour log prob.: -47.2885760372 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n","Correct log prob.: -51.2730261907 \tYour log prob.: -51.2730261907 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n","Test getSentenceProbability(...) passed!\n","\n","--- TEST: getCorpusPerplexity(...) ---\n","Correct train perp.: 12.2307627397 \tYour train perp.: 12.2307627397 \t PASSED\n","Correct test perp.: 26.7193157699 \tYour test perp.: 26.7193157699 \t PASSED\n","Test getCorpusPerplexity(...) passed!\n"]}],"source":["if __name__=='__main__':\n","    sanityCheck('smoothed-bigram')"]},{"cell_type":"markdown","metadata":{"id":"1dgNaW3HUGuU"},"source":["Since the next sanity check trains your model on the *entire* training set, you will likely see that it is taking too long if you have inefficiences in your code. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85sJoiXsS4Zv","outputId":"f0142ea1-14e1-41c3-ddc3-353a49cb96b3","executionInfo":{"status":"ok","timestamp":1739826655963,"user_tz":360,"elapsed":7966,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- TEST: getSentenceLogProbability(...) ---\n","Correct log prob.: -61.3754065648 \tYour log prob.: -61.3754065648 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n","Correct log prob.: -141.9754903887 \tYour log prob.: -141.9754903887 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n","Correct log prob.: -107.0849366076 \tYour log prob.: -107.0849366076 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n","Correct log prob.: -168.4944718788 \tYour log prob.: -168.4944718788 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n","Correct log prob.: -619.9409055374 \tYour log prob.: -619.9409055374 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n","Correct log prob.: -195.8159911677 \tYour log prob.: -195.8159911677 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n","Correct log prob.: -86.3762008156 \tYour log prob.: -86.3762008156 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n","Correct log prob.: -32.4764801981 \tYour log prob.: -32.4764801981 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n","Correct log prob.: -48.124714509 \tYour log prob.: -48.124714509 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n","Correct log prob.: -124.687107856 \tYour log prob.: -124.687107856 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n","Test getSentenceProbability(...) passed!\n","\n","--- TEST: getCorpusPerplexity(...) ---\n","Correct test perp.: 261.4247123506 \tYour test perp.: 261.4247123506 \t PASSED\n","Test getCorpusPerplexity(...) passed!\n"]}],"source":["if __name__=='__main__':\n","    sanityCheckFullDataset('smoothed-bigram')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9664,"status":"ok","timestamp":1739826668440,"user":{"displayName":"Edward Chen","userId":"02346495629728626876"},"user_tz":360},"id":"GibKGwdXtiUQ","outputId":"bf35bc7d-6ca3-490d-ce2d-250fba3a1495"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------- 5 sentences from your model ---------\n","Log Probability: -43.16946582037758 \tSentence: ['<s>', 'Likewise', ',', 'who', 'was', 'genius', ',', 'her', 'book', '.', '</s>']\n","Log Probability: -293.52634271511556 \tSentence: ['<s>', 'That', 'was', 'finished', 'third', 'and', 'claimed', 'the', 'United', ',', 'the', 'Susquehanna', 'River', 'Avenue', ',', 'Staley', 'broke', 'and', 'star', 'is', 'seen', 'in', 'the', 'Fox', '(', 'before', 'its', 'radical', 'agenda', 'and', 'Kansas', 'City', '<UNK>', ',', 'although', 'most', 'part', 'of', '36', 'crosses', 'for', 'the', 'absence', 'of', 'tribbles', 'kept', 'in', 'the', 'day', ',', 'began', 'to', 'widen', 'the', '2015', ',', 'is', 'so', 'that', 'the', 'final', 'season', ',', 'and', '<UNK>', '<UNK>', '.', '</s>']\n","Log Probability: -40.87629082628197 \tSentence: ['<s>', 'On', '29', 'when', 'young', ',', 'drummer', 'Barrett', '.', '</s>']\n","Log Probability: -312.97162084943824 \tSentence: ['<s>', 'The', 'final', 'storm', ',', 'if', 'they', 'may', 'be', '@-@', 'and', 'a', 'significant', 'challenges', 'that', '\"', 'To', 'promote', 'the', 'reintroduction', 'of', 'larger', 'than', 'the', 'Qedarites', 'were', 'targeted', 'at', '1', 'on', 'blood', 'more', 'famous', 'archery', 'teacher', 'helped', 'to', 'generate', 'high', 'scalers', 'received', 'word', 'spaces', 'leads', 'to', 'rebuild', 'her', 'into', 'the', 'eventual', '2', '%', 'of', 'marriage', 'of', '9', 'December', '2012', 'to', 'write', 'with', 'his', 'first', 'time', 'in', 'Boca', 'Raton', '.', '</s>']\n","Log Probability: -24.168752230511867 \tSentence: ['<s>', 'LÃ¼tzow', 'carried', 'out', 'of', 'Ireland', '.', '</s>']\n","\n","--------- Corpus Perplexities ---------\n","Training Set: 98.55812920532598\n","Testing Set: 272.57201979320354\n"]}],"source":["if __name__=='__main__':\n","    runModel('smoothed-bigram')"]},{"cell_type":"markdown","metadata":{"id":"DtMMWXC0Emwq"},"source":["## Food for Thought\n","We provide you some questions to think about. <b>You do not need to answer these questions, but we encourage you to give them some thought.</b>\n","<ol>\n","<li>When generating sentences with the unigram model, what controls the length of the generated sentences? How does this differ from the sentences produced by the bigram models?\n","<li>Consider the probability of the generated sentences according to your models. Do your models assign drastically different probabilities to the different sets of sentences? Why do you think that is?\n","<li>Look back at the sentences generated using your models. In your opinion, which model produces better / more realistic sentences?\n","<li>For each of the four models, which test corpus has the highest perplexity? Why?\n","<li> Why do you think it might be a bad idea to use Laplace (add-one) smoothing for a bigram model? How does the absolute discounting method help?\n","</ol>"]},{"cell_type":"markdown","metadata":{"id":"Fa3qZXUBePFW"},"source":["# What to Submit\n","\n","To submit the assignment, download this notebook as a <TT>.py</TT> file. You can do this by going to <TT>File > Download > Download .py</TT>. Then submit it to the autograder in Gradescope. <b>Do not try to submit it as a <TT>.ipynb</TT> file!</b>\n","\n","**Reminder: Make sure that you access the Gradescope submission page via the corresponding assignment in Coursera!** Failure to do so may result in the inability to push your grades to Coursera. (The same goes for quizzes!)\n","\n","Note that it should take <b>less than 10 minutes</b> to see your score after you have submitted to Gradescope. If your submission runs significantly longer than that, you probably have inefficiency issues in your code!"]}],"metadata":{"colab":{"provenance":[{"file_id":"1vDPATW6c5rFL3CMtABf4QfhFpb68imHC","timestamp":1738682459780}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6161ccd5dcc14353884876f717a012b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ae74e8391d44f4baca20f5225eea4d6","IPY_MODEL_2497a2bc0a6e4cbc846fcf248537f273","IPY_MODEL_36eb1bd6761543daa53a69043fe0aa23"],"layout":"IPY_MODEL_f39ecda8be43422f84defba1ec85278f"}},"7ae74e8391d44f4baca20f5225eea4d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e8b5a7771574e23b3984fb1618c3e5e","placeholder":"â€‹","style":"IPY_MODEL_2b9e219db59143ce8d0ae7ebed8f16b2","value":"100%"}},"2497a2bc0a6e4cbc846fcf248537f273":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e33fe9337d949dda61beb09aee747d7","max":603,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d21c21222c34c5e923d075e9aa65b7f","value":603}},"36eb1bd6761543daa53a69043fe0aa23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2628e2b9c132450db5ff97763465c684","placeholder":"â€‹","style":"IPY_MODEL_bb798ecc9b5046079944a3c5ac6de113","value":"â€‡603/603â€‡[00:53&lt;00:00,â€‡â€‡7.69it/s]"}},"f39ecda8be43422f84defba1ec85278f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e8b5a7771574e23b3984fb1618c3e5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b9e219db59143ce8d0ae7ebed8f16b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e33fe9337d949dda61beb09aee747d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d21c21222c34c5e923d075e9aa65b7f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2628e2b9c132450db5ff97763465c684":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb798ecc9b5046079944a3c5ac6de113":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}