{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL8T2iZUN2Qj"
      },
      "source": [
        "# CS 447 Homework 1 $-$ Tokenization & Language Models\n",
        "In this homework we will study **tokenization** and **language modeling**. In particular, you will build a WordPiece tokenizer and some n-gram language models on a corpus of Wikipedia articles.\n",
        "\n",
        "This notebook is designed to be run in Google Colab. Navigate to <TT>colab.research.google.com</TT> and upload this notebook. Then follow the instructions in the notebook to do the assignent.\n",
        "\n",
        "To run the notebook, you will need to connect to a Runtime. For this homework, all you need is a CPU. You can change the runtime by going to <TT>Runtime > Change runtime type</TT> and selecting <TT>None</TT> in the <TT>Hardware Accelerator</TT> field. We encourage you to disconnect from the runtime when you are not using it, as Google Colab can limit your resources if you overuse them. You can read more about Google Colab at https://research.google.com/colaboratory/faq.html.\n",
        "\n",
        "We have imported all the libraries you need to do this homework. <b>You should not import any extra libraries.</b> If you do, the autograder will fail to run your code.\n",
        "\n",
        "**Reminder: The course policy of this class prohibits the use of AI tools to help with coding, such as Chat-GPT or other code completion software. Further, you may not look at or copy from code repositories online; and while you may discuss the homework with your classmates, you may *not* share code with each other.** You are of course welcome to look at general Python materials (such as Python or Pytorch tutorials and documentation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zXwUyLZogP_",
        "outputId": "3dea8f08-29aa-4586-919f-edad2aede02a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCTcbDLsh-nS"
      },
      "source": [
        "# Part 1: WordPiece Tokenization [28 points]\n",
        "\n",
        "Here you will implement WordPiece tokenization (you can read more about WordPiece in this [paper](https://arxiv.org/pdf/1609.08144))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU08lg2JoX9A"
      },
      "source": [
        "## Download & preprocess the data\n",
        "\n",
        "We use a subset of the FineWeb BBC News dataset. You can see the Huggingface dataset card [here](https://huggingface.co/datasets/permutans/fineweb-bbc-news) (including a discussion of potential limitations or biases), and the corresponding research paper [here](https://arxiv.org/pdf/2406.17557)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwpobI7Moeo-",
        "outputId": "27cd9221-e006-497f-e57d-d3448082d545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " NewsDataset({'train': 7948, 'test': 883}) \n",
            "\n",
            "Example sentences:\n",
            "\t the three are suspected of aggravated attempts to avoid tax controls, according to swedish prosecutor olof sahlgren.\n",
            "\t missing killer alan john giles seen in worcestershire village a convicted killer who absconded from prison has possibly been seen in worcestershire, police said.\n",
            "\t it will be a lot tighter than people are expecting, said the three-times league cup winner.\n",
            "\t they look cold, half of them.\n",
            "\t but it could take a decade to build a base for the submarines elsewhere in the uk, it said.\n",
            "\t hes a proven premier league performer who brings quality to our squad, boss gary megson told the club website.\n",
            "\t but the industry is still so new there are many different approaches.\n",
            "\t un chief ban ki-moon is sending two top aides to the country to help investigate the alleged assaults in the countrys volatile eastern region.\n",
            "\t cheng cheng ensured that his classmates shouted down xu xiaofei before she had even started to speak, and she found it difficult to recover.\n",
            "\t the report in beijings official english-language china daily notes that the country has suffered a series of knife attacks in schools and day-care centres in recent months.\n"
          ]
        }
      ],
      "source": [
        "### DO NOT EDIT ###\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "class NewsDataset():\n",
        "    \"\"\"\n",
        "    A class to manage and preprocess the News dataset for this homework.\n",
        "\n",
        "    Attributes:\n",
        "        train (list[str]): The training dataset as a list of sentences.\n",
        "        test (list[str]): The testing dataset as a list of sentences.\n",
        "    \"\"\"\n",
        "    def __init__(self, redownload_dataset=False):\n",
        "        \"\"\"\n",
        "        Initializes the NewsDataset object. If the dataset files do not exist\n",
        "        or redownload_dataset is set to True, it downloads the dataset.\n",
        "\n",
        "        Args:\n",
        "            redownload_dataset (bool): If True, redownloads the dataset.\n",
        "        \"\"\"\n",
        "        self.train, self.test = [], []\n",
        "        self.load_data(redownload_dataset)\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Returns a string representation of the dataset, including the number of\n",
        "        training and testing examples.\n",
        "\n",
        "        Returns:\n",
        "            str: Information about the dataset size\n",
        "        \"\"\"\n",
        "        repr = {}\n",
        "        repr[\"train\"] = len(self.train) if self.train else 0\n",
        "        repr[\"test\"] = len(self.test) if self.test else 0\n",
        "        return f\"{type(self).__name__}({repr})\"\n",
        "\n",
        "    @staticmethod\n",
        "    def isValid(s: str) -> bool:\n",
        "        \"\"\"\n",
        "        Checks whether a given string is valid for inclusion in the dataset.\n",
        "        A string is considered valid if it contains more than five words and\n",
        "        at least one alphabetic character.\n",
        "\n",
        "        Args:\n",
        "            s (str): The string to validate.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the string is valid, False otherwise.\n",
        "        \"\"\"\n",
        "        return s and (len(s.strip().split()) > 5) and any(c.isalpha() for c in s)\n",
        "\n",
        "    @staticmethod\n",
        "    def preprocess(paragraph: str):\n",
        "        paragraph = paragraph.translate(str.maketrans({'?': '.', '!': '.'}))\n",
        "        sentences = paragraph.split('.')\n",
        "        sentences = [s.replace(\"\\\"\", \"\").replace(\"\\'\", \"\").strip().replace(\"\\n\", \" \") + '.' for s in sentences]\n",
        "        sentences = [s.lower() for s in sentences if '  ' not in s]\n",
        "        sentences = [s for s in sentences if not any(char.isdigit() for char in s)]\n",
        "        return sentences\n",
        "\n",
        "    def preprocessWP(self, data):\n",
        "        \"\"\"\n",
        "        Preprocesses a dataset by tokenizing them into\n",
        "        sentences and filtering out invalid ones.\n",
        "\n",
        "        Args:\n",
        "            data: An iterable containing the text of articles.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: A list of valid sentences.\n",
        "        \"\"\"\n",
        "        dataset = []\n",
        "        for item in data:\n",
        "            sentences = self.preprocess(item[\"text\"])\n",
        "            sentences = [sent for sent in sentences if self.isValid(sent)]\n",
        "            dataset += sentences\n",
        "        return dataset\n",
        "\n",
        "    def downloadDatasetWP(self):\n",
        "        \"\"\"\n",
        "        Downloads and preprocesses the News dataset.\n",
        "\n",
        "        Returns:\n",
        "            list[str]: A list of preprocessed sentences from the dataset.\n",
        "        \"\"\"\n",
        "        ds = load_dataset(\"permutans/fineweb-bbc-news\", \"sample-10BT\")\n",
        "        ds = self.preprocessWP(ds[\"train\"])\n",
        "        return ds\n",
        "\n",
        "    def generateDatasetWP(self):\n",
        "        \"\"\"\n",
        "        Downloads, preprocesses, and splits the dataset into training and\n",
        "        testing sets.\n",
        "        The processed data is saved as parquet files.\n",
        "        \"\"\"\n",
        "        ds = self.downloadDatasetWP()\n",
        "        data = {\"text\": ds}\n",
        "        df = pd.DataFrame(data)\n",
        "        train = df.sample(frac=0.05,random_state=200)\n",
        "        test = train.sample(frac=0.1,random_state=200)\n",
        "        train = train.drop(test.index)\n",
        "        train.to_parquet('train.parquet', index=False)\n",
        "        test.to_parquet('test.parquet', index=False)\n",
        "\n",
        "    def load_data(self, redownload_dataset):\n",
        "        \"\"\"\n",
        "        Loads the dataset from parquet files. If the files do not exist or\n",
        "        redownload_dataset is set to True, it downloads the dataset.\n",
        "        \"\"\"\n",
        "        if (redownload_dataset or\n",
        "                not os.path.exists('train.parquet') or\n",
        "                not os.path.exists('test.parquet')):\n",
        "            self.generateDatasetWP()\n",
        "        self.train, self.test = (pd.read_parquet('train.parquet')[\"text\"].tolist(),\n",
        "                                 pd.read_parquet('test.parquet')[\"text\"].tolist())\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    dataset = NewsDataset()\n",
        "\n",
        "    print('\\n', dataset, '\\n\\nExample sentences:')\n",
        "\n",
        "    for sentence in dataset.train[:10]:\n",
        "        print('\\t', sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqinUgNann-8"
      },
      "source": [
        "## <font color='red'>TODO:</font> Tokenizer\n",
        "\n",
        "First, you will write a training (learning) algorithm, which takes in a corpus (list of sentences), and returns a token vocabulary. Then, you will write the tokenization algorithm. This takes in a sentence and returns its tokenization based on the vocabulary.\n",
        "\n",
        "Your code for this section will all be in the `WordPieceTokenizer` class, where you will implement several functions.\n",
        "\n",
        "### Training algorithm\n",
        "\n",
        "WordPiece learns a token vocabulary in an iterative fashion. Starting with an initial token vocabulary (the set of characters in the training data), each step merges a pair of consecutive tokens, where the pair of tokens is selected according to a scoring function (described below). The process is continued until a desired `vocab_size` is reached.\n",
        "\n",
        "You will complete this algorithm by implementing several functions in the `WordPieceTokenizer` class:\n",
        "\n",
        "1. **`initialize(self, train_corpus)`: [6 points]** This returns the initial vocabulary, the initial tokenization of each word, the word frequncies, and a mapping from each token to the words containing that token. First, split each sentence in `train_corpus` on the space character. The initial vocabulary is the set of all characters that occur in any word; but characters occuring in the middle of a word should be prepended with the special sequence `##`. For example, if your corpus is the sentence `the old man the boat.`, your initial vocabulary would be:\n",
        "<div align=\"center\"><code>{'t', '##h', '##e', 'o', '##l', '##d', 'm', '##a', '##n', 't', 'b', '##o', '##t', '##.'}</code></div>\n",
        "This function should also compute the initial tokenization of each word, storing them in a dictionary from word to tokenization. For example:\n",
        "<div align=\"center\"><code>{'the': ('t', '##h', '##e;), 'old': ('o', '##l', '##d'),                \n",
        "    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}\n",
        "</code></div>\n",
        "The function should also count the frequency of each word in the training corpus, and return a mapping from token to the words that use that token. For example:\n",
        "<div align=\"center\"><code>word_freqs = {'the': 100, 'old': 34, 'man': 76, 'boat.': 18}</code></div>\n",
        "    <div align=\"center\"><code>tokens2word = {'t': {'the'}, '##h': {'the'}, '##e': {'the'}, ...}</code></div>\n",
        "\n",
        "2. **`find_best_pair(self, word_tokenizations, word_freqs, vocab)`: [6 points]** This function computes a score for each *consecutive pair* $(t_1, t_2)$ of tokens, and returns the pair of tokens with highest score. The scoring function is $$\\frac{c(t_1, t_2) \\cdot |V|}{c(t_1)\\cdot c(t_2)}$$where $c(t_1, t_2)$ is the number of times $t_1$ and $t_2$ occur consecutively in the corpus, $c(t_i)$ is the nubmer of times token $t_i$ occurs, and $|V|$ is the size of the current token vocabulary. The function will return the pair with highest score (ties broken alphabetically).\n",
        "\n",
        "    The idea behind the scoring function is that a pair of tokens will have high score if they occur frequently together *and* each token does not occur frequently on its own. Factoring in the size of the vocabulary gives a (slight) preference to longer subword units as the vocabulary size grows.\n",
        "    \n",
        "\n",
        "3. **`merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word)`: [6 points]** This function updates your current vocabulary (`vocab`), tokenizations (`word_tokenizations`), and token-to-word mapping (`tokens2word`) based on the pair of tokens to merge (`best_pair`). The `vocab` should simply be updated to include the new token, and any word in `word_tokenizations` that contains the consecutive pair of tokens should be updated to merge that pair. For example, suppose the pair of tokens to merge is `(##h, ##e)` and `word_tokenizations` is\n",
        "<div align=\"center\"><code>  {'the': ('t', '##h', '##e;), 'old': ('o', '##l', '##d'),                \n",
        "    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}</code></div>\n",
        "Then <code>word_tokenizations</code> should be updated to\n",
        "<div align=\"center\"><code>  {'the': ('t', '##he'), 'old': ('o', '##l', '##d'),                      \n",
        "    'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'}</code></div>\n",
        "Similarly, <code>tokens2word</code> should be updated to include the new token:\n",
        "<div align=\"center\"><code>tokens2word['##he'] = {'the'}</code></div>\n",
        "<i>Note:</i> In order to pass the autograder, you should <i>not</i> remove <code>'the'</code> from <code>tokens2word['##h']</code> or <code>tokens2word['##e']</code>, despite the fact that these tokens are no longer used for that word.\n",
        "\n",
        "We provide you the function `train(self, train_corpus, vocab_size)` which iteratively calls these functions until the desired `vocab_size` is hit. You should NOT modify this function, but you should look at it to understand how it works.\n",
        "\n",
        "### Tokenization algorithm\n",
        "\n",
        "Finally, you will write the `tokenize` and `detokenize` functions, which use the token vocabulary you learned to tokenize and detokenize arbitrary sentences:\n",
        "\n",
        "1. **`tokenize(self, sentence)`: [5 points]** This function takes a sentence and returns its tokenization. First, split on the space character. For each word, find the longest substring starting at the beginning of the word that is in the token vocabulary, and choose that token. Then, iteratively repeat this procedure on the remainder of the word, until the entire word is tokenized. For example, consider the word `swiftly`:\n",
        "    * If `s` and `sw` are tokens but `swi` is not, then choose `sw`.\n",
        "    * Then, proceed with `##iftly`. If `##i`, `##if`, and `##ift` are tokens, but `##iftl` is not, choose `##ift`.\n",
        "    * Then, proceed with `##ly`. If both `##l` and `##ly` are tokens, choose `##ly`.\n",
        "\n",
        "  After tokenizing each word in this way, return a list containing the tokenization of each word separated by space tokens. For the sentence `he is swift.`, for example, you might return `['he', ' ', 'is', ' ', 'sw', '##ift', '##.']`\n",
        "\n",
        "  **Note:** If at any step during the tokenization of a word, you cannot find a matching token in your vocabulary, then the *entire* word should be tokenized as `<UNK>`. For example, if you are considering `##mat` but `##m`, `##ma`, and `##mat` are not in the vocabulary, the entire word (including the part of the word preceeding `##mat`) should be tokenized as `<UNK>`.\n",
        "\n",
        "\n",
        "2. **`detokenize(self, tokens)`: [5 points]** This takes a list of tokens and returns its corresponding sentence. Simply copy tokens into a string from left to right, being careful to remove `##` at the beginning of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWRZljnLnMWu"
      },
      "outputs": [],
      "source": [
        "class WordPieceTokenizer(object):\n",
        "    def __init__(self, train_corpus, vocab_size, do_print=False, do_tqdm=True):\n",
        "        '''\n",
        "        ### DO NOT EDIT ###\n",
        "\n",
        "        Trains a WordPiece tokenizer.\n",
        "        Args:\n",
        "            train_corpus: List of strings (each string is a sentence)\n",
        "            vocab_size: Integer indicating how big the vocabulary should be\n",
        "            do_print: Whether or not to print merges\n",
        "        '''\n",
        "        self.train_corpus = train_corpus\n",
        "        self.vocab_size = vocab_size\n",
        "        self.do_print = do_print\n",
        "        self.do_tqdm = do_tqdm\n",
        "        for sent in train_corpus: assert '##' not in sent, sent\n",
        "        self.vocab = None\n",
        "\n",
        "    def initialize(self, train_corpus):\n",
        "        '''\n",
        "        Initialize token vocabulary, word frequencies, and token-to-word dictionaries.\n",
        "        Args:\n",
        "            train_corpus: List of strings, where each string is a sentence\n",
        "        Returns:\n",
        "            vocab: A set containing the initial token vocabulary (each token is a string)\n",
        "            word_tokenizations: A dictionary of word to its tokenization (a tuple of strings)\n",
        "            word_freqs: A dictionary that maps each word to its frequency in the training data\n",
        "            tokens2word: A dictionary that maps each token to the set of words with that token\n",
        "        '''\n",
        "\n",
        "        vocab = {'<UNK>', ' '} # Initialize vocab with special tokens (don't change this)\n",
        "        word_tokenizations, word_freqs, tokens2word = {}, {}, {}\n",
        "\n",
        "        ### TODO ###\n",
        "        for string in train_corpus:\n",
        "          tc = string.split(\" \")\n",
        "\n",
        "          #get counts of each word\n",
        "          for item in tc:\n",
        "\n",
        "            if item in word_tokenizations:\n",
        "              word_freqs[item] +=1\n",
        "              continue\n",
        "            token_list =[]\n",
        "            if item not in word_freqs:\n",
        "              word_freqs[item] = 1\n",
        "            else:\n",
        "              word_freqs[item] +=1\n",
        "\n",
        "            for i in range(len(item)):\n",
        "\n",
        "              #item not in vocab, and its the first element\n",
        "              if item[i] not in vocab and i==0:\n",
        "                vocab.add(item[i])\n",
        "                token_list.append(item[i])\n",
        "              #item not in vocab, not first element\n",
        "              elif item[i] not in vocab:\n",
        "                vocab.add(\"##\"+item[i])\n",
        "                token_list.append(\"##\"+item[i])\n",
        "              #item in vocab, but its the first element\n",
        "              elif item[i] in vocab and i==0:\n",
        "                token_list.append(item[i])\n",
        "              #item not in vocab, not first element\n",
        "              else:\n",
        "                token_list.append(\"##\"+item[i])\n",
        "                vocab.add(\"##\"+item[i])\n",
        "\n",
        "            word_tokenizations[item] =tuple(token_list)\n",
        "\n",
        "            for j in range(len(item)):\n",
        "              if item[j] not in tokens2word.keys() and j==0:\n",
        "                a = set()\n",
        "                a.add(item)\n",
        "                tokens2word[item[j]] = a\n",
        "              elif item[j] in tokens2word.keys() and j==0:\n",
        "                if item not in tokens2word[item[j]]:\n",
        "                  tokens2word[item[j]].add(item)\n",
        "              elif (\"##\"+item[j]) not in tokens2word.keys() and j>0:\n",
        "                a = set()\n",
        "                a.add(item)\n",
        "                tokens2word[\"##\"+ item[j]] = a\n",
        "              elif (\"##\"+item[j]) in tokens2word.keys() and j>0:\n",
        "                if item not in tokens2word[\"##\"+item[j]]:\n",
        "                  tokens2word[\"##\"+ item[j]].add(item)\n",
        "\n",
        "\n",
        "\n",
        "        return vocab, word_tokenizations, word_freqs, tokens2word\n",
        "\n",
        "    def find_best_pair(self, word_tokenizations, word_freqs, vocab):\n",
        "        '''\n",
        "        Score all pairs of consecutive tokens (bigrams) in train_corpus, and return the pair with\n",
        "          highest score. If there is a tie, choose the pair that is first alphabetically.\n",
        "        Args:\n",
        "            word_freqs: Dictionary of word to frqeuency\n",
        "            word_tokenizations: Dictionary of word to its tokenization (a tuple of strings)\n",
        "            vocab: A set of tokens (strings)\n",
        "        Returns:\n",
        "            best_pair: The pair (tuple) of tokens (t1, t2) that has highest score\n",
        "            scores: Dictionary of tokens (t1, t2) to its score\n",
        "        '''\n",
        "\n",
        "        best_pair, scores = None, {}\n",
        "        unicountdict ={}\n",
        "        bincountdict ={}\n",
        "\n",
        "        for word, tokens in word_tokenizations.items():\n",
        "          for i in range(0, len(tokens)):\n",
        "            if tokens[i] not in unicountdict:\n",
        "              unicountdict[tokens[i]]=word_freqs[word]\n",
        "            else:\n",
        "              unicountdict[tokens[i]]+=word_freqs[word]\n",
        "\n",
        "        for word, tokens in word_tokenizations.items():\n",
        "          for i in range(1, len(tokens)):\n",
        "            if (tokens[i-1], tokens[i]) not in bincountdict:\n",
        "              bincountdict[(tokens[i-1], tokens[i])] = word_freqs[word]\n",
        "            else:\n",
        "              bincountdict[(tokens[i-1], tokens[i])]+=word_freqs[word]\n",
        "\n",
        "        for tup, ct1t2 in bincountdict.items():\n",
        "          ct1 = unicountdict[tup[0]]\n",
        "          ct2 = unicountdict[tup[1]]\n",
        "          scores[tup] = ct1t2*len(vocab)/(ct1*ct2)\n",
        "\n",
        "        max_value = max(scores.values())\n",
        "        max_keys = [k for k, v in scores.items() if v == max_value]\n",
        "        best_pair = min(max_keys)\n",
        "\n",
        "\n",
        "        ### TODO ###\n",
        "\n",
        "        return best_pair, scores\n",
        "\n",
        "    def merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word):\n",
        "        '''\n",
        "        Update vocab, word_tokenizations, and tokens2word based on pair to merge.\n",
        "        Args:\n",
        "            best_pair: The pair of tokens that had highest score; that is, the pair to be merged\n",
        "            vocab: The token vocabulary (set of strings)\n",
        "            word_tokenizations: Dictionary from word to tokenization\n",
        "            tokens2word: A dictionary from token to the set of words with that token\n",
        "        Returns: Nothing\n",
        "        Modifies:\n",
        "            vocab: The new token should be added to the vocab set.\n",
        "            word_tokenizations: Any word that contains the consecutive pair of tokens in best_pair\n",
        "              should be re-tokenized such that that pair of tokens has been merged.\n",
        "            tokens2word: The new token should be mapped to the set of words that use this token;\n",
        "              do NOT otherwise modify this dictionary.\n",
        "        Hint: When looking for words that contain the new token, look ONLY at words that use either of\n",
        "          the tokens in best_pair (by using tokens2word).\n",
        "        '''\n",
        "\n",
        "        ### TODO ###\n",
        "        merged_pair = self.pair_concat(best_pair)\n",
        "        if merged_pair not in vocab:\n",
        "            vocab.add(merged_pair)\n",
        "\n",
        "        words1 = tokens2word[best_pair[0]]\n",
        "\n",
        "        words2 = tokens2word[best_pair[1]]\n",
        "        tokens2word[merged_pair] = set()\n",
        "\n",
        "\n",
        "        # tokens2word[merged_pair] = set()\n",
        "        for word in (words1 | words2):\n",
        "            if word in words1 and word in words2:\n",
        "\n",
        "                tokenlist = list(word_tokenizations[word])\n",
        "\n",
        "                for i in range(0, len(tokenlist)-1):\n",
        "                  if i ==len(tokenlist)-1:\n",
        "                    if best_pair[0] == tokenlist[i] and best_pair[1] == tokenlist[i+1]:\n",
        "                      tokenlist[i+1]=merged_pair\n",
        "                      tokenlist.remove(tokenlist[i])\n",
        "                      tokens2word[merged_pair].add(word)\n",
        "                      word_tokenizations[word] = tuple(tokenlist)\n",
        "                      break\n",
        "                    # else:\n",
        "                    #   del tokens2word[merged_pair]\n",
        "                    #   break\n",
        "                  if best_pair[0] == tokenlist[i] and best_pair[1] == tokenlist[i+1]:\n",
        "                    tokenlist[i+1]=merged_pair\n",
        "                    tokenlist.remove(tokenlist[i])\n",
        "                    tokens2word[merged_pair].add(word)\n",
        "                    word_tokenizations[word] = tuple(tokenlist)\n",
        "                    break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def pair_concat(self,pair):\n",
        "      if pair[1].startswith('##'):\n",
        "        return pair[0] + pair[1][2:]\n",
        "      else:\n",
        "        return pair[0] + pair[1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        '''\n",
        "        ### DO NOT EDIT ###\n",
        "\n",
        "        Trains the WordPiece tokenization algorithm by calling iteratively merging tokens until\n",
        "          vocab_size is reached.\n",
        "        Args:\n",
        "            train_corpus: List of strings, where each string is a sentence\n",
        "            vocab_size: The desired vocabulary size\n",
        "        Returns:\n",
        "            vocab: The set of tokens in the vocabulary, returned as a sorted list\n",
        "        '''\n",
        "        vocab, word_tokenizations, word_freqs, tokens2word = self.initialize(self.train_corpus)\n",
        "        init_vocab_size = len(vocab)\n",
        "        if self.do_print: print(\"Initial vocab size:\", init_vocab_size)\n",
        "        assert self.vocab_size >= len(vocab), 'Cannot give a vocab size smaller than initial vocab size'\n",
        "\n",
        "        itr = tqdm(range(self.vocab_size - len(vocab))) if self.do_tqdm else range(self.vocab_size - len(vocab))\n",
        "        for i in itr:\n",
        "            best_pair, scores = self.find_best_pair(word_tokenizations, word_freqs, vocab)\n",
        "            self.merge_best_pair(best_pair, vocab, word_tokenizations, tokens2word)\n",
        "            outputs = (vocab, word_tokenizations, tokens2word)\n",
        "            print(\"\\tMerging \", best_pair) # UNCOMMENT IF YOU WANT TO SEE THE MERGES\n",
        "            assert len(vocab) == init_vocab_size + i + 1, str(len(vocab)) + ' '+ str(init_vocab_size) + ' ' + str(i)\n",
        "            if all(len(word_tokenizations[x]) == 1 for x in word_tokenizations):\n",
        "                print(\"All words have been maximally merged at vocab_size=\" +str(len(vocab)) + \" \u2013 breaking.\")\n",
        "                break\n",
        "        if self.do_print: print(\"Done!\")\n",
        "        self.vocab = sorted(vocab)\n",
        "\n",
        "    def longest_matching_substring(self, word, string_set):\n",
        "      longest_substring = \"\"\n",
        "      current_substring = \"\"\n",
        "      for char in word:\n",
        "          current_substring += char\n",
        "          if current_substring in string_set:\n",
        "            if len(current_substring) > len(longest_substring):\n",
        "              longest_substring = current_substring\n",
        "\n",
        "      return longest_substring\n",
        "\n",
        "\n",
        "    def tokenize(self, sentence):\n",
        "        '''\n",
        "        Tokenizes a sentence.\n",
        "        Args:\n",
        "            sentence: A string representing a sentence\n",
        "        Returns:\n",
        "            tokens: A list containing the tokens of the sentence\n",
        "        '''\n",
        "        assert type(sentence) == str\n",
        "        tokens = []\n",
        "\n",
        "        ### TODO ###\n",
        "        sentence_split = sentence.split(\" \")\n",
        "        listtokens=[]\n",
        "        for word in sentence_split:\n",
        "          i = 0\n",
        "          wordremain = word\n",
        "          int_tokens=[]\n",
        "          while i <len(word):\n",
        "            if i ==0:\n",
        "              token = self.longest_matching_substring(wordremain,self.vocab)\n",
        "            else:\n",
        "              token = self.longest_matching_substring((\"##\" +wordremain),self.vocab)\n",
        "\n",
        "            if len(token) == 0:\n",
        "              int_tokens.append('<UNK>')\n",
        "              break\n",
        "\n",
        "            elif (i==0 and len(token)>0):\n",
        "              tokenlen = len(token)\n",
        "              wordremain = wordremain[tokenlen:]\n",
        "              int_tokens.append(token)\n",
        "            elif (i>0 and len(token)>2):\n",
        "              tokenlen = len(token)-2\n",
        "              wordremain = wordremain[tokenlen:]\n",
        "              int_tokens.append(token)\n",
        "            i += tokenlen\n",
        "          listtokens.append(int_tokens)\n",
        "\n",
        "        newlist = []\n",
        "        for word in listtokens:\n",
        "          if '<UNK>' in word:\n",
        "            newlist.append(['<UNK>'])\n",
        "          else:\n",
        "            newlist.append(word)\n",
        "\n",
        "        for word in newlist:\n",
        "          for token in word:\n",
        "            tokens.append(token)\n",
        "          tokens.append(\" \")\n",
        "\n",
        "        tokens = tokens[:-1]\n",
        "\n",
        "\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def detokenize(self, tokens):\n",
        "        '''\n",
        "        Detokenizes a sentence.\n",
        "        Args:\n",
        "            tokens: A list of tokens representing a sentence\n",
        "        Returns:\n",
        "            sentence: A string representing the sentence\n",
        "        '''\n",
        "        assert type(tokens) == list and len(tokens) > 0 and type(tokens[0]) == str\n",
        "        sentence = ''\n",
        "\n",
        "        for token in tokens:\n",
        "          if token[0:2] == \"##\":\n",
        "            sentence += str(token[2:])\n",
        "          else:\n",
        "            sentence += str(token)\n",
        "\n",
        "        return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXCkjvBaLp_B"
      },
      "source": [
        "##Sanity Check: Tokenizer Class\n",
        "\n",
        "The code below runs a sanity check for your `WordPieceTokenizer` class. The tests are similar to the hidden ones in Gradescope. However, note that passing the sanity check does <b>not</b> guarantee that you will pass the autograder; it is intended to help you debug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1h6jCy1WL9WG"
      },
      "outputs": [],
      "source": [
        "# @title Sanity Check Code\n",
        "### DO NOT EDIT ###\n",
        "\n",
        "def check_dictionary(test_dict, correct_dict):\n",
        "    if not isinstance(test_dict, dict) or not isinstance(correct_dict, dict):\n",
        "        return \"INCORRECT\", \"Is not a dictionary.\"\n",
        "    if set(test_dict.keys()) != set(correct_dict.keys()):\n",
        "        return \"INCORRECT\", f\"Key mismatch: Missing {set(correct_dict.keys()) - set(test_dict.keys())}, Extra {set(test_dict.keys()) - set(correct_dict.keys())}\"\n",
        "    for key in correct_dict:\n",
        "        if test_dict[key] != correct_dict[key]:\n",
        "            return \"INCORRECT\", f\"Value mismatch for key '{key}': Expected {correct_dict[key]}, Got {test_dict[key]}\"\n",
        "    return \"CORRECT\", ''\n",
        "\n",
        "def check_set(test_set, correct_set):\n",
        "    if not isinstance(test_set, set) or not isinstance(correct_set, set):\n",
        "        return \"INCORRECT\", \"Is not a set.\"\n",
        "    if test_set != correct_set:\n",
        "        return \"INCORRECT\", f\"Set mismatch: Missing {correct_set - test_set}, Extra {test_set - correct_set}\"\n",
        "    return \"CORRECT\", \"\"\n",
        "\n",
        "def sanityCheckInitialize(train):\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "\n",
        "    print('\\n\\n--- TEST: initialize(self, train_corpus) ---')\n",
        "    res = tokenizer.initialize(train)\n",
        "    if len(res) != 4:\n",
        "        print('FAILED\\ninitialize(self, train_corpus) must return 4 items.')\n",
        "        return\n",
        "    vocab, word_tokenizations, word_freqs, tokens2word = res\n",
        "\n",
        "    correct_vocab = {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}\n",
        "    correct_word_tokenizations = {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\n",
        "    correct_word_freqs = {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\n",
        "    correct_tokens2word = {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'that', 'banana.', 'boat.', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'that', 'boat.', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'lied.', 'like'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}\n",
        "\n",
        "    pass1, msg1 = check_set(vocab, correct_vocab)\n",
        "    pass2, msg2 = check_dictionary(word_tokenizations, correct_word_tokenizations)\n",
        "    pass3, msg3 = check_dictionary(word_freqs, correct_word_freqs)\n",
        "    pass4, msg4 = check_dictionary(tokens2word, correct_tokens2word)\n",
        "\n",
        "    print('\\tvocab:\\t\\t\\t'+pass1+'\\t'+msg1 + '\\tYour vocab: ' +str(vocab) + '\\tCorrect vocab:' + str(correct_vocab))\n",
        "    print('\\tword_tokenizations:\\t'+pass2+'\\t'+msg2 + '\\tYour word_tokenizations: ' +str(word_tokenizations) + '\\tCorrect word_tokenizations: ' + str(correct_word_tokenizations))\n",
        "    print('\\tword_freqs:\\t\\t'+pass3+'\\t'+msg3 + '\\tYour word_freqs: ' +str(word_freqs) + '\\tCorrect word_freqs: ' + str(correct_word_freqs))\n",
        "    print('\\ttokens2word:\\t\\t'+pass4+'\\t'+msg4 + '\\tYour tokens2word: ' +str(tokens2word) + '\\tCorrect tokens2word: ' + str(correct_tokens2word))\n",
        "\n",
        "    if len({pass1, pass2, pass3, pass4}) == 1 and pass1 == 'CORRECT':\n",
        "        print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "def sanityCheckFindBestPair(train):\n",
        "    print('\\n\\n--- TEST: find_best_pair(self, word_tokenizations, word_freqs, vocab) ---') # max_len does not matter for this test\n",
        "    test_cases = ({'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}, [(({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##w', '##;'), {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##o', '##w;'), {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##r', '##ow;'), {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}), (('##r', '##row;'), {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##r', '##u'), {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('a', '##rrow;'), {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('f', '##ru'), {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('b', '##o'), {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##o', '##l'), {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##ol', '##d'), {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('f', '##l'), {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5})), (({'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('o', '##l'), {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667})), (({'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('ol', '##d'), {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('##d', '##.'), {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}), (('k', '##n'), {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335})), (({'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}), (('s', '##h'), {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}))])\n",
        "\n",
        "    word_freqs, test_cases = test_cases[0], test_cases[1]\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "\n",
        "    overall_pass = True\n",
        "    for i in range(len(test_cases)):\n",
        "        res = tokenizer.find_best_pair(test_cases[i][0][0], word_freqs, test_cases[i][0][1])\n",
        "        if len(res) != 2:\n",
        "            pass1, msg1 = 'INCORRECT', 'Did not return 2 items.'\n",
        "        else:\n",
        "            best_pair, scores = res\n",
        "            bp = best_pair == test_cases[i][1][0]\n",
        "            sc, scc = check_dictionary(scores, test_cases[i][1][1])\n",
        "            if bp and sc == 'CORRECT':\n",
        "                pass1, msg1 = 'CORRECT', ''\n",
        "            else:\n",
        "                overall_pass = False\n",
        "                pass1 = 'INCORRECT'\n",
        "                msg1 = ''\n",
        "                if not bp: msg1 += 'best_pair is incorrect. '\n",
        "                if sc == 'INCORRECT': msg1 += 'scores is incorrect (' + scc + ')'\n",
        "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tword_tokenizations: ' + str(test_cases[i][0][0]) + '\\tword_freqs: ' + str(word_freqs) + '\\tvocab: ' + str(test_cases[i][0][1]) +\n",
        "              '\\tYour best_pair: ' + str(best_pair) + '\\tCorrect best_pair: ' + str(test_cases[i][1][0]) + '\\tYour scores: ' + str(scores) + '\\tCorrect scores: ' + str(test_cases[i][1][1]))\n",
        "    if overall_pass: print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "def sanityCheckMergeBestPair(train):\n",
        "    print('\\n\\n--- TEST: merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word) ---') # max_len does not matter for this test\n",
        "    test_cases = [((('##w', '##;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}})), ((('##o', '##w;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}})), ((('##r', '##ow;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}})), ((('##r', '##row;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}})), ((('##r', '##u'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}})), ((('a', '##rrow;'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}})), ((('f', '##ru'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}})), ((('b', '##o'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}})), ((('##o', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}})), ((('##ol', '##d'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}})), ((('f', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}})), ((('o', '##l'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}), ({'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}})), ((('ol', '##d'), {'<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}})), ((('##d', '##.'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}})), ((('k', '##n'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}), ({'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}})), ((('s', '##h'), {'old', '<UNK>', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}), ({'old', '<UNK>', 'sh', 'm', 'h', '##t', 'a', '##a', '##k', '##ow;', '##ol', 'fru', 'bo', '##i', '##m', '##old', '##row;', 't', '##d', 'l', '##r', '##o', 'fl', 'ol', 'arrow;', '##u', 'k', '##n', '##w;', 'b', '##e', '##ru', '##w', '##l', ' ', '##;', '##d.', '##.', '##rrow;', 's', 'kn', 'o', '##h', '##s', 'f'}, {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}, {'t': {'time', 'the', 'that', 'told'}, '##h': {'she', 'the', 'that'}, '##e': {'knew', 'the', 'like', 'time', 'she', 'he', 'flies', 'lied.'}, 'o': {'old'}, '##l': {'old', 'flies', 'told'}, '##d': {'old', 'told', 'lied.'}, 'm': {'man'}, '##a': {'boat.', 'banana.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'arrow;', 'boat.', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'him', 'like', 'time', 'fruit', 'flies', 'lied.'}, '##m': {'time', 'him'}, 'f': {'flies', 'fruit'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'a', 'arrow;'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}))]\n",
        "\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "\n",
        "    overall_pass = True\n",
        "    for i in range(len(test_cases)):\n",
        "        from copy import deepcopy\n",
        "        vocab, word_tokenizations, tokens2word = deepcopy(test_cases[i][0][1]), deepcopy(test_cases[i][0][2]), deepcopy(test_cases[i][0][3])\n",
        "        vocabo, word_tokenizationso, tokens2wordo = deepcopy(vocab), deepcopy(word_tokenizations), deepcopy(tokens2word)\n",
        "        tokenizer.merge_best_pair(test_cases[i][0][0], vocab, word_tokenizations, tokens2word)\n",
        "\n",
        "        pass1, msg0 = check_set(vocab, test_cases[i][1][0])\n",
        "        pass2, msg2 = check_dictionary(word_tokenizations, test_cases[i][1][1])\n",
        "        pass3, msg3 = check_dictionary(tokens2word, test_cases[i][1][2])\n",
        "        if pass1 == 'CORRECT' and pass2 == 'CORRECT' and pass3 == 'CORRECT':\n",
        "            pass1, msg1 = 'CORRECT', ''\n",
        "        else:\n",
        "            overall_pass = False\n",
        "            pass1 = 'INCORRECT'\n",
        "            msg1 = ''\n",
        "            if pass1 != 'CORRECT': msg1 += 'vocab is incorrect. (' + msg0 + ') '\n",
        "            if pass2 != 'CORRECT': msg1 += 'word_tokenizations is incorrect. (' + msg2 + ') '\n",
        "            if pass3 != 'CORRECT': msg1 += 'tokens2word is incorrect. (' + msg3 + ') '\n",
        "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tbest_pair: ' + str(test_cases[i][0][0]) + '\\tvocab: ' + str(vocabo) + '\\tword_tokenizations: ' + str(word_tokenizationso) + '\\ttokens2word: ' + str(tokens2wordo) +\n",
        "              '\\tYour vocab: ' + str(vocab) + '\\tCorrect vocab: ' + str(test_cases[i][1][0]) + '\\tYour word_tokenizations: ' + str(word_tokenizations) + '\\tCorrect word_tokenizations: ' + str(test_cases[i][1][1])+\n",
        "              '\\tYour tokens2word: ' + str(tokens2word) + '\\tCorrect tokens2word: ' + str(test_cases[i][1][2]))\n",
        "    if overall_pass: print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "\n",
        "def sanityCheckTokenize(train, test):\n",
        "\n",
        "    print('\\n\\n--- TEST: tokenize(self, sentence) ---')\n",
        "\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "    tokenizer.vocab = [' ', '##.', '##;', '##a', '##d', '##d.', '##e', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##ol', '##old', '##ow;', '##r', '##row;', '##rrow;', '##ru', '##s', '##t', '##u', '##w', '##w;', '<UNK>', 'a', 'arrow;', 'b', 'bo', 'f', 'fl', 'fru', 'h', 'k', 'kn', 'l', 'm', 'o', 'ol', 'old', 's', 'sh', 't']\n",
        "    correct_ans = [['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>'], ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']]\n",
        "\n",
        "    overall_pass = True\n",
        "    for i in range(len(test)):\n",
        "        toks = tokenizer.tokenize(test[i])\n",
        "        if toks == correct_ans[i]: pass1, msg1 = 'CORRECT', ''\n",
        "        else:\n",
        "            pass1, msg1 = 'INCORRECT', 'Tokenization is incorrect.'\n",
        "            overall_pass = False\n",
        "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\tsentence: ' + str(test[i]) +\n",
        "              '\\tYour tokenization: ' + str(toks) + '\\tCorrect tokenization: ' + str(correct_ans[i]))\n",
        "\n",
        "    if overall_pass: print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "def sanityCheckDetokenize(train):\n",
        "\n",
        "    print('\\n\\n--- TEST: detokenize(self, tokens) ---')\n",
        "\n",
        "    tokenizer = WordPieceTokenizer(train, 45, do_print=False, do_tqdm=False)\n",
        "    tokenizer.vocab = [' ', '##.', '##;', '##a', '##d', '##d.', '##e', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##ol', '##old', '##ow;', '##r', '##row;', '##rrow;', '##ru', '##s', '##t', '##u', '##w', '##w;', '<UNK>', 'a', 'arrow;', 'b', 'bo', 'f', 'fl', 'fru', 'h', 'k', 'kn', 'l', 'm', 'o', 'ol', 'old', 's', 'sh', 't']\n",
        "    inputs = [['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>'], ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']]\n",
        "    correct_ans = ['she <UNK> the book <UNK>', 'time <UNK> for <UNK> one.']\n",
        "\n",
        "    overall_pass = True\n",
        "    for i in range(len(inputs)):\n",
        "        sent = tokenizer.detokenize(inputs[i])\n",
        "        if sent == correct_ans[i]: pass1, msg1 = 'CORRECT', ''\n",
        "        else:\n",
        "            pass1, msg1 = 'INCORRECT', 'Sentence is incorrect.'\n",
        "            overall_pass = False\n",
        "        print('\\tCase ' + (' ' if i < 10 else '') + str(i) + ':\\t' + pass1 + '\\t' + msg1 + '\\ttokens: ' + str(inputs[i]) +\n",
        "              '\\tYour sentence: ' + str(sent) + '\\tCorrect sentence: ' + str(correct_ans[i]))\n",
        "\n",
        "    if overall_pass: print('\\n  Passed!')\n",
        "    else: print('  Failed.')\n",
        "\n",
        "\n",
        "def sanityCheckTokenizer():\n",
        "    sample_train_corpus = [\"the old man the boat.\",\n",
        "                   \"time flies like an arrow; fruit flies like a banana.\",\n",
        "                   \"she told him that she knew that he lied.\"]\n",
        "    sample_test_corpus = [\"she read the book 1984.\",\n",
        "                          \"time waits for no one.\"]\n",
        "\n",
        "    print(\"Sample train corpus:\", sample_train_corpus)\n",
        "    print(\"Sample test corpus:\", sample_test_corpus)\n",
        "    sanityCheckInitialize(sample_train_corpus)\n",
        "    sanityCheckFindBestPair(sample_train_corpus)\n",
        "    sanityCheckMergeBestPair(sample_train_corpus)\n",
        "    sanityCheckTokenize(sample_train_corpus, sample_test_corpus)\n",
        "    sanityCheckDetokenize(sample_train_corpus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQvj7i7S9_Ru",
        "outputId": "d324d4ec-537d-490e-aac1-1236cd7687a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample train corpus: ['the old man the boat.', 'time flies like an arrow; fruit flies like a banana.', 'she told him that she knew that he lied.']\n",
            "Sample test corpus: ['she read the book 1984.', 'time waits for no one.']\n",
            "\n",
            "\n",
            "--- TEST: initialize(self, train_corpus) ---\n",
            "\tvocab:\t\t\tCORRECT\t\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', '##d', 'h', 'l', '##k', 'b', '##.', '##u', 'f', 'k', '##s', '##a', '##;', '##w', '##m', '##e', 't', 'o', 'm', '##r', 's', '##l', '##o', '##i', '##t'}\tCorrect vocab:{'a', '##n', 't', ' ', '##h', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\n",
            "\tword_tokenizations:\tCORRECT\t\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\n",
            "\tword_freqs:\t\tCORRECT\t\tYour word_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tCorrect word_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\n",
            "\ttokens2word:\t\tCORRECT\t\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'man', 'that'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'lied.', 'like'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}}\n",
            "\n",
            "  Passed!\n",
            "\n",
            "\n",
            "--- TEST: find_best_pair(self, word_tokenizations, word_freqs, vocab) ---\n",
            "\tCase  0:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', 't', ' ', '##h', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour best_pair: ('##w', '##;')\tCorrect best_pair: ('##w', '##;')\tYour scores: {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223}\tCorrect scores: {('t', '##h'): 3.2222222222222223, ('##h', '##e'): 1.6111111111111112, ('o', '##l'): 7.25, ('##l', '##d'): 4.833333333333333, ('m', '##a'): 4.142857142857143, ('##a', '##n'): 2.4857142857142858, ('b', '##o'): 4.833333333333333, ('##o', '##a'): 1.380952380952381, ('##a', '##t'): 3.107142857142857, ('##t', '##.'): 2.4166666666666665, ('t', '##i'): 0.6041666666666666, ('##i', '##m'): 3.625, ('##m', '##e'): 1.2083333333333333, ('f', '##l'): 4.833333333333333, ('##l', '##i'): 1.8125, ('##i', '##e'): 0.90625, ('##e', '##s'): 2.4166666666666665, ('l', '##i'): 3.625, ('##i', '##k'): 3.625, ('##k', '##e'): 2.4166666666666665, ('a', '##n'): 1.9333333333333333, ('a', '##r'): 3.2222222222222223, ('##r', '##r'): 3.2222222222222223, ('##r', '##o'): 3.2222222222222223, ('##o', '##w'): 4.833333333333333, ('##w', '##;'): 14.5, ('f', '##r'): 3.2222222222222223, ('##r', '##u'): 9.666666666666666, ('##u', '##i'): 3.625, ('##i', '##t'): 0.90625, ('b', '##a'): 2.0714285714285716, ('##n', '##a'): 1.6571428571428573, ('##a', '##.'): 1.380952380952381, ('s', '##h'): 4.833333333333333, ('t', '##o'): 1.6111111111111112, ('##o', '##l'): 2.4166666666666665, ('h', '##i'): 1.8125, ('##h', '##a'): 1.380952380952381, ('k', '##n'): 5.8, ('##n', '##e'): 0.48333333333333334, ('##e', '##w'): 1.2083333333333333, ('h', '##e'): 1.2083333333333333, ('##e', '##d'): 0.8055555555555556, ('##d', '##.'): 3.2222222222222223}\n",
            "\tCase  1:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', 't', ' ', '##h', '##w;', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour best_pair: ('##o', '##w;')\tCorrect best_pair: ('##o', '##w;')\tYour scores: {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335}\tCorrect scores: {('t', '##h'): 3.3333333333333335, ('##h', '##e'): 1.6666666666666667, ('o', '##l'): 7.5, ('##l', '##d'): 5.0, ('m', '##a'): 4.285714285714286, ('##a', '##n'): 2.5714285714285716, ('b', '##o'): 5.0, ('##o', '##a'): 1.4285714285714286, ('##a', '##t'): 3.2142857142857144, ('##t', '##.'): 2.5, ('t', '##i'): 0.625, ('##i', '##m'): 3.75, ('##m', '##e'): 1.25, ('f', '##l'): 5.0, ('##l', '##i'): 1.875, ('##i', '##e'): 0.9375, ('##e', '##s'): 2.5, ('l', '##i'): 3.75, ('##i', '##k'): 3.75, ('##k', '##e'): 2.5, ('a', '##n'): 2.0, ('a', '##r'): 3.3333333333333335, ('##r', '##r'): 3.3333333333333335, ('##r', '##o'): 3.3333333333333335, ('##o', '##w;'): 10.0, ('f', '##r'): 3.3333333333333335, ('##r', '##u'): 10.0, ('##u', '##i'): 3.75, ('##i', '##t'): 0.9375, ('b', '##a'): 2.142857142857143, ('##n', '##a'): 1.7142857142857142, ('##a', '##.'): 1.4285714285714286, ('s', '##h'): 5.0, ('t', '##o'): 1.6666666666666667, ('##o', '##l'): 2.5, ('h', '##i'): 1.875, ('##h', '##a'): 1.4285714285714286, ('k', '##n'): 6.0, ('##n', '##e'): 0.5, ('##e', '##w'): 2.5, ('h', '##e'): 1.25, ('##e', '##d'): 0.8333333333333334, ('##d', '##.'): 3.3333333333333335}\n",
            "\tCase  2:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', 't', ' ', '##h', '##w;', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour best_pair: ('##r', '##ow;')\tCorrect best_pair: ('##r', '##ow;')\tYour scores: {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446}\tCorrect scores: {('t', '##h'): 3.4444444444444446, ('##h', '##e'): 1.7222222222222223, ('o', '##l'): 7.75, ('##l', '##d'): 5.166666666666667, ('m', '##a'): 4.428571428571429, ('##a', '##n'): 2.657142857142857, ('b', '##o'): 7.75, ('##o', '##a'): 2.2142857142857144, ('##a', '##t'): 3.3214285714285716, ('##t', '##.'): 2.5833333333333335, ('t', '##i'): 0.6458333333333334, ('##i', '##m'): 3.875, ('##m', '##e'): 1.2916666666666667, ('f', '##l'): 5.166666666666667, ('##l', '##i'): 1.9375, ('##i', '##e'): 0.96875, ('##e', '##s'): 2.5833333333333335, ('l', '##i'): 3.875, ('##i', '##k'): 3.875, ('##k', '##e'): 2.5833333333333335, ('a', '##n'): 2.066666666666667, ('a', '##r'): 3.4444444444444446, ('##r', '##r'): 3.4444444444444446, ('##r', '##ow;'): 10.333333333333334, ('f', '##r'): 3.4444444444444446, ('##r', '##u'): 10.333333333333334, ('##u', '##i'): 3.875, ('##i', '##t'): 0.96875, ('b', '##a'): 2.2142857142857144, ('##n', '##a'): 1.7714285714285714, ('##a', '##.'): 1.4761904761904763, ('s', '##h'): 5.166666666666667, ('t', '##o'): 2.5833333333333335, ('##o', '##l'): 3.875, ('h', '##i'): 1.9375, ('##h', '##a'): 1.4761904761904763, ('k', '##n'): 6.2, ('##n', '##e'): 0.5166666666666667, ('##e', '##w'): 2.5833333333333335, ('h', '##e'): 1.2916666666666667, ('##e', '##d'): 0.8611111111111112, ('##d', '##.'): 3.4444444444444446}\n",
            "\tCase  3:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##row;', '##t'}\tYour best_pair: ('##r', '##row;')\tCorrect best_pair: ('##r', '##row;')\tYour scores: {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554}\tCorrect scores: {('t', '##h'): 3.5555555555555554, ('##h', '##e'): 1.7777777777777777, ('o', '##l'): 8.0, ('##l', '##d'): 5.333333333333333, ('m', '##a'): 4.571428571428571, ('##a', '##n'): 2.742857142857143, ('b', '##o'): 8.0, ('##o', '##a'): 2.2857142857142856, ('##a', '##t'): 3.4285714285714284, ('##t', '##.'): 2.6666666666666665, ('t', '##i'): 0.6666666666666666, ('##i', '##m'): 4.0, ('##m', '##e'): 1.3333333333333333, ('f', '##l'): 5.333333333333333, ('##l', '##i'): 2.0, ('##i', '##e'): 1.0, ('##e', '##s'): 2.6666666666666665, ('l', '##i'): 4.0, ('##i', '##k'): 4.0, ('##k', '##e'): 2.6666666666666665, ('a', '##n'): 2.1333333333333333, ('a', '##r'): 5.333333333333333, ('##r', '##row;'): 16.0, ('f', '##r'): 5.333333333333333, ('##r', '##u'): 16.0, ('##u', '##i'): 4.0, ('##i', '##t'): 1.0, ('b', '##a'): 2.2857142857142856, ('##n', '##a'): 1.8285714285714285, ('##a', '##.'): 1.5238095238095237, ('s', '##h'): 5.333333333333333, ('t', '##o'): 2.6666666666666665, ('##o', '##l'): 4.0, ('h', '##i'): 2.0, ('##h', '##a'): 1.5238095238095237, ('k', '##n'): 6.4, ('##n', '##e'): 0.5333333333333333, ('##e', '##w'): 2.6666666666666665, ('h', '##e'): 1.3333333333333333, ('##e', '##d'): 0.8888888888888888, ('##d', '##.'): 3.5555555555555554}\n",
            "\tCase  4:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('##r', '##u')\tCorrect best_pair: ('##r', '##u')\tYour scores: {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665}\tCorrect scores: {('t', '##h'): 3.6666666666666665, ('##h', '##e'): 1.8333333333333333, ('o', '##l'): 8.25, ('##l', '##d'): 5.5, ('m', '##a'): 4.714285714285714, ('##a', '##n'): 2.8285714285714287, ('b', '##o'): 8.25, ('##o', '##a'): 2.357142857142857, ('##a', '##t'): 3.5357142857142856, ('##t', '##.'): 2.75, ('t', '##i'): 0.6875, ('##i', '##m'): 4.125, ('##m', '##e'): 1.375, ('f', '##l'): 5.5, ('##l', '##i'): 2.0625, ('##i', '##e'): 1.03125, ('##e', '##s'): 2.75, ('l', '##i'): 4.125, ('##i', '##k'): 4.125, ('##k', '##e'): 2.75, ('a', '##n'): 2.2, ('a', '##rrow;'): 11.0, ('f', '##r'): 11.0, ('##r', '##u'): 33.0, ('##u', '##i'): 4.125, ('##i', '##t'): 1.03125, ('b', '##a'): 2.357142857142857, ('##n', '##a'): 1.8857142857142857, ('##a', '##.'): 1.5714285714285714, ('s', '##h'): 5.5, ('t', '##o'): 2.75, ('##o', '##l'): 4.125, ('h', '##i'): 2.0625, ('##h', '##a'): 1.5714285714285714, ('k', '##n'): 6.6, ('##n', '##e'): 0.55, ('##e', '##w'): 2.75, ('h', '##e'): 1.375, ('##e', '##d'): 0.9166666666666666, ('##d', '##.'): 3.6666666666666665}\n",
            "\tCase  5:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('a', '##rrow;')\tCorrect best_pair: ('a', '##rrow;')\tYour scores: {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777}\tCorrect scores: {('t', '##h'): 3.7777777777777777, ('##h', '##e'): 1.8888888888888888, ('o', '##l'): 8.5, ('##l', '##d'): 5.666666666666667, ('m', '##a'): 4.857142857142857, ('##a', '##n'): 2.914285714285714, ('b', '##o'): 8.5, ('##o', '##a'): 2.4285714285714284, ('##a', '##t'): 3.642857142857143, ('##t', '##.'): 2.8333333333333335, ('t', '##i'): 0.7083333333333334, ('##i', '##m'): 4.25, ('##m', '##e'): 1.4166666666666667, ('f', '##l'): 5.666666666666667, ('##l', '##i'): 2.125, ('##i', '##e'): 1.0625, ('##e', '##s'): 2.8333333333333335, ('l', '##i'): 4.25, ('##i', '##k'): 4.25, ('##k', '##e'): 2.8333333333333335, ('a', '##n'): 2.2666666666666666, ('a', '##rrow;'): 11.333333333333334, ('f', '##ru'): 11.333333333333334, ('##ru', '##i'): 4.25, ('##i', '##t'): 1.0625, ('b', '##a'): 2.4285714285714284, ('##n', '##a'): 1.9428571428571428, ('##a', '##.'): 1.619047619047619, ('s', '##h'): 5.666666666666667, ('t', '##o'): 2.8333333333333335, ('##o', '##l'): 4.25, ('h', '##i'): 2.125, ('##h', '##a'): 1.619047619047619, ('k', '##n'): 6.8, ('##n', '##e'): 0.5666666666666667, ('##e', '##w'): 2.8333333333333335, ('h', '##e'): 1.4166666666666667, ('##e', '##d'): 0.9444444444444444, ('##d', '##.'): 3.7777777777777777}\n",
            "\tCase  6:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('f', '##ru')\tCorrect best_pair: ('f', '##ru')\tYour scores: {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889}\tCorrect scores: {('t', '##h'): 3.888888888888889, ('##h', '##e'): 1.9444444444444444, ('o', '##l'): 8.75, ('##l', '##d'): 5.833333333333333, ('m', '##a'): 5.0, ('##a', '##n'): 3.0, ('b', '##o'): 8.75, ('##o', '##a'): 2.5, ('##a', '##t'): 3.75, ('##t', '##.'): 2.9166666666666665, ('t', '##i'): 0.7291666666666666, ('##i', '##m'): 4.375, ('##m', '##e'): 1.4583333333333333, ('f', '##l'): 5.833333333333333, ('##l', '##i'): 2.1875, ('##i', '##e'): 1.09375, ('##e', '##s'): 2.9166666666666665, ('l', '##i'): 4.375, ('##i', '##k'): 4.375, ('##k', '##e'): 2.9166666666666665, ('a', '##n'): 3.5, ('f', '##ru'): 11.666666666666666, ('##ru', '##i'): 4.375, ('##i', '##t'): 1.09375, ('b', '##a'): 2.5, ('##n', '##a'): 2.0, ('##a', '##.'): 1.6666666666666667, ('s', '##h'): 5.833333333333333, ('t', '##o'): 2.9166666666666665, ('##o', '##l'): 4.375, ('h', '##i'): 2.1875, ('##h', '##a'): 1.6666666666666667, ('k', '##n'): 7.0, ('##n', '##e'): 0.5833333333333334, ('##e', '##w'): 2.9166666666666665, ('h', '##e'): 1.4583333333333333, ('##e', '##d'): 0.9722222222222222, ('##d', '##.'): 3.888888888888889}\n",
            "\tCase  7:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('b', '##o')\tCorrect best_pair: ('b', '##o')\tYour scores: {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0}\tCorrect scores: {('t', '##h'): 4.0, ('##h', '##e'): 2.0, ('o', '##l'): 9.0, ('##l', '##d'): 6.0, ('m', '##a'): 5.142857142857143, ('##a', '##n'): 3.085714285714286, ('b', '##o'): 9.0, ('##o', '##a'): 2.5714285714285716, ('##a', '##t'): 3.857142857142857, ('##t', '##.'): 3.0, ('t', '##i'): 0.75, ('##i', '##m'): 4.5, ('##m', '##e'): 1.5, ('f', '##l'): 9.0, ('##l', '##i'): 2.25, ('##i', '##e'): 1.125, ('##e', '##s'): 3.0, ('l', '##i'): 4.5, ('##i', '##k'): 4.5, ('##k', '##e'): 3.0, ('a', '##n'): 3.6, ('fru', '##i'): 4.5, ('##i', '##t'): 1.125, ('b', '##a'): 2.5714285714285716, ('##n', '##a'): 2.057142857142857, ('##a', '##.'): 1.7142857142857142, ('s', '##h'): 6.0, ('t', '##o'): 3.0, ('##o', '##l'): 4.5, ('h', '##i'): 2.25, ('##h', '##a'): 1.7142857142857142, ('k', '##n'): 7.2, ('##n', '##e'): 0.6, ('##e', '##w'): 3.0, ('h', '##e'): 1.5, ('##e', '##d'): 1.0, ('##d', '##.'): 4.0}\n",
            "\tCase  8:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('##o', '##l')\tCorrect best_pair: ('##o', '##l')\tYour scores: {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111}\tCorrect scores: {('t', '##h'): 4.111111111111111, ('##h', '##e'): 2.0555555555555554, ('o', '##l'): 9.25, ('##l', '##d'): 6.166666666666667, ('m', '##a'): 5.285714285714286, ('##a', '##n'): 3.1714285714285713, ('bo', '##a'): 5.285714285714286, ('##a', '##t'): 3.9642857142857144, ('##t', '##.'): 3.0833333333333335, ('t', '##i'): 0.7708333333333334, ('##i', '##m'): 4.625, ('##m', '##e'): 1.5416666666666667, ('f', '##l'): 9.25, ('##l', '##i'): 2.3125, ('##i', '##e'): 1.15625, ('##e', '##s'): 3.0833333333333335, ('l', '##i'): 4.625, ('##i', '##k'): 4.625, ('##k', '##e'): 3.0833333333333335, ('a', '##n'): 3.7, ('fru', '##i'): 4.625, ('##i', '##t'): 1.15625, ('b', '##a'): 5.285714285714286, ('##n', '##a'): 2.1142857142857143, ('##a', '##.'): 1.7619047619047619, ('s', '##h'): 6.166666666666667, ('t', '##o'): 6.166666666666667, ('##o', '##l'): 9.25, ('h', '##i'): 2.3125, ('##h', '##a'): 1.7619047619047619, ('k', '##n'): 7.4, ('##n', '##e'): 0.6166666666666667, ('##e', '##w'): 3.0833333333333335, ('h', '##e'): 1.5416666666666667, ('##e', '##d'): 1.0277777777777777, ('##d', '##.'): 4.111111111111111}\n",
            "\tCase  9:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('##ol', '##d')\tCorrect best_pair: ('##ol', '##d')\tYour scores: {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222}\tCorrect scores: {('t', '##h'): 4.222222222222222, ('##h', '##e'): 2.111111111111111, ('o', '##l'): 12.666666666666666, ('##l', '##d'): 4.222222222222222, ('m', '##a'): 5.428571428571429, ('##a', '##n'): 3.257142857142857, ('bo', '##a'): 5.428571428571429, ('##a', '##t'): 4.071428571428571, ('##t', '##.'): 3.1666666666666665, ('t', '##i'): 0.7916666666666666, ('##i', '##m'): 4.75, ('##m', '##e'): 1.5833333333333333, ('f', '##l'): 12.666666666666666, ('##l', '##i'): 3.1666666666666665, ('##i', '##e'): 1.1875, ('##e', '##s'): 3.1666666666666665, ('l', '##i'): 4.75, ('##i', '##k'): 4.75, ('##k', '##e'): 3.1666666666666665, ('a', '##n'): 3.8, ('fru', '##i'): 4.75, ('##i', '##t'): 1.1875, ('b', '##a'): 5.428571428571429, ('##n', '##a'): 2.1714285714285713, ('##a', '##.'): 1.8095238095238095, ('s', '##h'): 6.333333333333333, ('t', '##ol'): 6.333333333333333, ('##ol', '##d'): 12.666666666666666, ('h', '##i'): 2.375, ('##h', '##a'): 1.8095238095238095, ('k', '##n'): 7.6, ('##n', '##e'): 0.6333333333333333, ('##e', '##w'): 3.1666666666666665, ('h', '##e'): 1.5833333333333333, ('##e', '##d'): 1.0555555555555556, ('##d', '##.'): 4.222222222222222}\n",
            "\tCase 10:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('f', '##l')\tCorrect best_pair: ('f', '##l')\tYour scores: {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5}\tCorrect scores: {('t', '##h'): 4.333333333333333, ('##h', '##e'): 2.1666666666666665, ('o', '##l'): 13.0, ('##l', '##d'): 6.5, ('m', '##a'): 5.571428571428571, ('##a', '##n'): 3.342857142857143, ('bo', '##a'): 5.571428571428571, ('##a', '##t'): 4.178571428571429, ('##t', '##.'): 3.25, ('t', '##i'): 0.8125, ('##i', '##m'): 4.875, ('##m', '##e'): 1.625, ('f', '##l'): 13.0, ('##l', '##i'): 3.25, ('##i', '##e'): 1.21875, ('##e', '##s'): 3.25, ('l', '##i'): 4.875, ('##i', '##k'): 4.875, ('##k', '##e'): 3.25, ('a', '##n'): 3.9, ('fru', '##i'): 4.875, ('##i', '##t'): 1.21875, ('b', '##a'): 5.571428571428571, ('##n', '##a'): 2.2285714285714286, ('##a', '##.'): 1.8571428571428572, ('s', '##h'): 6.5, ('t', '##old'): 6.5, ('h', '##i'): 2.4375, ('##h', '##a'): 1.8571428571428572, ('k', '##n'): 7.8, ('##n', '##e'): 0.65, ('##e', '##w'): 3.25, ('h', '##e'): 1.625, ('##e', '##d'): 1.625, ('##d', '##.'): 6.5}\n",
            "\tCase 11:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('o', '##l')\tCorrect best_pair: ('o', '##l')\tYour scores: {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667}\tCorrect scores: {('t', '##h'): 4.444444444444445, ('##h', '##e'): 2.2222222222222223, ('o', '##l'): 40.0, ('##l', '##d'): 20.0, ('m', '##a'): 5.714285714285714, ('##a', '##n'): 3.4285714285714284, ('bo', '##a'): 5.714285714285714, ('##a', '##t'): 4.285714285714286, ('##t', '##.'): 3.3333333333333335, ('t', '##i'): 0.8333333333333334, ('##i', '##m'): 5.0, ('##m', '##e'): 1.6666666666666667, ('fl', '##i'): 5.0, ('##i', '##e'): 1.25, ('##e', '##s'): 3.3333333333333335, ('l', '##i'): 5.0, ('##i', '##k'): 5.0, ('##k', '##e'): 3.3333333333333335, ('a', '##n'): 4.0, ('fru', '##i'): 5.0, ('##i', '##t'): 1.25, ('b', '##a'): 5.714285714285714, ('##n', '##a'): 2.2857142857142856, ('##a', '##.'): 1.9047619047619047, ('s', '##h'): 6.666666666666667, ('t', '##old'): 6.666666666666667, ('h', '##i'): 2.5, ('##h', '##a'): 1.9047619047619047, ('k', '##n'): 8.0, ('##n', '##e'): 0.6666666666666666, ('##e', '##w'): 3.3333333333333335, ('h', '##e'): 1.6666666666666667, ('##e', '##d'): 1.6666666666666667, ('##d', '##.'): 6.666666666666667}\n",
            "\tCase 12:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('ol', '##d')\tCorrect best_pair: ('ol', '##d')\tYour scores: {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333}\tCorrect scores: {('t', '##h'): 4.555555555555555, ('##h', '##e'): 2.2777777777777777, ('ol', '##d'): 20.5, ('m', '##a'): 5.857142857142857, ('##a', '##n'): 3.5142857142857142, ('bo', '##a'): 5.857142857142857, ('##a', '##t'): 4.392857142857143, ('##t', '##.'): 3.4166666666666665, ('t', '##i'): 0.8541666666666666, ('##i', '##m'): 5.125, ('##m', '##e'): 1.7083333333333333, ('fl', '##i'): 5.125, ('##i', '##e'): 1.28125, ('##e', '##s'): 3.4166666666666665, ('l', '##i'): 5.125, ('##i', '##k'): 5.125, ('##k', '##e'): 3.4166666666666665, ('a', '##n'): 4.1, ('fru', '##i'): 5.125, ('##i', '##t'): 1.28125, ('b', '##a'): 5.857142857142857, ('##n', '##a'): 2.342857142857143, ('##a', '##.'): 1.9523809523809523, ('s', '##h'): 6.833333333333333, ('t', '##old'): 6.833333333333333, ('h', '##i'): 2.5625, ('##h', '##a'): 1.9523809523809523, ('k', '##n'): 8.2, ('##n', '##e'): 0.6833333333333333, ('##e', '##w'): 3.4166666666666665, ('h', '##e'): 1.7083333333333333, ('##e', '##d'): 1.7083333333333333, ('##d', '##.'): 6.833333333333333}\n",
            "\tCase 13:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('##d', '##.')\tCorrect best_pair: ('##d', '##.')\tYour scores: {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0}\tCorrect scores: {('t', '##h'): 4.666666666666667, ('##h', '##e'): 2.3333333333333335, ('m', '##a'): 6.0, ('##a', '##n'): 3.6, ('bo', '##a'): 6.0, ('##a', '##t'): 4.5, ('##t', '##.'): 3.5, ('t', '##i'): 0.875, ('##i', '##m'): 5.25, ('##m', '##e'): 1.75, ('fl', '##i'): 5.25, ('##i', '##e'): 1.3125, ('##e', '##s'): 3.5, ('l', '##i'): 5.25, ('##i', '##k'): 5.25, ('##k', '##e'): 3.5, ('a', '##n'): 4.2, ('fru', '##i'): 5.25, ('##i', '##t'): 1.3125, ('b', '##a'): 6.0, ('##n', '##a'): 2.4, ('##a', '##.'): 2.0, ('s', '##h'): 7.0, ('t', '##old'): 7.0, ('h', '##i'): 2.625, ('##h', '##a'): 2.0, ('k', '##n'): 8.4, ('##n', '##e'): 0.7, ('##e', '##w'): 3.5, ('h', '##e'): 1.75, ('##e', '##d'): 3.5, ('##d', '##.'): 14.0}\n",
            "\tCase 14:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('k', '##n')\tCorrect best_pair: ('k', '##n')\tYour scores: {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335}\tCorrect scores: {('t', '##h'): 4.777777777777778, ('##h', '##e'): 2.388888888888889, ('m', '##a'): 6.142857142857143, ('##a', '##n'): 3.6857142857142855, ('bo', '##a'): 6.142857142857143, ('##a', '##t'): 4.607142857142857, ('##t', '##.'): 5.375, ('t', '##i'): 0.8958333333333334, ('##i', '##m'): 5.375, ('##m', '##e'): 1.7916666666666667, ('fl', '##i'): 5.375, ('##i', '##e'): 1.34375, ('##e', '##s'): 3.5833333333333335, ('l', '##i'): 5.375, ('##i', '##k'): 5.375, ('##k', '##e'): 3.5833333333333335, ('a', '##n'): 4.3, ('fru', '##i'): 5.375, ('##i', '##t'): 1.34375, ('b', '##a'): 6.142857142857143, ('##n', '##a'): 2.4571428571428573, ('##a', '##.'): 3.0714285714285716, ('s', '##h'): 7.166666666666667, ('t', '##old'): 7.166666666666667, ('h', '##i'): 2.6875, ('##h', '##a'): 2.0476190476190474, ('k', '##n'): 8.6, ('##n', '##e'): 0.7166666666666667, ('##e', '##w'): 3.5833333333333335, ('h', '##e'): 1.7916666666666667, ('##e', '##d.'): 3.5833333333333335}\n",
            "\tCase 15:\tCORRECT\t\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tword_freqs: {'the': 2, 'old': 1, 'man': 1, 'boat.': 1, 'time': 1, 'flies': 2, 'like': 2, 'an': 1, 'arrow;': 1, 'fruit': 1, 'a': 1, 'banana.': 1, 'she': 2, 'told': 1, 'him': 1, 'that': 2, 'knew': 1, 'he': 1, 'lied.': 1}\tvocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour best_pair: ('s', '##h')\tCorrect best_pair: ('s', '##h')\tYour scores: {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}\tCorrect scores: {('t', '##h'): 4.888888888888889, ('##h', '##e'): 2.4444444444444446, ('m', '##a'): 6.285714285714286, ('##a', '##n'): 4.714285714285714, ('bo', '##a'): 6.285714285714286, ('##a', '##t'): 4.714285714285714, ('##t', '##.'): 5.5, ('t', '##i'): 0.9166666666666666, ('##i', '##m'): 5.5, ('##m', '##e'): 1.8333333333333333, ('fl', '##i'): 5.5, ('##i', '##e'): 1.375, ('##e', '##s'): 3.6666666666666665, ('l', '##i'): 5.5, ('##i', '##k'): 5.5, ('##k', '##e'): 3.6666666666666665, ('a', '##n'): 5.5, ('fru', '##i'): 5.5, ('##i', '##t'): 1.375, ('b', '##a'): 6.285714285714286, ('##n', '##a'): 3.142857142857143, ('##a', '##.'): 3.142857142857143, ('s', '##h'): 7.333333333333333, ('t', '##old'): 7.333333333333333, ('h', '##i'): 2.75, ('##h', '##a'): 2.0952380952380953, ('kn', '##e'): 3.6666666666666665, ('##e', '##w'): 3.6666666666666665, ('h', '##e'): 1.8333333333333333, ('##e', '##d.'): 3.6666666666666665}\n",
            "\n",
            "  Passed!\n",
            "\n",
            "\n",
            "--- TEST: merge_best_pair(self, best_pair, vocab, word_tokenizations, tokens2word) ---\n",
            "\tCase  0:\tCORRECT\t\tbest_pair: ('##w', '##;')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w', '##;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}}\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tCorrect vocab: {'a', '##n', 't', ' ', '##h', '##w;', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}}\n",
            "\tCase  1:\tCORRECT\t\tbest_pair: ('##o', '##w;')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##o', '##w;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}}\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tCorrect vocab: {'a', '##n', 't', ' ', '##h', '##w;', '<UNK>', 'h', '##d', 'o', 'm', '##r', 'l', 's', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##o', '##l', '##s', '##a', '##;', '##i', '##w', '##t', '##m', '##e'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}\n",
            "\tCase  2:\tCORRECT\t\tbest_pair: ('##r', '##ow;')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##r', '##ow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}}\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##row;', '##t'}\tCorrect vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}\n",
            "\tCase  3:\tCORRECT\t\tbest_pair: ('##r', '##row;')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##r', '##row;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}}\tYour vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}\n",
            "\tCase  4:\tCORRECT\t\tbest_pair: ('##r', '##u')\tvocab: {'a', '##n', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##r', '##u', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}\n",
            "\tCase  5:\tCORRECT\t\tbest_pair: ('a', '##rrow;')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('a', '##rrow;'), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}\n",
            "\tCase  6:\tCORRECT\t\tbest_pair: ('f', '##ru')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('f', '##ru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}\n",
            "\tCase  7:\tCORRECT\t\tbest_pair: ('b', '##o')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('b', '##o', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}\n",
            "\tCase  8:\tCORRECT\t\tbest_pair: ('##o', '##l')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##o', '##l', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}\n",
            "\tCase  9:\tCORRECT\t\tbest_pair: ('##ol', '##d')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##ol', '##d'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}}\tYour vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', 'l', '##old', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}\n",
            "\tCase 10:\tCORRECT\t\tbest_pair: ('f', '##l')\tvocab: {'a', '##n', '##ru', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('f', '##l', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}}\tYour vocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}\n",
            "\tCase 11:\tCORRECT\t\tbest_pair: ('o', '##l')\tvocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('o', '##l', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}}\tYour vocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}\n",
            "\tCase 12:\tCORRECT\t\tbest_pair: ('ol', '##d')\tvocab: {'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('ol', '##d'), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}}\tYour vocab: {'old', 'a', '##n', '##ru', ' ', 'fl', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}\n",
            "\tCase 13:\tCORRECT\t\tbest_pair: ('##d', '##.')\tvocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d', '##.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}}\tYour vocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}\n",
            "\tCase 14:\tCORRECT\t\tbest_pair: ('k', '##n')\tvocab: {'old', 'a', '##n', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('k', '##n', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}}\tYour vocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tCorrect vocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}\n",
            "\tCase 15:\tCORRECT\t\tbest_pair: ('s', '##h')\tvocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t'}\tword_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('s', '##h', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\ttokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}}\tYour vocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t', 'sh'}\tCorrect vocab: {'old', 'a', '##n', 'kn', '##ru', 'fl', ' ', '##h', '<UNK>', 'h', '##d', '##old', 'l', '##k', '##ow;', 'k', '##u', 'b', '##.', 'f', 'bo', '##ol', '##d.', '##s', '##a', '##;', '##w', '##m', '##e', 't', '##w;', 'o', 'fru', 'm', '##r', 's', 'ol', 'arrow;', '##o', '##l', '##i', '##rrow;', '##row;', '##t', 'sh'}\tYour word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tCorrect word_tokenizations: {'the': ('t', '##h', '##e'), 'old': ('old',), 'man': ('m', '##a', '##n'), 'boat.': ('bo', '##a', '##t', '##.'), 'time': ('t', '##i', '##m', '##e'), 'flies': ('fl', '##i', '##e', '##s'), 'like': ('l', '##i', '##k', '##e'), 'an': ('a', '##n'), 'arrow;': ('arrow;',), 'fruit': ('fru', '##i', '##t'), 'a': ('a',), 'banana.': ('b', '##a', '##n', '##a', '##n', '##a', '##.'), 'she': ('sh', '##e'), 'told': ('t', '##old'), 'him': ('h', '##i', '##m'), 'that': ('t', '##h', '##a', '##t'), 'knew': ('kn', '##e', '##w'), 'he': ('h', '##e'), 'lied.': ('l', '##i', '##e', '##d.')}\tYour tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'fruit', 'time', 'flies', 'him', 'lied.'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'he', 'him'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}\tCorrect tokens2word: {'t': {'the', 'told', 'that', 'time'}, '##h': {'the', 'that', 'she'}, '##e': {'like', 'knew', 'time', 'she', 'the', 'flies', 'he', 'lied.'}, 'o': {'old'}, '##l': {'old', 'told', 'flies'}, '##d': {'lied.', 'old', 'told'}, 'm': {'man'}, '##a': {'banana.', 'boat.', 'that', 'man'}, '##n': {'an', 'knew', 'banana.', 'man'}, 'b': {'banana.', 'boat.'}, '##o': {'boat.', 'arrow;', 'told'}, '##t': {'boat.', 'that', 'fruit'}, '##.': {'banana.', 'boat.', 'lied.'}, '##i': {'like', 'him', 'fruit', 'lied.', 'time', 'flies'}, '##m': {'him', 'time'}, 'f': {'fruit', 'flies'}, '##s': {'flies'}, 'l': {'like', 'lied.'}, '##k': {'like'}, 'a': {'an', 'arrow;', 'a'}, '##r': {'arrow;', 'fruit'}, '##w': {'knew', 'arrow;'}, '##;': {'arrow;'}, '##u': {'fruit'}, 's': {'she'}, 'h': {'him', 'he'}, 'k': {'knew'}, '##w;': {'arrow;'}, '##ow;': {'arrow;'}, '##row;': {'arrow;'}, '##rrow;': {'arrow;'}, '##ru': {'fruit'}, 'arrow;': {'arrow;'}, 'fru': {'fruit'}, 'bo': {'boat.'}, '##ol': {'told'}, '##old': {'told'}, 'fl': {'flies'}, 'ol': {'old'}, 'old': {'old'}, '##d.': {'lied.'}, 'kn': {'knew'}, 'sh': {'she'}}\n",
            "\n",
            "  Passed!\n",
            "\n",
            "\n",
            "--- TEST: tokenize(self, sentence) ---\n",
            "\tCase  0:\tCORRECT\t\tsentence: she read the book 1984.\tYour tokenization: ['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>']\tCorrect tokenization: ['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>']\n",
            "\tCase  1:\tCORRECT\t\tsentence: time waits for no one.\tYour tokenization: ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']\tCorrect tokenization: ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']\n",
            "\n",
            "  Passed!\n",
            "\n",
            "\n",
            "--- TEST: detokenize(self, tokens) ---\n",
            "\tCase  0:\tCORRECT\t\ttokens: ['sh', '##e', ' ', '<UNK>', ' ', 't', '##h', '##e', ' ', 'bo', '##o', '##k', ' ', '<UNK>']\tYour sentence: she <UNK> the book <UNK>\tCorrect sentence: she <UNK> the book <UNK>\n",
            "\tCase  1:\tCORRECT\t\ttokens: ['t', '##i', '##m', '##e', ' ', '<UNK>', ' ', 'f', '##o', '##r', ' ', '<UNK>', ' ', 'o', '##n', '##e', '##.']\tYour sentence: time <UNK> for <UNK> one.\tCorrect sentence: time <UNK> for <UNK> one.\n",
            "\n",
            "  Passed!\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckTokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCbUEgXfLz81"
      },
      "source": [
        "## Run your tokenizer\n",
        "\n",
        "Now that you have written your tokenizer, we can run it on real data. If implemented correctly. your tokenizer should train in **less than 4 minutes**. If your code takes too long, make sure your `merge_best_pair(best_pair, ...)` function uses the `tokens2word` dictionary to only iterate over the words that use one of the tokens in `best_pair`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6161ccd5dcc14353884876f717a012b7",
            "7ae74e8391d44f4baca20f5225eea4d6",
            "2497a2bc0a6e4cbc846fcf248537f273",
            "36eb1bd6761543daa53a69043fe0aa23",
            "f39ecda8be43422f84defba1ec85278f",
            "7e8b5a7771574e23b3984fb1618c3e5e",
            "2b9e219db59143ce8d0ae7ebed8f16b2",
            "7e33fe9337d949dda61beb09aee747d7",
            "7d21c21222c34c5e923d075e9aa65b7f",
            "2628e2b9c132450db5ff97763465c684",
            "bb798ecc9b5046079944a3c5ac6de113"
          ]
        },
        "id": "lPdN2aTVvTu9",
        "outputId": "ad6b82c4-b912-4579-9321-0b1fe4f15efd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/603 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6161ccd5dcc14353884876f717a012b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tMerging  ('##^', '##^')\n",
            "\tMerging  ('>', '##>')\n",
            "\tMerging  ('##>', '##>')\n",
            "\tMerging  ('|', '##^^')\n",
            "\tMerging  ('|', '##\u00a9')\n",
            "\tMerging  ('m', '##\u00f3')\n",
            "\tMerging  ('e', '##x')\n",
            "\tMerging  ('x', '##-')\n",
            "\tMerging  ('b', '##\u00e4')\n",
            "\tMerging  ('#', '##b')\n",
            "\tMerging  ('d', '##\u00ed')\n",
            "\tMerging  ('x', '##z')\n",
            "\tMerging  ('|\u00a9', '##m')\n",
            "\tMerging  ('|\u00a9m', '##m')\n",
            "\tMerging  ('|\u00a9mm', '##v')\n",
            "\tMerging  ('f', '##\u00e9')\n",
            "\tMerging  ('b\u00e4', '##c')\n",
            "\tMerging  ('b\u00e4c', '##k')\n",
            "\tMerging  ('xz', '##c')\n",
            "\tMerging  ('(', '##j')\n",
            "\tMerging  ('##b', '(j')\n",
            "\tMerging  ('##b(j', '##p')\n",
            "\tMerging  ('##b(jp', '##)')\n",
            "\tMerging  ('r', '##&')\n",
            "\tMerging  ('r&', '##b')\n",
            "\tMerging  ('##b', '##&')\n",
            "\tMerging  ('b', '##b&')\n",
            "\tMerging  ('d', '##&')\n",
            "\tMerging  ('##z', '##z')\n",
            "\tMerging  ('a', '##&')\n",
            "\tMerging  ('a', '##*')\n",
            "\tMerging  ('##u', '##\u00ed')\n",
            "\tMerging  ('##q', '##u\u00ed')\n",
            "\tMerging  ('##(', '##p')\n",
            "\tMerging  ('##(', '##m')\n",
            "\tMerging  ('##f', '##\u00e9')\n",
            "\tMerging  ('o', '##f')\n",
            "\tMerging  ('q', '##u')\n",
            "\tMerging  ('q', '##:')\n",
            "\tMerging  ('ex', '##p')\n",
            "\tMerging  ('\u201c', '##w')\n",
            "\tMerging  ('d&', '##d')\n",
            "\tMerging  ('##q', '##u')\n",
            "\tMerging  ('##-', '##q')\n",
            "\tMerging  ('e', '##qu')\n",
            "\tMerging  ('##m', '##\u2510')\n",
            "\tMerging  ('##\u2510', '##.')\n",
            "\tMerging  ('##l', '##-q')\n",
            "\tMerging  ('a', '##l-q')\n",
            "\tMerging  ('`', '##l')\n",
            "\tMerging  ('##\u00bf', '##h')\n",
            "\tMerging  ('b\u00e4ck', '##h')\n",
            "\tMerging  ('(j', '##v')\n",
            "\tMerging  ('##p', '##|')\n",
            "\tMerging  ('##m\u2510', '##s')\n",
            "\tMerging  ('##s', '##(m')\n",
            "\tMerging  ('##y', '##s(m')\n",
            "\tMerging  ('##\u2510', '##s')\n",
            "\tMerging  ('(jv', '##s')\n",
            "\tMerging  ('(jvs', '##)')\n",
            "\tMerging  ('a*', '##s')\n",
            "\tMerging  ('##f', '##f')\n",
            "\tMerging  ('e', '##ff')\n",
            "\tMerging  ('of', '##f')\n",
            "\tMerging  ('e', '##v')\n",
            "\tMerging  ('##t', '##ys(m')\n",
            "\tMerging  ('bb&', '##t')\n",
            "\tMerging  ('d\u00ed', '##t')\n",
            "\tMerging  ('d\u00edt', '##r')\n",
            "\tMerging  ('m\u00f3', '##r')\n",
            "\tMerging  ('j', '##u')\n",
            "\tMerging  ('##i', '##\u0146')\n",
            "\tMerging  ('n', '##i\u0146')\n",
            "\tMerging  ('f\u00e9', '##i')\n",
            "\tMerging  ('|\u00a9mmv', '##i')\n",
            "\tMerging  ('|\u00a9mmvi', '##i')\n",
            "\tMerging  ('|\u00a9mmvii', '##i')\n",
            "\tMerging  ('u', '##p')\n",
            "\tMerging  ('up', '##-')\n",
            "\tMerging  ('##x', 'up')\n",
            "\tMerging  ('u', '##k')\n",
            "\tMerging  ('uk', '##\u2019')\n",
            "\tMerging  ('uk\u2019', '##s')\n",
            "\tMerging  ('##a', '##f\u00e9')\n",
            "\tMerging  ('c', '##af\u00e9')\n",
            "\tMerging  ('##a', '##m\u2510s')\n",
            "\tMerging  ('##a', '##am\u2510s')\n",
            "\tMerging  ('##d', '##aam\u2510s')\n",
            "\tMerging  ('##d', '##daam\u2510s')\n",
            "\tMerging  ('s', '##ddaam\u2510s')\n",
            "\tMerging  ('##a', '##qu\u00ed')\n",
            "\tMerging  ('##\u00bfh', '##a')\n",
            "\tMerging  ('##\u00bfha', '##v')\n",
            "\tMerging  ('al-q', '##a')\n",
            "\tMerging  ('ni\u0146', '##a')\n",
            "\tMerging  ('xzc', '##a')\n",
            "\tMerging  ('\u201cw', '##a')\n",
            "\tMerging  ('\u2510', '##a')\n",
            "\tMerging  ('\u2510a', '##c')\n",
            "\tMerging  ('\u2510ac', '##h')\n",
            "\tMerging  ('\u2510ach', '##i')\n",
            "\tMerging  ('t', '##h')\n",
            "\tMerging  ('##(p', '##o')\n",
            "\tMerging  ('##(po', '##r')\n",
            "\tMerging  ('##(por', '##t')\n",
            "\tMerging  ('##(port', '##h')\n",
            "\tMerging  ('##(porth', '##m')\n",
            "\tMerging  ('##(porthm', '##a')\n",
            "\tMerging  ('##(porthma', '##d')\n",
            "\tMerging  ('##(porthmad', '##o')\n",
            "\tMerging  ('##(porthmado', '##g')\n",
            "\tMerging  ('##(porthmadog', '##,')\n",
            "\tMerging  ('##o', '##aqu\u00ed')\n",
            "\tMerging  ('j', '##oaqu\u00ed')\n",
            "\tMerging  ('##o', '##p|')\n",
            "\tMerging  ('t', '##op|')\n",
            "\tMerging  ('##tys(m', '##o')\n",
            "\tMerging  ('`l', '##o')\n",
            "\tMerging  ('uk', '##)')\n",
            "\tMerging  ('##|', '##.')\n",
            "\tMerging  ('##d', '##|')\n",
            "\tMerging  ('##d', '##:')\n",
            "\tMerging  ('@', '##d')\n",
            "\tMerging  ('##d:', '@d')\n",
            "\tMerging  ('##k', '##d:@d')\n",
            "\tMerging  ('##r', '##kd:@d')\n",
            "\tMerging  ('##r', '##rkd:@d')\n",
            "\tMerging  ('##o', '##rrkd:@d')\n",
            "\tMerging  ('##w', '##orrkd:@d')\n",
            "\tMerging  ('##w', '##worrkd:@d')\n",
            "\tMerging  ('##wworrkd:@d', '##o')\n",
            "\tMerging  ('##-', '##j')\n",
            "\tMerging  ('##-j', '##j')\n",
            "\tMerging  ('##-jj', '##a')\n",
            "\tMerging  ('##-jja', '##r')\n",
            "\tMerging  ('##-jjar', '##,')\n",
            "\tMerging  ('##a', '##-jjar,')\n",
            "\tMerging  ('##b', '##j')\n",
            "\tMerging  ('o', '##bj')\n",
            "\tMerging  ('##u', '##bj')\n",
            "\tMerging  ('s', '##ubj')\n",
            "\tMerging  ('##bj', '##k')\n",
            "\tMerging  ('##bjk', '##s')\n",
            "\tMerging  ('##n', '##a-jjar,')\n",
            "\tMerging  ('##l', '##na-jjar,')\n",
            "\tMerging  ('a', '##lna-jjar,')\n",
            "\tMerging  ('##n', '##d|')\n",
            "\tMerging  ('##n', '##tys(mo')\n",
            "\tMerging  ('##o', '##ntys(mo')\n",
            "\tMerging  ('m', '##ontys(mo')\n",
            "\tMerging  ('##n', '##\u00e9')\n",
            "\tMerging  ('##n', '##n\u00e9')\n",
            "\tMerging  ('##n', '##\u2510.')\n",
            "\tMerging  ('##o', '##n\u2510.')\n",
            "\tMerging  ('##i', '##on\u2510.')\n",
            "\tMerging  ('##i', '##ion\u2510.')\n",
            "\tMerging  ('##s', '##iion\u2510.')\n",
            "\tMerging  ('##s', '##siion\u2510.')\n",
            "\tMerging  ('m', '##ssiion\u2510.')\n",
            "\tMerging  ('`lo', '##n')\n",
            "\tMerging  ('`lon', '##g')\n",
            "\tMerging  ('f\u00e9i', '##n')\n",
            "\tMerging  ('joaqu\u00ed', '##n')\n",
            "\tMerging  ('joaqu\u00edn', '##,')\n",
            "\tMerging  ('montys(mo', '##n')\n",
            "\tMerging  ('montys(mon', '##t')\n",
            "\tMerging  ('montys(mont', '##g')\n",
            "\tMerging  ('montys(montg', '##o')\n",
            "\tMerging  ('montys(montgo', '##m')\n",
            "\tMerging  ('\u201cwa', '##n')\n",
            "\tMerging  ('\u201cwan', '##t')\n",
            "\tMerging  ('up-', '##t')\n",
            "\tMerging  ('up-', '##c')\n",
            "\tMerging  ('ex', '##-')\n",
            "\tMerging  ('ex-', '##b')\n",
            "\tMerging  ('ex-b', '##r')\n",
            "\tMerging  ('ex-br', '##i')\n",
            "\tMerging  ('ex-bri', '##t')\n",
            "\tMerging  ('ex-brit', '##i')\n",
            "\tMerging  ('ex-briti', '##s')\n",
            "\tMerging  ('ex-britis', '##h')\n",
            "\tMerging  ('up-t', '##o')\n",
            "\tMerging  ('up-to', '##-')\n",
            "\tMerging  ('up-to', '##,')\n",
            "\tMerging  ('##p', 'up-to,')\n",
            "\tMerging  ('##m', '##pup-to,')\n",
            "\tMerging  ('up-to-', '##d')\n",
            "\tMerging  ('up-to-d', '##a')\n",
            "\tMerging  ('up-to-da', '##t')\n",
            "\tMerging  ('##n', 'up-t')\n",
            "\tMerging  ('##o', '##nup-t')\n",
            "\tMerging  ('##r', '##onup-t')\n",
            "\tMerging  ('##f', '##ronup-t')\n",
            "\tMerging  ('x-', '##r')\n",
            "\tMerging  ('x-', '##t')\n",
            "\tMerging  ('x-t', '##b')\n",
            "\tMerging  ('##r', 'x-tb')\n",
            "\tMerging  ('##d', '##rx-tb')\n",
            "\tMerging  ('x-r', '##a')\n",
            "\tMerging  ('x-ra', '##y')\n",
            "\tMerging  ('x-ra', '##t')\n",
            "\tMerging  ('##a', '##q')\n",
            "\tMerging  ('##q', '##x')\n",
            "\tMerging  ('u', '##q')\n",
            "\tMerging  ('uq', '##r')\n",
            "\tMerging  ('uqr', '##d')\n",
            "\tMerging  ('##r', '##aq')\n",
            "\tMerging  ('##f', '##aq')\n",
            "\tMerging  ('w', '##aq')\n",
            "\tMerging  ('i', '##raq')\n",
            "\tMerging  ('##h', '##faq')\n",
            "\tMerging  ('##s', '##hfaq')\n",
            "\tMerging  ('i', '##shfaq')\n",
            "\tMerging  ('a', '##shfaq')\n",
            "\tMerging  ('waq', '##t')\n",
            "\tMerging  ('##o', '##qx')\n",
            "\tMerging  ('##l', '##oqx')\n",
            "\tMerging  ('ex-', '##v')\n",
            "\tMerging  ('ex-', '##c')\n",
            "\tMerging  ('ex-c', '##h')\n",
            "\tMerging  ('ex-', '##g')\n",
            "\tMerging  ('ex-g', '##u')\n",
            "\tMerging  ('ex-c', '##i')\n",
            "\tMerging  ('ex-v', '##i')\n",
            "\tMerging  ('ex-ch', '##a')\n",
            "\tMerging  ('ex-ci', '##a')\n",
            "\tMerging  ('ex-vi', '##c')\n",
            "\tMerging  ('##o', 'ex-vi')\n",
            "\tMerging  ('##s', '##oex-vi')\n",
            "\tMerging  ('ex-cha', '##n')\n",
            "\tMerging  ('ex-chan', '##c')\n",
            "\tMerging  ('##]', '##.')\n",
            "\tMerging  ('##k', '##].')\n",
            "\tMerging  ('##c', '##k].')\n",
            "\tMerging  ('##a', '##ck].')\n",
            "\tMerging  ('##b', '##ack].')\n",
            "\tMerging  ('[', '##back].')\n",
            "\tMerging  ('##q', '##b')\n",
            "\tMerging  ('i', '##qb')\n",
            "\tMerging  ('iqb', '##a')\n",
            "\tMerging  ('iqba', '##l')\n",
            "\tMerging  ('up-c', '##l')\n",
            "\tMerging  ('##r', 'up-cl')\n",
            "\tMerging  ('##rup-cl', '##a')\n",
            "\tMerging  ('##rup-cla', '##s')\n",
            "\tMerging  ('##rup-clas', '##s')\n",
            "\tMerging  ('up-c', '##o')\n",
            "\tMerging  ('up-co', '##m')\n",
            "\tMerging  ('up-com', '##i')\n",
            "\tMerging  ('up-comi', '##n')\n",
            "\tMerging  ('up-comin', '##g')\n",
            "\tMerging  ('##/', '##h')\n",
            "\tMerging  ('##/', '##f')\n",
            "\tMerging  ('##s', '##/f')\n",
            "\tMerging  ('##s', '##s/f')\n",
            "\tMerging  ('##t', '##ss/f')\n",
            "\tMerging  ('##l', '##tss/f')\n",
            "\tMerging  ('##u', '##ltss/f')\n",
            "\tMerging  ('##ultss/f', '##i')\n",
            "\tMerging  ('##ultss/fi', '##x')\n",
            "\tMerging  ('##ultss/fix', '##t')\n",
            "\tMerging  ('##ultss/fixt', '##u')\n",
            "\tMerging  ('##ultss/fixtu', '##r')\n",
            "\tMerging  ('##k', '##/')\n",
            "\tMerging  ('##k/', '##c')\n",
            "\tMerging  ('##k/', '##u')\n",
            "\tMerging  ('##u', '##k/u')\n",
            "\tMerging  ('(', '##uk/u')\n",
            "\tMerging  ('##s', '##k/c')\n",
            "\tMerging  ('##l', '##sk/c')\n",
            "\tMerging  ('##l', '##lsk/c')\n",
            "\tMerging  ('(uk/u', '##s')\n",
            "\tMerging  ('##i', '##llsk/c')\n",
            "\tMerging  ('s', '##illsk/c')\n",
            "\tMerging  ('(uk/us', '##a')\n",
            "\tMerging  ('(uk/usa', '##)')\n",
            "\tMerging  ('sillsk/c', '##o')\n",
            "\tMerging  ('sillsk/co', '##n')\n",
            "\tMerging  ('sillsk/con', '##n')\n",
            "\tMerging  ('##-', '##qu')\n",
            "\tMerging  ('##-qu', '##a')\n",
            "\tMerging  ('##-qua', '##k')\n",
            "\tMerging  ('##t', '##-quak')\n",
            "\tMerging  ('##s', '##t-quak')\n",
            "\tMerging  ('##o', '##st-quak')\n",
            "\tMerging  ('p', '##ost-quak')\n",
            "\tMerging  ('##-qua', '##l')\n",
            "\tMerging  ('##-qua', '##r')\n",
            "\tMerging  ('##-quar', '##t')\n",
            "\tMerging  ('##-qual', '##i')\n",
            "\tMerging  ('##-quali', '##t')\n",
            "\tMerging  ('##-qualit', '##y')\n",
            "\tMerging  ('##y', '##].')\n",
            "\tMerging  ('##g', '##y].')\n",
            "\tMerging  ('##a', '##y].')\n",
            "\tMerging  ('##a', '##ay].')\n",
            "\tMerging  ('##d', '##aay].')\n",
            "\tMerging  ('##r', '##daay].')\n",
            "\tMerging  ('##u', '##rdaay].')\n",
            "\tMerging  ('##t', '##urdaay].')\n",
            "\tMerging  ('s', '##turdaay].')\n",
            "\tMerging  ('##n', '##gy].')\n",
            "\tMerging  ('##i', '##ngy].')\n",
            "\tMerging  ('##l', '##ingy].')\n",
            "\tMerging  ('##p', '##lingy].')\n",
            "\tMerging  ('##p', '##plingy].')\n",
            "\tMerging  ('##u', '##pplingy].')\n",
            "\tMerging  ('s', '##upplingy].')\n",
            "\tMerging  ('##v', '##/')\n",
            "\tMerging  ('##p', '##/')\n",
            "\tMerging  ('##p/', '##r')\n",
            "\tMerging  ('##o', '##p/r')\n",
            "\tMerging  ('p', '##op/r')\n",
            "\tMerging  ('pop/r', '##o')\n",
            "\tMerging  ('pop/ro', '##c')\n",
            "\tMerging  ('pop/roc', '##k')\n",
            "\tMerging  ('[', '##w')\n",
            "\tMerging  ('##)', '##,')\n",
            "\tMerging  ('##)', '##.')\n",
            "\tMerging  ('##x', '##).')\n",
            "\tMerging  ('##i', '##x).')\n",
            "\tMerging  ('m', '##ix).')\n",
            "\tMerging  ('(', '##f')\n",
            "\tMerging  ('(f', '##),')\n",
            "\tMerging  ('(f', '##)')\n",
            "\tMerging  ('##w', '(f),')\n",
            "\tMerging  ('##w', '(f)')\n",
            "\tMerging  ('##g', '##w(f)')\n",
            "\tMerging  ('##p', '(f),')\n",
            "\tMerging  ('##u', '(f),')\n",
            "\tMerging  ('##d', '(f),')\n",
            "\tMerging  ('##d', '##d(f),')\n",
            "\tMerging  ('##a', '##w(f),')\n",
            "\tMerging  ('##n', '##u(f),')\n",
            "\tMerging  ('iraq', '##.')\n",
            "\tMerging  ('##\u2019', '##s')\n",
            "\tMerging  ('i', '##\u2019')\n",
            "\tMerging  ('i\u2019', '##m')\n",
            "\tMerging  ('##i', '##d:')\n",
            "\tMerging  ('##d', '##d:')\n",
            "\tMerging  ('##d', '##dd:')\n",
            "\tMerging  ('##a', '##id:')\n",
            "\tMerging  ('s', '##aid:')\n",
            "\tMerging  ('##g', '##id:')\n",
            "\tMerging  ('##n', '##gid:')\n",
            "\tMerging  ('##d', '##ngid:')\n",
            "\tMerging  ('a', '##dngid:')\n",
            "\tMerging  ('##n', '##ngid:')\n",
            "\tMerging  ('##u', '##nngid:')\n",
            "\tMerging  ('##l', '##unngid:')\n",
            "\tMerging  ('##c', '##lunngid:')\n",
            "\tMerging  ('i', '##clunngid:')\n",
            "\tMerging  ('w', '##h')\n",
            "\tMerging  ('##\u2019', '##t')\n",
            "\tMerging  ('##\u2019', '##d')\n",
            "\tMerging  ('##n', '##\u2019t')\n",
            "\tMerging  ('##d', '##n\u2019t')\n",
            "\tMerging  ('##i', '##dn\u2019t')\n",
            "\tMerging  ('d', '##idn\u2019t')\n",
            "\tMerging  ('##o', '##n\u2019t')\n",
            "\tMerging  ('d', '##on\u2019t')\n",
            "\tMerging  ('##c', '##k')\n",
            "\tMerging  ('(', '##c')\n",
            "\tMerging  ('(c', '##)')\n",
            "\tMerging  ('(c', '##ck')\n",
            "\tMerging  ('(c', '##h')\n",
            "\tMerging  ('(c', '##p')\n",
            "\tMerging  ('##ff', '##)')\n",
            "\tMerging  ('##a', '##ff)')\n",
            "\tMerging  ('(', '##aff)')\n",
            "\tMerging  ('##a', '(cp')\n",
            "\tMerging  ('##a(cp', '##o')\n",
            "\tMerging  ('##a(cpo', '##),')\n",
            "\tMerging  ('##s', '(c)')\n",
            "\tMerging  ('##s', '##s(c)')\n",
            "\tMerging  ('##p', '##s(c)')\n",
            "\tMerging  ('##m', '##s(c)')\n",
            "\tMerging  ('##i', '(ch')\n",
            "\tMerging  ('##h', '##i(ch')\n",
            "\tMerging  ('##w', '##hi(ch')\n",
            "\tMerging  ('##i(ch', '##i')\n",
            "\tMerging  ('##o', '##i(chi')\n",
            "\tMerging  ('##k', '##oi(chi')\n",
            "\tMerging  ('(ch', '##n')\n",
            "\tMerging  ('(chn', '##o')\n",
            "\tMerging  ('(chno', '##l')\n",
            "\tMerging  ('(chnol', '##o')\n",
            "\tMerging  ('(chnolo', '##g')\n",
            "\tMerging  ('(chnolog', '##y')\n",
            "\tMerging  ('(chnology', '##,')\n",
            "\tMerging  ('##s', '##dd:')\n",
            "\tMerging  ('a', '##sdd:')\n",
            "\tMerging  ('##n', '##sdd:')\n",
            "\tMerging  ('##n', '##nsdd:')\n",
            "\tMerging  ('##a', '##nnsdd:')\n",
            "\tMerging  ('##t', '##annsdd:')\n",
            "\tMerging  ('##r', '##tannsdd:')\n",
            "\tMerging  ('##zz', '##y')\n",
            "\tMerging  ('##i', '##zzy')\n",
            "\tMerging  ('f', '##izzy')\n",
            "\tMerging  ('t', '##izzy')\n",
            "\tMerging  ('(', '##w')\n",
            "\tMerging  ('(w', '##h')\n",
            "\tMerging  ('##b', '(w')\n",
            "\tMerging  ('(w', '##b')\n",
            "\tMerging  ('(wb', '##)')\n",
            "\tMerging  ('##u', '(wb)')\n",
            "\tMerging  ('##b(w', '##r')\n",
            "\tMerging  ('##b(wr', '##)')\n",
            "\tMerging  ('##-', '##b')\n",
            "\tMerging  ('##-', '##-b')\n",
            "\tMerging  ('##f', '##--b')\n",
            "\tMerging  ('uk', '##-b')\n",
            "\tMerging  ('uk-b', '##a')\n",
            "\tMerging  ('uk-ba', '##s')\n",
            "\tMerging  ('##o', '##f--b')\n",
            "\tMerging  ('e', '##-b')\n",
            "\tMerging  ('##m', 'e-b')\n",
            "\tMerging  ('e-b', '##o')\n",
            "\tMerging  ('e-b', '##a')\n",
            "\tMerging  ('e-ba', '##s')\n",
            "\tMerging  ('##n', 'e-bas')\n",
            "\tMerging  ('##o', '##ne-bas')\n",
            "\tMerging  ('##i', '##one-bas')\n",
            "\tMerging  ('##t', '##ione-bas')\n",
            "\tMerging  ('##a', '##tione-bas')\n",
            "\tMerging  ('##c', '##atione-bas')\n",
            "\tMerging  ('##u', '##catione-bas')\n",
            "\tMerging  ('##d', '##ucatione-bas')\n",
            "\tMerging  ('##-', '##f')\n",
            "\tMerging  ('##ff', '##-f')\n",
            "\tMerging  ('##ff-f', '##,')\n",
            "\tMerging  ('##u', '##ff-f')\n",
            "\tMerging  ('##t', '##uff-f')\n",
            "\tMerging  ('s', '##tuff-f')\n",
            "\tMerging  ('stuff-f', '##i')\n",
            "\tMerging  ('stuff-fi', '##l')\n",
            "\tMerging  ('stuff-fil', '##l')\n",
            "\tMerging  ('##x', '##-f')\n",
            "\tMerging  ('##x-f', '##r')\n",
            "\tMerging  ('##a', '##x-fr')\n",
            "\tMerging  ('t', '##ax-fr')\n",
            "\tMerging  ('off', '##-')\n",
            "\tMerging  ('off-', '##p')\n",
            "\tMerging  ('off-p', '##u')\n",
            "\tMerging  ('##s', 'off-p')\n",
            "\tMerging  ('off-pu', '##t')\n",
            "\tMerging  ('off-put', '##t')\n",
            "\tMerging  ('##soff-p', '##i')\n",
            "\tMerging  ('off-putt', '##i')\n",
            "\tMerging  ('##soff-pi', '##n')\n",
            "\tMerging  ('##soff-pin', '##n')\n",
            "\tMerging  ('off-putti', '##n')\n",
            "\tMerging  ('off-puttin', '##g')\n",
            "\tMerging  ('off-putting', '##,')\n",
            "\tMerging  ('off-', '##c')\n",
            "\tMerging  ('##x', '##-')\n",
            "\tMerging  ('##x-', '##m')\n",
            "\tMerging  ('##x-', '##w')\n",
            "\tMerging  ('##x-', '##y')\n",
            "\tMerging  ('##x-', '##h')\n",
            "\tMerging  ('##i', '##x-h')\n",
            "\tMerging  ('s', '##ix-h')\n",
            "\tMerging  ('##i', '##x-w')\n",
            "\tMerging  ('s', '##ix-w')\n",
            "\tMerging  ('##i', '##x-y')\n",
            "\tMerging  ('s', '##ix-y')\n",
            "\tMerging  ('##x-m', '##o')\n",
            "\tMerging  ('six-h', '##o')\n",
            "\tMerging  ('six-ho', '##u')\n",
            "\tMerging  ('six-hou', '##r')\n",
            "\tMerging  ('##i', '##x-mo')\n",
            "\tMerging  ('s', '##ix-mo')\n",
            "\tMerging  ('##l', '##x-mo')\n",
            "\tMerging  ('##l', '##lx-mo')\n",
            "\tMerging  ('##llx-mo', '##r')\n",
            "\tMerging  ('##llx-mor', '##r')\n",
            "\tMerging  ('##llx-morr', '##i')\n",
            "\tMerging  ('##llx-morri', '##s')\n",
            "\tMerging  ('six-mo', '##n')\n",
            "\tMerging  ('six-mon', '##t')\n",
            "\tMerging  ('six-mont', '##h')\n",
            "\tMerging  ('##g', '##q')\n",
            "\tMerging  ('##gq', '##v')\n",
            "\tMerging  ('##r', '##gqv')\n",
            "\tMerging  ('##gq', '##i')\n",
            "\tMerging  ('##rgqv', '##i')\n",
            "\tMerging  ('##rgqvi', '##s')\n",
            "\tMerging  ('##rgqvis', '##t')\n",
            "\tMerging  ('##gqi', '##n')\n",
            "\tMerging  ('##gqin', '##g')\n",
            "\tMerging  ('##n', '##gqing')\n",
            "\tMerging  ('##o', '##ngqing')\n",
            "\tMerging  ('##h', '##ongqing')\n",
            "\tMerging  ('c', '##hongqing')\n",
            "\tMerging  ('e-bo', '##o')\n",
            "\tMerging  ('e-boo', '##k')\n",
            "\tMerging  ('e-bo', '##r')\n",
            "\tMerging  ('e-bor', '##d')\n",
            "\tMerging  ('##c', '##/h')\n",
            "\tMerging  ('##c', '##c/h')\n",
            "\tMerging  ('t', '##o')\n",
            "\tMerging  ('y', '##o')\n",
            "\tMerging  ('yo', '##u')\n",
            "\tMerging  ('##ck', 'yo')\n",
            "\tMerging  ('##ckyo', '##,')\n",
            "\tMerging  ('yo', '##w')\n",
            "\tMerging  ('##l', 'yow')\n",
            "\tMerging  ('##l', '##lyow')\n",
            "\tMerging  ('##-', 'yo')\n",
            "\tMerging  ('##-yo', '##m')\n",
            "\tMerging  ('##-yom', '##o')\n",
            "\tMerging  ('##-yomo', '##s')\n",
            "\tMerging  ('##-yomos', '##o')\n",
            "\tMerging  ('##-yomoso', '##m')\n",
            "\tMerging  ('##r', '##-yomosom')\n",
            "\tMerging  ('##h', '##r-yomosom')\n",
            "\tMerging  ('##c', '##hr-yomosom')\n",
            "\tMerging  ('##-yo', '##n')\n",
            "\tMerging  ('##-yon', '##-')\n",
            "\tMerging  ('##-yon-', '##y')\n",
            "\tMerging  ('##-yon', '##g')\n",
            "\tMerging  ('##l', '##-yong')\n",
            "\tMerging  ('##r', '##-yon-y')\n",
            "\tMerging  ('##r', '##l-yong')\n",
            "\tMerging  ('##a', '##r-yon-y')\n",
            "\tMerging  ('##a', '##rl-yong')\n",
            "\tMerging  ('##n', '##ckyo')\n",
            "\tMerging  ('##n', '##ckyo,')\n",
            "\tMerging  ('yo', '##r')\n",
            "\tMerging  ('yor', '##k')\n",
            "\tMerging  ('yo', '##v')\n",
            "\tMerging  ('##k', 'yov')\n",
            "\tMerging  ('##kyov', '##y')\n",
            "\tMerging  ('##kyovy', '##c')\n",
            "\tMerging  ('##kyovyc', '##h')\n",
            "\tMerging  ('yo', '##b')\n",
            "\tMerging  ('yob', '##b')\n",
            "\tMerging  ('##u', '##kyovych')\n",
            "\tMerging  ('yo', '##k')\n",
            "\tMerging  ('yov', '##,')\n",
            "\tMerging  ('##p', 'yov')\n",
            "\tMerging  ('##u', '##pyov')\n",
            "\tMerging  ('##u', '##upyov')\n",
            "\tMerging  ('##v', 'yo')\n",
            "\tMerging  ('york', '##,')\n",
            "\tMerging  ('##h', 'yo')\n",
            "\tMerging  ('##hyo', '##y')\n",
            "\tMerging  ('##d', '##hyoy')\n",
            "\tMerging  ('##u', '##dhyoy')\n",
            "\tMerging  ('##s', '##uupyov')\n",
            "\tMerging  ('##udhyoy', '##o')\n",
            "\tMerging  ('yob', '##o')\n",
            "\tMerging  ('yok', '##o')\n",
            "\tMerging  ('yoko', '##h')\n",
            "\tMerging  ('yobb', '##i')\n",
            "\tMerging  ('yobbi', '##s')\n",
            "\tMerging  ('yobbis', '##h')\n",
            "\tMerging  ('yov', '##i')\n",
            "\tMerging  ('##a', 'yovi')\n",
            "\tMerging  ('##l', '##ayovi')\n",
            "\tMerging  ('##s', '##layovi')\n",
            "\tMerging  ('##g', '##slayovi')\n",
            "\tMerging  ('##u', '##gslayovi')\n",
            "\tMerging  ('##ugslayovi', '##a')\n",
            "\tMerging  ('##ugslayovia', '##.')\n",
            "\tMerging  ('yokoh', '##a')\n",
            "\tMerging  ('yokoha', '##m')\n",
            "\tMerging  ('yokoham', '##a')\n",
            "\tMerging  ('yokohama', '##,')\n",
            "\tMerging  ('##n', '##ukyovych')\n",
            "\tMerging  ('##a', '##nukyovych')\n",
            "\tMerging  ('##udhyoyo', '##n')\n",
            "\tMerging  ('##udhyoyon', '##o')\n",
            "\tMerging  ('##udhyoyono', '##,')\n",
            "\tMerging  ('##vyo', '##n')\n",
            "\tMerging  ('##vyon', '##n')\n",
            "\tMerging  ('##l', 'yov,')\n",
            "\tMerging  ('##u', '##lyov,')\n",
            "\tMerging  ('##g', '##ulyov,')\n",
            "\tMerging  ('##a', '##gulyov,')\n",
            "\tMerging  ('##z', '##agulyov,')\n",
            "\tMerging  ('##r', '##zagulyov,')\n",
            "\tMerging  ('##a', '##agulyov,')\n",
            "\tMerging  ('##i', '##aagulyov,')\n",
            "\tMerging  ('##s', '##iaagulyov,')\n",
            "\tMerging  ('york', '##.')\n",
            "\tMerging  ('(wh', '##o')\n",
            "\tMerging  ('(who', '##)')\n",
            "\tMerging  ('(who', '##u')\n",
            "\tMerging  ('##t', '(whou')\n",
            "\tMerging  ('##t(whou', '##t')\n",
            "\tMerging  ('##i', '##t(whout')\n",
            "\tMerging  ('##y', '##/')\n",
            "\tMerging  ('##y/', '##u')\n",
            "\tMerging  ('##l', '##y/u')\n",
            "\tMerging  ('##o', '##ly/u')\n",
            "\tMerging  ('p', '##oly/u')\n",
            "\tMerging  ('poly/u', '##n')\n",
            "\tMerging  ('poly/un', '##i')\n",
            "\tMerging  ('poly/uni', '##v')\n",
            "\tMerging  ('##d', '##/')\n",
            "\tMerging  ('##d', '##d/')\n",
            "Vocab: [' ', '#', '##&', '##(', '##(m', '##(p', '##(po', '##(por', '##(port', '##(porth', '##(porthm', '##(porthma', '##(porthmad', '##(porthmado', '##(porthmadog', '##(porthmadog,', '##)', '##),', '##).', '##*', '##,', '##-', '##--b', '##-b', '##-f', '##-j', '##-jj', '##-jja', '##-jjar', '##-jjar,', '##-q', '##-qu', '##-qua', '##-quak', '##-qual', '##-quali', '##-qualit', '##-quality', '##-quar', '##-quart', '##-yo', '##-yom', '##-yomo', '##-yomos', '##-yomoso', '##-yomosom', '##-yon', '##-yon-', '##-yon-y', '##-yong', '##.', '##/', '##/f', '##/h', '##:', '##;', '##>', '##>>', '##]', '##].', '##^', '##^^', '##a', '##a(cp', '##a(cpo', '##a(cpo),', '##a-jjar,', '##aagulyov,', '##aam\u2510s', '##aay].', '##ack].', '##aff)', '##af\u00e9', '##agulyov,', '##aid:', '##am\u2510s', '##annsdd:', '##anukyovych', '##aq', '##aqu\u00ed', '##ar-yon-y', '##arl-yong', '##atione-bas', '##aw(f),', '##ax-fr', '##ay].', '##ayovi', '##b', '##b&', '##b(j', '##b(jp', '##b(jp)', '##b(w', '##b(wr', '##b(wr)', '##back].', '##bj', '##bjk', '##bjks', '##c', '##c/h', '##catione-bas', '##cc/h', '##chr-yomosom', '##ck', '##ck].', '##ckyo', '##ckyo,', '##clunngid:', '##d', '##d(f),', '##d/', '##d:', '##d:@d', '##daam\u2510s', '##daay].', '##dd(f),', '##dd/', '##dd:', '##ddaam\u2510s', '##ddd:', '##dhyoy', '##dngid:', '##dn\u2019t', '##drx-tb', '##ducatione-bas', '##d|', '##e', '##f', '##f--b', '##faq', '##ff', '##ff)', '##ff-f', '##ff-f,', '##fronup-t', '##f\u00e9', '##g', '##gid:', '##gq', '##gqi', '##gqin', '##gqing', '##gqv', '##gslayovi', '##gulyov,', '##gw(f)', '##gy].', '##h', '##hfaq', '##hi(ch', '##hongqing', '##hr-yomosom', '##hyo', '##hyoy', '##i', '##i(ch', '##i(chi', '##iaagulyov,', '##id:', '##idn\u2019t', '##iion\u2510.', '##illsk/c', '##ingy].', '##ione-bas', '##ion\u2510.', '##it(whout', '##ix).', '##ix-h', '##ix-mo', '##ix-w', '##ix-y', '##izzy', '##i\u0146', '##j', '##k', '##k/', '##k/c', '##k/u', '##k].', '##kd:@d', '##koi(chi', '##kyov', '##kyovy', '##kyovyc', '##kyovych', '##l', '##l-q', '##l-yong', '##layovi', '##lingy].', '##llsk/c', '##llx-mo', '##llx-mor', '##llx-morr', '##llx-morri', '##llx-morris', '##llyow', '##lna-jjar,', '##loqx', '##lsk/c', '##ltss/f', '##lunngid:', '##lx-mo', '##ly/u', '##lyov,', '##lyow', '##m', '##me-b', '##mpup-to,', '##ms(c)', '##m\u2510', '##m\u2510s', '##n', '##na-jjar,', '##nckyo', '##nckyo,', '##nd|', '##ne-bas', '##ngid:', '##ngqing', '##ngy].', '##nngid:', '##nnsdd:', '##nn\u00e9', '##nsdd:', '##ntys(mo', '##nu(f),', '##nukyovych', '##nup-t', '##n\u00e9', '##n\u2019t', '##n\u2510.', '##o', '##oaqu\u00ed', '##oex-vi', '##of--b', '##oi(chi', '##oly/u', '##one-bas', '##ongqing', '##ontys(mo', '##onup-t', '##on\u2019t', '##on\u2510.', '##op/r', '##op|', '##oqx', '##orrkd:@d', '##ost-quak', '##p', '##p(f),', '##p/', '##p/r', '##plingy].', '##pplingy].', '##ps(c)', '##pup-to,', '##pyov', '##p|', '##q', '##qb', '##qu', '##qu\u00ed', '##qx', '##r', '##r-yomosom', '##r-yon-y', '##raq', '##rdaay].', '##rgqv', '##rgqvi', '##rgqvis', '##rgqvist', '##rkd:@d', '##rl-yong', '##ronup-t', '##rrkd:@d', '##rtannsdd:', '##rup-cl', '##rup-cla', '##rup-clas', '##rup-class', '##rx-tb', '##rzagulyov,', '##s', '##s(c)', '##s(m', '##s/f', '##sdd:', '##shfaq', '##siaagulyov,', '##siion\u2510.', '##sk/c', '##slayovi', '##soex-vi', '##soff-p', '##soff-pi', '##soff-pin', '##soff-pinn', '##ss(c)', '##ss/f', '##ssiion\u2510.', '##st-quak', '##suupyov', '##t', '##t(whou', '##t(whout', '##t-quak', '##tannsdd:', '##tione-bas', '##tss/f', '##tuff-f', '##turdaay].', '##tys(m', '##tys(mo', '##u', '##u(f),', '##u(wb)', '##ubj', '##ucatione-bas', '##udhyoy', '##udhyoyo', '##udhyoyon', '##udhyoyono', '##udhyoyono,', '##uff-f', '##ugslayovi', '##ugslayovia', '##ugslayovia.', '##uk/u', '##ukyovych', '##ultss/f', '##ultss/fi', '##ultss/fix', '##ultss/fixt', '##ultss/fixtu', '##ultss/fixtur', '##ulyov,', '##unngid:', '##upplingy].', '##upyov', '##urdaay].', '##uupyov', '##u\u00ed', '##v', '##v/', '##vyo', '##vyon', '##vyonn', '##w', '##w(f)', '##w(f),', '##whi(ch', '##worrkd:@d', '##wworrkd:@d', '##wworrkd:@do', '##x', '##x).', '##x-', '##x-f', '##x-fr', '##x-h', '##x-m', '##x-mo', '##x-w', '##x-y', '##xup', '##y', '##y/', '##y/u', '##y].', '##ys(m', '##z', '##zagulyov,', '##zz', '##zzy', '##|', '##|.', '##\u00a9', '##\u00bf', '##\u00bfh', '##\u00bfha', '##\u00bfhav', '##\u00e4', '##\u00e9', '##\u00ed', '##\u00f3', '##\u0146', '##\u2019', '##\u2019d', '##\u2019s', '##\u2019t', '##\u201d', '##\u2026', '##\u2510', '##\u2510.', '##\u2510s', '#b', '&', '(', '(aff)', '(c', '(c)', '(cck', '(ch', '(chn', '(chno', '(chnol', '(chnolo', '(chnolog', '(chnology', '(chnology,', '(cp', '(f', '(f)', '(f),', '(j', '(jv', '(jvs', '(jvs)', '(uk/u', '(uk/us', '(uk/usa', '(uk/usa)', '(w', '(wb', '(wb)', '(wh', '(who', '(who)', '(whou', ')', ',', '-', ':', '<UNK>', '>', '>>', '@', '@d', '[', '[back].', '[w', '`', '`l', '`lo', '`lon', '`long', 'a', 'a&', 'a*', 'a*s', 'adngid:', 'al-q', 'al-qa', 'alna-jjar,', 'asdd:', 'ashfaq', 'b', 'bb&', 'bb&t', 'b\u00e4', 'b\u00e4c', 'b\u00e4ck', 'b\u00e4ckh', 'c', 'caf\u00e9', 'chongqing', 'd', 'd&', 'd&d', 'didn\u2019t', 'don\u2019t', 'd\u00ed', 'd\u00edt', 'd\u00edtr', 'e', 'e-b', 'e-ba', 'e-bas', 'e-bo', 'e-boo', 'e-book', 'e-bor', 'e-bord', 'eff', 'equ', 'ev', 'ex', 'ex-', 'ex-b', 'ex-br', 'ex-bri', 'ex-brit', 'ex-briti', 'ex-britis', 'ex-british', 'ex-c', 'ex-ch', 'ex-cha', 'ex-chan', 'ex-chanc', 'ex-ci', 'ex-cia', 'ex-g', 'ex-gu', 'ex-v', 'ex-vi', 'ex-vic', 'exp', 'f', 'fizzy', 'f\u00e9', 'f\u00e9i', 'f\u00e9in', 'g', 'h', 'i', 'iclunngid:', 'iqb', 'iqba', 'iqbal', 'iraq', 'iraq.', 'ishfaq', 'i\u2019', 'i\u2019m', 'j', 'joaqu\u00ed', 'joaqu\u00edn', 'joaqu\u00edn,', 'ju', 'k', 'l', 'm', 'mix).', 'montys(mo', 'montys(mon', 'montys(mont', 'montys(montg', 'montys(montgo', 'montys(montgom', 'mssiion\u2510.', 'm\u00f3', 'm\u00f3r', 'n', 'ni\u0146', 'ni\u0146a', 'o', 'obj', 'of', 'off', 'off-', 'off-c', 'off-p', 'off-pu', 'off-put', 'off-putt', 'off-putti', 'off-puttin', 'off-putting', 'off-putting,', 'p', 'poly/u', 'poly/un', 'poly/uni', 'poly/univ', 'pop/r', 'pop/ro', 'pop/roc', 'pop/rock', 'post-quak', 'q', 'q:', 'qu', 'r', 'r&', 'r&b', 's', 'said:', 'sddaam\u2510s', 'sillsk/c', 'sillsk/co', 'sillsk/con', 'sillsk/conn', 'six-h', 'six-ho', 'six-hou', 'six-hour', 'six-mo', 'six-mon', 'six-mont', 'six-month', 'six-w', 'six-y', 'stuff-f', 'stuff-fi', 'stuff-fil', 'stuff-fill', 'sturdaay].', 'subj', 'supplingy].', 't', 'tax-fr', 'th', 'tizzy', 'to', 'top|', 'u', 'uk', 'uk)', 'uk-b', 'uk-ba', 'uk-bas', 'uk\u2019', 'uk\u2019s', 'up', 'up-', 'up-c', 'up-cl', 'up-co', 'up-com', 'up-comi', 'up-comin', 'up-coming', 'up-t', 'up-to', 'up-to,', 'up-to-', 'up-to-d', 'up-to-da', 'up-to-dat', 'uq', 'uqr', 'uqrd', 'v', 'w', 'waq', 'waqt', 'wh', 'x', 'x-', 'x-r', 'x-ra', 'x-rat', 'x-ray', 'x-t', 'x-tb', 'xz', 'xzc', 'xzca', 'y', 'yo', 'yob', 'yobb', 'yobbi', 'yobbis', 'yobbish', 'yobo', 'yok', 'yoko', 'yokoh', 'yokoha', 'yokoham', 'yokohama', 'yokohama,', 'yor', 'york', 'york,', 'york.', 'you', 'yov', 'yov,', 'yovi', 'yow', 'z', '|', '|^^', '|\u00a9', '|\u00a9m', '|\u00a9mm', '|\u00a9mmv', '|\u00a9mmvi', '|\u00a9mmvii', '|\u00a9mmviii', '\u00a9', '\u00f3', '\u201c', '\u201cw', '\u201cwa', '\u201cwan', '\u201cwant', '\u201d', '\u2022', '\u2510', '\u2510a', '\u2510ac', '\u2510ach', '\u2510achi']\n"
          ]
        }
      ],
      "source": [
        "### DO NOT EDIT ###\n",
        "\n",
        "if __name__=='__main__':\n",
        "    tokenizer = WordPieceTokenizer(dataset.train, 700, do_tqdm = True, do_print=False)\n",
        "    tokenizer.train()\n",
        "    print(\"Vocab:\", tokenizer.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCR5Iq6oo6wd"
      },
      "source": [
        "Now we can look at some tokenizations on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csdUptctgyVu",
        "outputId": "7a8dd883-3315-487a-8be1-2766fdf5392d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: five killed in new mumbai building collapse at least five people have been killed in a building collapse in the indian city of mumbai - the second such incident in as many days.\n",
            "Tokenization: ['f', '##i', '##v', '##e', ' ', 'k', '##i', '##l', '##l', '##e', '##d', ' ', 'i', '##n', ' ', 'n', '##e', '##w', ' ', 'm', '##u', '##m', '##b', '##a', '##i', ' ', 'b', '##u', '##i', '##l', '##d', '##i', '##n', '##g', ' ', 'c', '##o', '##l', '##l', '##a', '##p', '##s', '##e', ' ', 'a', '##t', ' ', 'l', '##e', '##a', '##s', '##t', ' ', 'f', '##i', '##v', '##e', ' ', 'p', '##e', '##o', '##p', '##l', '##e', ' ', 'h', '##a', '##v', '##e', ' ', 'b', '##e', '##e', '##n', ' ', 'k', '##i', '##l', '##l', '##e', '##d', ' ', 'i', '##n', ' ', 'a', ' ', 'b', '##u', '##i', '##l', '##d', '##i', '##n', '##g', ' ', 'c', '##o', '##l', '##l', '##a', '##p', '##s', '##e', ' ', 'i', '##n', ' ', 'th', '##e', ' ', 'i', '##n', '##d', '##i', '##a', '##n', ' ', 'c', '##i', '##t', '##y', ' ', 'of', ' ', 'm', '##u', '##m', '##b', '##a', '##i', ' ', '-', ' ', 'th', '##e', ' ', 's', '##e', '##c', '##o', '##n', '##d', ' ', 's', '##u', '##c', '##h', ' ', 'i', '##n', '##c', '##i', '##d', '##e', '##n', '##t', ' ', 'i', '##n', ' ', 'a', '##s', ' ', 'm', '##a', '##n', '##y', ' ', 'd', '##a', '##y', '##s', '##.']\n",
            "Detokenization: five killed in new mumbai building collapse at least five people have been killed in a building collapse in the indian city of mumbai - the second such incident in as many days.\n",
            "\n",
            "Sentence: thank you and all best wishes.\n",
            "Tokenization: ['th', '##a', '##n', '##k', ' ', 'you', ' ', 'a', '##n', '##d', ' ', 'a', '##l', '##l', ' ', 'b', '##e', '##s', '##t', ' ', 'w', '##i', '##s', '##h', '##e', '##s', '##.']\n",
            "Detokenization: thank you and all best wishes.\n",
            "\n",
            "Sentence: indeed, ive only just married my british-born pakistani girlfriend (im white), and have found her family and culture to be very open and welcoming.\n",
            "Tokenization: ['i', '##n', '##d', '##e', '##e', '##d', '##,', ' ', 'i', '##v', '##e', ' ', 'o', '##n', '##l', '##y', ' ', 'ju', '##s', '##t', ' ', 'm', '##a', '##r', '##r', '##i', '##e', '##d', ' ', 'm', '##y', ' ', 'b', '##r', '##i', '##t', '##i', '##s', '##h', '##-b', '##o', '##r', '##n', ' ', 'p', '##a', '##k', '##i', '##s', '##t', '##a', '##n', '##i', ' ', 'g', '##i', '##r', '##l', '##f', '##r', '##i', '##e', '##n', '##d', ' ', '(', '##i', '##m', ' ', 'wh', '##i', '##t', '##e', '##),', ' ', 'a', '##n', '##d', ' ', 'h', '##a', '##v', '##e', ' ', 'f', '##o', '##u', '##n', '##d', ' ', 'h', '##e', '##r', ' ', 'f', '##a', '##m', '##i', '##l', '##y', ' ', 'a', '##n', '##d', ' ', 'c', '##u', '##l', '##t', '##u', '##r', '##e', ' ', 'to', ' ', 'b', '##e', ' ', 'v', '##e', '##r', '##y', ' ', 'o', '##p', '##e', '##n', ' ', 'a', '##n', '##d', ' ', 'w', '##e', '##l', '##c', '##o', '##m', '##i', '##n', '##g', '##.']\n",
            "Detokenization: indeed, ive only just married my british-born pakistani girlfriend (im white), and have found her family and culture to be very open and welcoming.\n",
            "\n",
            "Sentence: unfortunately it is extremely hard to hear such news although there are thousands of such cases in pakistan.\n",
            "Tokenization: ['u', '##n', '##f', '##o', '##r', '##t', '##u', '##n', '##a', '##t', '##e', '##l', '##y', ' ', 'i', '##t', ' ', 'i', '##s', ' ', 'ex', '##t', '##r', '##e', '##m', '##e', '##l', '##y', ' ', 'h', '##a', '##r', '##d', ' ', 'to', ' ', 'h', '##e', '##a', '##r', ' ', 's', '##u', '##c', '##h', ' ', 'n', '##e', '##w', '##s', ' ', 'a', '##l', '##t', '##h', '##o', '##u', '##g', '##h', ' ', 'th', '##e', '##r', '##e', ' ', 'a', '##r', '##e', ' ', 'th', '##o', '##u', '##s', '##a', '##n', '##d', '##s', ' ', 'of', ' ', 's', '##u', '##c', '##h', ' ', 'c', '##a', '##s', '##e', '##s', ' ', 'i', '##n', ' ', 'p', '##a', '##k', '##i', '##s', '##t', '##a', '##n', '##.']\n",
            "Detokenization: unfortunately it is extremely hard to hear such news although there are thousands of such cases in pakistan.\n",
            "\n",
            "Sentence: the buy-to-let market is pressurising the housing market and new development does not appear to be the best of options in this crowded country with poor road infrastructure and public transport.\n",
            "Tokenization: ['th', '##e', ' ', 'b', '##u', '##y', '##-', '##t', '##o', '##-', '##l', '##e', '##t', ' ', 'm', '##a', '##r', '##k', '##e', '##t', ' ', 'i', '##s', ' ', 'p', '##r', '##e', '##s', '##s', '##u', '##r', '##i', '##s', '##i', '##n', '##g', ' ', 'th', '##e', ' ', 'h', '##o', '##u', '##s', '##i', '##n', '##g', ' ', 'm', '##a', '##r', '##k', '##e', '##t', ' ', 'a', '##n', '##d', ' ', 'n', '##e', '##w', ' ', 'd', '##e', '##v', '##e', '##l', '##o', '##p', '##m', '##e', '##n', '##t', ' ', 'd', '##o', '##e', '##s', ' ', 'n', '##o', '##t', ' ', 'a', '##p', '##p', '##e', '##a', '##r', ' ', 'to', ' ', 'b', '##e', ' ', 'th', '##e', ' ', 'b', '##e', '##s', '##t', ' ', 'of', ' ', 'o', '##p', '##t', '##i', '##o', '##n', '##s', ' ', 'i', '##n', ' ', 'th', '##i', '##s', ' ', 'c', '##r', '##o', '##w', '##d', '##e', '##d', ' ', 'c', '##o', '##u', '##n', '##t', '##r', '##y', ' ', 'w', '##i', '##t', '##h', ' ', 'p', '##o', '##o', '##r', ' ', 'r', '##o', '##a', '##d', ' ', 'i', '##n', '##f', '##r', '##a', '##s', '##t', '##r', '##u', '##c', '##t', '##u', '##r', '##e', ' ', 'a', '##n', '##d', ' ', 'p', '##u', '##b', '##l', '##i', '##c', ' ', 't', '##r', '##a', '##n', '##s', '##p', '##o', '##r', '##t', '##.']\n",
            "Detokenization: the buy-to-let market is pressurising the housing market and new development does not appear to be the best of options in this crowded country with poor road infrastructure and public transport.\n",
            "\n",
            "Sentence: the sbvt ad is yet another example of what the bush camp has been doing to denigrate veterans who oppose george bush - like senator mccain for instance.\n",
            "Tokenization: ['th', '##e', ' ', 's', '##b', '##v', '##t', ' ', 'a', '##d', ' ', 'i', '##s', ' ', 'y', '##e', '##t', ' ', 'a', '##n', '##o', '##t', '##h', '##e', '##r', ' ', 'ex', '##a', '##m', '##p', '##l', '##e', ' ', 'of', ' ', 'wh', '##a', '##t', ' ', 'th', '##e', ' ', 'b', '##u', '##s', '##h', ' ', 'c', '##a', '##m', '##p', ' ', 'h', '##a', '##s', ' ', 'b', '##e', '##e', '##n', ' ', 'd', '##o', '##i', '##n', '##g', ' ', 'to', ' ', 'd', '##e', '##n', '##i', '##g', '##r', '##a', '##t', '##e', ' ', 'v', '##e', '##t', '##e', '##r', '##a', '##n', '##s', ' ', 'wh', '##o', ' ', 'o', '##p', '##p', '##o', '##s', '##e', ' ', 'g', '##e', '##o', '##r', '##g', '##e', ' ', 'b', '##u', '##s', '##h', ' ', '-', ' ', 'l', '##i', '##k', '##e', ' ', 's', '##e', '##n', '##a', '##t', '##o', '##r', ' ', 'm', '##c', '##c', '##a', '##i', '##n', ' ', 'f', '##o', '##r', ' ', 'i', '##n', '##s', '##t', '##a', '##n', '##c', '##e', '##.']\n",
            "Detokenization: the sbvt ad is yet another example of what the bush camp has been doing to denigrate veterans who oppose george bush - like senator mccain for instance.\n",
            "\n",
            "Sentence: those are all people that have money, mr schwarzenegger said when asked about the strike.\n",
            "Tokenization: ['th', '##o', '##s', '##e', ' ', 'a', '##r', '##e', ' ', 'a', '##l', '##l', ' ', 'p', '##e', '##o', '##p', '##l', '##e', ' ', 'th', '##a', '##t', ' ', 'h', '##a', '##v', '##e', ' ', 'm', '##o', '##n', '##e', '##y', '##,', ' ', 'm', '##r', ' ', 's', '##c', '##h', '##w', '##a', '##r', '##z', '##e', '##n', '##e', '##g', '##g', '##e', '##r', ' ', 's', '##a', '##i', '##d', ' ', 'wh', '##e', '##n', ' ', 'a', '##s', '##k', '##e', '##d', ' ', 'a', '##b', '##o', '##u', '##t', ' ', 'th', '##e', ' ', 's', '##t', '##r', '##i', '##k', '##e', '##.']\n",
            "Detokenization: those are all people that have money, mr schwarzenegger said when asked about the strike.\n",
            "\n",
            "Sentence: lead research professor allen wilcox said: there apparently are biological factors promoting intercourse during a womans six fertile days, whether she wants a baby or not.\n",
            "Tokenization: ['l', '##e', '##a', '##d', ' ', 'r', '##e', '##s', '##e', '##a', '##r', '##c', '##h', ' ', 'p', '##r', '##o', '##f', '##e', '##s', '##s', '##o', '##r', ' ', 'a', '##l', '##l', '##e', '##n', ' ', 'w', '##i', '##l', '##c', '##o', '##x', ' ', 'said:', ' ', 'th', '##e', '##r', '##e', ' ', 'a', '##p', '##p', '##a', '##r', '##e', '##n', '##t', '##l', '##y', ' ', 'a', '##r', '##e', ' ', 'b', '##i', '##o', '##l', '##o', '##g', '##i', '##c', '##a', '##l', ' ', 'f', '##a', '##c', '##t', '##o', '##r', '##s', ' ', 'p', '##r', '##o', '##m', '##o', '##t', '##i', '##n', '##g', ' ', 'i', '##n', '##t', '##e', '##r', '##c', '##o', '##u', '##r', '##s', '##e', ' ', 'd', '##u', '##r', '##i', '##n', '##g', ' ', 'a', ' ', 'w', '##o', '##m', '##a', '##n', '##s', ' ', 's', '##i', '##x', ' ', 'f', '##e', '##r', '##t', '##i', '##l', '##e', ' ', 'd', '##a', '##y', '##s', '##,', ' ', 'wh', '##e', '##t', '##h', '##e', '##r', ' ', 's', '##h', '##e', ' ', 'w', '##a', '##n', '##t', '##s', ' ', 'a', ' ', 'b', '##a', '##b', '##y', ' ', 'o', '##r', ' ', 'n', '##o', '##t', '##.']\n",
            "Detokenization: lead research professor allen wilcox said: there apparently are biological factors promoting intercourse during a womans six fertile days, whether she wants a baby or not.\n",
            "\n",
            "Sentence: put offshore skills to work on marine energy.\n",
            "Tokenization: ['p', '##u', '##t', ' ', 'of', '##f', '##s', '##h', '##o', '##r', '##e', ' ', 's', '##k', '##i', '##l', '##l', '##s', ' ', 'to', ' ', 'w', '##o', '##r', '##k', ' ', 'o', '##n', ' ', 'm', '##a', '##r', '##i', '##n', '##e', ' ', 'e', '##n', '##e', '##r', '##g', '##y', '##.']\n",
            "Detokenization: put offshore skills to work on marine energy.\n",
            "\n",
            "Sentence: jan peter balkenende (left) has been out trying to persuade voters the trouble is, said the woman standing next to him, were trying to persuade people who dont want to be persuaded.\n",
            "Tokenization: ['j', '##a', '##n', ' ', 'p', '##e', '##t', '##e', '##r', ' ', 'b', '##a', '##l', '##k', '##e', '##n', '##e', '##n', '##d', '##e', ' ', '(', '##l', '##e', '##f', '##t', '##)', ' ', 'h', '##a', '##s', ' ', 'b', '##e', '##e', '##n', ' ', 'o', '##u', '##t', ' ', 't', '##r', '##y', '##i', '##n', '##g', ' ', 'to', ' ', 'p', '##e', '##r', '##s', '##u', '##a', '##d', '##e', ' ', 'v', '##o', '##t', '##e', '##r', '##s', ' ', 'th', '##e', ' ', 't', '##r', '##o', '##u', '##b', '##l', '##e', ' ', 'i', '##s', '##,', ' ', 's', '##a', '##i', '##d', ' ', 'th', '##e', ' ', 'w', '##o', '##m', '##a', '##n', ' ', 's', '##t', '##a', '##n', '##d', '##i', '##n', '##g', ' ', 'n', '##e', '##x', '##t', ' ', 'to', ' ', 'h', '##i', '##m', '##,', ' ', 'w', '##e', '##r', '##e', ' ', 't', '##r', '##y', '##i', '##n', '##g', ' ', 'to', ' ', 'p', '##e', '##r', '##s', '##u', '##a', '##d', '##e', ' ', 'p', '##e', '##o', '##p', '##l', '##e', ' ', 'wh', '##o', ' ', 'd', '##o', '##n', '##t', ' ', 'w', '##a', '##n', '##t', ' ', 'to', ' ', 'b', '##e', ' ', 'p', '##e', '##r', '##s', '##u', '##a', '##d', '##e', '##d', '##.']\n",
            "Detokenization: jan peter balkenende (left) has been out trying to persuade voters the trouble is, said the woman standing next to him, were trying to persuade people who dont want to be persuaded.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "### DO NOT EDIT ###\n",
        "\n",
        "if __name__=='__main__':\n",
        "    for sent in random.sample(dataset.test, 10):\n",
        "        print('Sentence:', sent)\n",
        "        toks = tokenizer.tokenize(sent)\n",
        "        print('Tokenization:', toks)\n",
        "        print('Detokenization:', tokenizer.detokenize(toks))\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKt-WVr6OtP4"
      },
      "source": [
        "# Part 2: Language Models [72 points]\n",
        "\n",
        "Here, you will train some <b>n-gram language models</b> on WikiText-2, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following paper: https://arxiv.org/pdf/1609.07843v1.pdf. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN2Ja8MNP4qS"
      },
      "source": [
        "## Download & preprocess the data\n",
        "\n",
        "To make your models more robust, it is necessary to perform some basic preprocessing on the corpora. <i>You do not need to edit this code.</i>\n",
        "\n",
        "* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp;In this homework, we are interested in modeling individual sentences, rather than longer chunks of text such as paragraphs or documents. The WikiTest dataset provides paragraphs; thus, we provide a simple method to identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n",
        "\n",
        "* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). These markers will allow your models to generate sentences that have realistic beginnings and endings.\n",
        "\n",
        "* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown words in the test corpora, all words that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`) before estimating your models. The WikiText dataset has already done this, and you can read about the method in the paper above. When unknown words are encountered in the test corpus, they should be treated as that special token instead.\n",
        "\n",
        "We provide you with preprocessing code here, and you should not modify it.\n",
        "\n",
        "After the preprocessing, you may assume that all words in the test set appear in the training set, as this code has already replaced the unseen tokens with `<UNK>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCkrUjKEBrNp"
      },
      "outputs": [],
      "source": [
        "### DO NOT EDIT ###\n",
        "\n",
        "# Constants (feel free to use these in your code, but do not change them)\n",
        "START = \"<s>\"   # Start-of-sentence token\n",
        "END = \"</s>\"    # End-of-sentence-token\n",
        "UNK = \"<UNK>\"   # Unknown word token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUdZstjH30DL"
      },
      "outputs": [],
      "source": [
        "### DO NOT EDIT ###\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "def preprocess(data, vocab=None):\n",
        "    final_data = []\n",
        "    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "    for paragraph in data:\n",
        "        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()]\n",
        "        if vocab is not None:\n",
        "            paragraph = [x if x in vocab else UNK for x in paragraph]\n",
        "        if paragraph == [] or paragraph.count('=') >= 2: continue\n",
        "        sen = []\n",
        "        prev_punct, prev_quot = False, False\n",
        "        for word in paragraph:\n",
        "            if prev_quot:\n",
        "                if word[0] not in lowercase:\n",
        "                    final_data.append(sen)\n",
        "                    sen = []\n",
        "                    prev_punct, prev_quot = False, False\n",
        "            if prev_punct:\n",
        "                if word == '\"':\n",
        "                    prev_punct, prev_quot = False, True\n",
        "                else:\n",
        "                    if word[0] not in lowercase:\n",
        "                        final_data.append(sen)\n",
        "                        sen = []\n",
        "                        prev_punct, prev_quot = False, False\n",
        "            if word in {'.', '?', '!'}: prev_punct = True\n",
        "            sen += [word]\n",
        "        if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n",
        "        final_data.append(sen)\n",
        "    vocab_was_none = vocab is None\n",
        "    if vocab is None:\n",
        "        vocab = set()\n",
        "    for i in range(len(final_data)):\n",
        "        final_data[i] = [START] + final_data[i] + [END]\n",
        "        if vocab_was_none:\n",
        "            for word in final_data[i]:\n",
        "                vocab.add(word)\n",
        "    return final_data, vocab\n",
        "\n",
        "def getDataset():\n",
        "    splits = ['train', 'valid']\n",
        "    datasets = []\n",
        "    path = './{}.txt'\n",
        "    url = 'https://raw.githubusercontent.com/pytorch/examples/main/word_language_model/data/wikitext-2/{}.txt'\n",
        "    for split in splits:\n",
        "        if os.path.exists(path.format(split)):\n",
        "            print(f\"{split} dataset already downloaded\")\n",
        "        else:\n",
        "            filename = f'{split}.txt'\n",
        "            urlretrieve(url.format(split), filename)\n",
        "        datasets.append(open(f'{split}.txt').read().split('\\n'))\n",
        "    train_dataset, vocab = preprocess(datasets[0])\n",
        "    test_dataset, _ = preprocess(datasets[1], vocab)\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_dataset, test_dataset = getDataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSFJ07ELGUMh"
      },
      "source": [
        "Run the next cell to see 10 random sentences of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swPwiHBHDDkT",
        "outputId": "6eb2f277-b47e-40e9-9686-304f9e7ca0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'Philadelphia', ':', 'University', 'of', 'Pennsylvania', 'Press', '.', '</s>']\n",
            "['<s>', 'It', 'is', 'written', 'as', '<UNK>', '(', '<UNK>', ')', 'in', 'Biblical', 'Aramaic', 'and', '<UNK>', '(', '<UNK>', ')', 'in', 'Syriac', 'as', 'used', 'by', 'the', 'Assyrian', 'Church', ',', 'both', 'meaning', 'simply', '\"', 'God', '\"', '.', '</s>']\n",
            "['<s>', 'The', 'medley', 'starts', 'off', 'with', '\"', 'Vintage', 'Clothes', '\"', ',', 'which', 'McCartney', '\"', 'sat', 'down', 'one', 'day', '\"', 'to', 'write', ',', 'that', 'was', '\"', 'looking', 'back', ',', '[', 'and', ']', 'looking', 'back', '.', '\"', '</s>']\n",
            "['<s>', 'Robin', '<UNK>', ',', 'a', 'professor', 'of', 'strategic', 'management', ',', 'believes', 'that', 'Isabella', \"'s\", 'advice', 'and', 'guidance', 'on', 'household', 'management', 'can', 'also', 'be', 'applied', 'to', 'business', 'management', ',', 'and', 'her', 'lessons', 'on', 'the', 'subject', 'have', 'stood', 'the', 'test', 'of', 'time', 'better', 'than', 'some', 'of', 'her', 'advice', 'on', 'cooking', 'or', '<UNK>', '.', '</s>']\n",
            "['<s>', 'It', 'was', 'rare', 'for', 'contemporary', 'portraits', 'to', 'show', 'women', 'reading', ',', 'and', 'if', 'the', 'model', 'herself', 'could', 'read', 'then', 'she', 'was', 'likely', 'from', 'a', 'noble', 'family', '.', '</s>']\n",
            "['<s>', 'Across', 'southern', 'China', ',', 'the', 'typhoon', 'damaged', 'over', '10', 'million', 'hectares', '(', '25', 'million', 'acres', ')', 'of', 'crop', 'fields', '.', '</s>']\n",
            "['<s>', 'There', 'are', '13', '\u2013', '18', '<UNK>', 'and', '16', '\u2013', '22', '<UNK>', '.', '</s>']\n",
            "['<s>', 'Bishop', 'Fulton', 'J.', 'Sheen', \"'s\", 'devotional', 'program', 'Life', 'Is', 'Worth', 'Living', 'went', 'up', 'against', 'Milton', 'Berle', 'in', 'many', 'cities', ',', 'and', 'was', 'the', 'first', 'show', 'to', 'compete', 'successfully', 'in', 'the', 'ratings', 'against', '\"', 'Mr.', 'Television', '\"', '.', '</s>']\n",
            "['<s>', 'In', 'the', 'months', 'before', 'Domoina', 'struck', ',', 'dry', 'conditions', 'persisted', 'across', 'southeastern', 'Africa', '.', '</s>']\n",
            "['<s>', 'After', 'the', 'Fifth', 'Crusade', 'ended', '\"', 'in', 'colossal', 'and', '<UNK>', 'failure', '\"', ',', 'John', 'returned', 'to', 'his', 'kingdom', '.', '</s>']\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    for x in random.sample(train_dataset, 10):\n",
        "        print (x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM6hNHMqTMt2"
      },
      "source": [
        "## The LanguageModel Class\n",
        "\n",
        "You will implement 4 types of language models: a <b>unigram</b> model, a <b>smoothed unigram</b> model, a <b>bigram</b> model, and a <b>smoothed bigram</b> model. Each of the models is worth 25 points and extends the following base class. <b>You do not need to implement anything in this class</b>; you will instead implement each of the following methods in the relevant subclass:\n",
        "\n",
        "* <b>`__init__(self, trainCorpus)`</b>: Train the language model on `trainCorpus`. This will involve calculating relative frequency estimates according to the type of model you're implementing.\n",
        "\n",
        "* <b>`generateSentence(self)`</b>: <b>[4 points]</b> Return a sentence that is generated by the language model. It should be a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>, where each <TT>w<sup>(i)</sup></TT> is a word in your vocabulary (including <TT>&lt;UNK&gt;</TT> but exlcuding <TT>&lt;s&gt;</TT> and <TT>&lt;&sol;s&gt;</TT>). You may assume that <TT>&lt;s&gt;</TT> starts each sentence (with probability $1$). The following words <TT>w<sup>(1)</sup></TT>, ... , <TT>w<sup>(n)</sup></TT>, <TT>&lt;&sol;s&gt;</TT> are generated according to your language model's distribution. Note that the number of words <TT>n</TT> is not fixed; instead, you should stop the sentence as soon as you generate the stop token <TT>&lt;&sol;s&gt;</TT>.\n",
        "\n",
        "* <b>`getSentenceLogProbability(self, sentence)`</b>: <b>[7 points]</b> Return the <em> logarithm of the probability</em> of <TT>sentence</TT>, which is again a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>. You should use the natural logarithm $-$ that is, the base-<em>e</em> logarithm. See the note below about performing your calculations in log space.\n",
        "\n",
        "* <b>`getCorpusPerplexity(self, testCorpus)`</b>: <b>[7 points]</b> You need to compute the perplexity (normalized inverse log probability) of `testCorpus` according to your model. For a corpus $W$ with $N$ words and a bigram model, Jurafsky and Martin tells you to compute perplexity as follows:\n",
        "\n",
        "$$Perplexity(W) = \\Big [ \\prod_{i=1}^N \\frac{1}{P(w^{(i)}|w^{(i-1)})} \\Big ]^{1/N}$$\n",
        "\n",
        "<b>Implementation Hint:</b> In order to avoid underflow, you will likely need to do all of your calculations in log-space. That is, instead of multiplying probabilities, you should add the logarithms of the probabilities and exponentiate the result:\n",
        "\n",
        "$$\\prod_{i=1}^N P(w^{(i)}|w^{(i-1)}) = \\exp\\Big (\\sum_{i=1}^N \\log P(w^{(i)}|w^{(i-1)}) \\Big ) $$\n",
        "\n",
        "Using this property should help you in your implementation of `generateSentence(self)` and `getCorpusPerplexity(self, testCorpus)`.\n",
        "\n",
        "Feel free to implement helper methods as you wish (either in the base class or in the subclases). <b>But be sure not to change the function signatures of the provided methods</b> (i.e. the function and argument names), or else the autograder will fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "uKO6dHNSS45P"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class LanguageModel(object):\n",
        "    def naturalLog(self, prob):\n",
        "      if not prob:\n",
        "        return []\n",
        "\n",
        "      log_data = []\n",
        "      for x in prob:\n",
        "        if x <= 0:\n",
        "          return [] # Return empty list if non-positive number is found\n",
        "        log_data.append(math.log(x))\n",
        "      return log_data\n",
        "\n",
        "    def unigram_count(self, d, word):\n",
        "      if word not in d.keys():\n",
        "          d[word] = 1\n",
        "      else:\n",
        "          d[word] += 1\n",
        "\n",
        "\n",
        "    def bigram_count(self, d, word1, word2):\n",
        "      if word1 not in d.keys():\n",
        "        d[word1] = {word2: 1}\n",
        "      else:\n",
        "        if word2 in d[word1].keys():\n",
        "          d[word1][word2] += 1\n",
        "        else:\n",
        "          d[word1][word2] = 1\n",
        "\n",
        "\n",
        "    def __init__(self, trainCorpus):\n",
        "        '''\n",
        "        Initialize and train the model (i.e. estimate the model's underlying probability\n",
        "        distribution from the training corpus.)\n",
        "        '''\n",
        "\n",
        "        return\n",
        "\n",
        "    def generateSentence(self):\n",
        "        '''\n",
        "        Generate a sentence by drawing words according to the model's probability distribution.\n",
        "        Note: Think about how to set the length of the sentence in a principled way.\n",
        "        '''\n",
        "\n",
        "        raise NotImplementedError(\"Implement generateSentence in each subclass.\")\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        '''\n",
        "        Calculate the log probability of the sentence provided.\n",
        "        '''\n",
        "\n",
        "        raise NotImplementedError(\"Implement getSentenceProbability in each subclass.\")\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        '''\n",
        "        Calculate the perplexity of the corpus provided.\n",
        "        '''\n",
        "\n",
        "        raise NotImplementedError(\"Implement getCorpusPerplexity in each subclass.\")\n",
        "\n",
        "    def printSentences(self, n):\n",
        "        '''\n",
        "        Prints n sentences generated by your model.\n",
        "        '''\n",
        "\n",
        "        ### DO NOT EDIT ###\n",
        "        for i in range(n):\n",
        "            sent = self.generateSentence()\n",
        "            prob = self.getSentenceLogProbability(sent)\n",
        "            print('Log Probability:', prob , '\\tSentence:',sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf15l6f3etMV"
      },
      "source": [
        "## <font color='red'>TODO:</font> Unigram Model [18 points]\n",
        "\n",
        "Here, you will implement each of the 4 functions described above for an <b>unsmoothed unigram</b> model. The probability distribution of a word is given by $\\hat P(w)$.\n",
        "\n",
        "<font color='green'><b>Hints:</b></font>\n",
        "* <font color='green'>You should use a <b>dictionary</b> to map tokens to their unigram counts.</font>\n",
        "* <font color='green'>Since you never want to generate the start-of-sentence token `<s>`, you should <b>not</b> include it in your counts.</font>\n",
        "* <font color='green'>In general, avoid checking for membership in a list (i.e. avoid `x in lst`). Instead, use sets or dictionaries for this purpose $-$ membership checks are much faster on these data structures.</font>\n",
        "* <font color='green'>Do <b>not</b> modify the training or test corpora by using `.append(...)` or `.pop(...)` on them. This will cause unexpected behavior in the autograder tests, which do not expect you to be changing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2uZdMsqeuf2"
      },
      "outputs": [],
      "source": [
        "class UnigramModel(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "\n",
        "        ### TODO ###\n",
        "        self.train_corpus = trainCorpus\n",
        "        token_dict = self.token_dict()\n",
        "        self.p_w = self.token_prob(token_dict)\n",
        "\n",
        "\n",
        "    def token_dict(self):\n",
        "        t_d = {}\n",
        "\n",
        "        for sentence in self.train_corpus:\n",
        "            for word in sentence[1:]:\n",
        "                super().unigram_count(t_d, word)\n",
        "\n",
        "        return t_d\n",
        "\n",
        "    def token_prob(self, token_dict):\n",
        "\n",
        "        N = sum(token_dict.values())\n",
        "        p_w ={}\n",
        "        for key, value in token_dict.items():\n",
        "          p_w[key] = value/N\n",
        "\n",
        "        return p_w\n",
        "\n",
        "    def generateSentence(self):\n",
        "\n",
        "        sentence = []\n",
        "        sentence.append(START)\n",
        "\n",
        "        word = ''\n",
        "        while word != END:\n",
        "          word = random.choices(list(self.p_w.keys()), weights = list(self.p_w.values()), k=1)[0]\n",
        "          sentence.append(word)\n",
        "\n",
        "\n",
        "        return sentence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "\n",
        "        probarray =[]\n",
        "        for word in sentence[1:]:\n",
        "          #print(word)\n",
        "          probarray.append(self.p_w[word])\n",
        "\n",
        "        #print(probarray)\n",
        "        logarray = super().naturalLog(probarray)\n",
        "\n",
        "        ### TODO ###\n",
        "        return sum(logarray)\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "\n",
        "        ### TODO ###\n",
        "        N = 0\n",
        "        logprob = 0\n",
        "\n",
        "\n",
        "        for sentence in testCorpus:\n",
        "          #print(sentence)\n",
        "          N += (len(sentence)-1)\n",
        "          logprob += self.getSentenceLogProbability(sentence)\n",
        "\n",
        "        perplexity = math.exp(-1/N * logprob)\n",
        "        return perplexity\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c6zfDsT-GrU"
      },
      "source": [
        "We provide you with a testing function that uses very simple training & test corpora (you could compute probability/perplexity by hand if you wanted to). This is just a <b>sanity check</b> $-$ passing this test does not guarantee you a perfect score in the autograder; this is simply to help you debug your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8uRRgLi6IVt",
        "outputId": "eabe20af-8db3-458f-ac1d-fed5e5dbefd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -19.08542845 \tYour log prob.: -19.08542845 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -114.5001481799 \tYour log prob.: -114.5001481799 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -108.7963657053 \tYour log prob.: -108.7963657053 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -53.6727664115 \tYour log prob.: -53.6727664115 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -55.4645258807 \tYour log prob.: -55.4645258807 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 41.3308239726 \tYour train perp.: 41.3308239726 \t PASSED\n",
            "Correct test perp.: 38.0122981569 \tYour test perp.: 38.0122981569 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ],
      "source": [
        "def sanityCheck(model_type):\n",
        "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
        "\n",
        "    #\tRead in the test corpus\n",
        "    train_corpus = [\"By the Late Classic , a network of few <unk> ( few <unk> ) linked various parts of the city , running for several kilometres through its urban core .\",\n",
        "    \"Few people realize how difficult it was to create Sonic 's graphics engine , which allowed for the incredible rate of speed the game 's known for .\"]\n",
        "    test_corpus = [\"Classic few parts of the game allowed for few <unk> <unk> incredible city .\",\n",
        "                   \"Few <unk> realize the difficult network , which linked the game to Sonic .\"]\n",
        "    train_corpus, _ = preprocess(train_corpus)\n",
        "    test_corpus, _ = preprocess(test_corpus)\n",
        "    sentence = preprocess([\"Sonic was difficult .\"])[0][0]\n",
        "\n",
        "    # These are the correct answers (don't change them!)\n",
        "    if model_type == \"unigram\":\n",
        "       senprobs = [-19.08542845, -114.5001481799, -108.7963657053, -53.6727664115, -55.4645258807]\n",
        "       trainPerp, testPerp = 41.3308239726, 38.0122981569\n",
        "       model = UnigramModel(train_corpus)\n",
        "    elif model_type == \"smoothed-unigram\":\n",
        "       senprobs = [-19.0405293515, -115.3479413049, -108.9114348746, -54.8190029616, -55.8122547346]\n",
        "       trainPerp, testPerp = 41.9994393615, 39.9531928383\n",
        "       model = SmoothedUnigramModel(train_corpus)\n",
        "    elif model_type == \"bigram\":\n",
        "       senprobs = [-float('inf'), -10.3450917073, -9.2464794186, -float('inf'), -float('inf')]\n",
        "       trainPerp, testPerp = 1.3861445461, float('inf')\n",
        "       model = BigramModel(train_corpus)\n",
        "    elif model_type == \"smoothed-bigram\":\n",
        "       senprobs = [-16.355820202, -76.0026113319, -74.2346475108, -47.2885760372, -51.2730261907]\n",
        "       trainPerp, testPerp = 12.2307627397, 26.7193157699\n",
        "       model = SmoothedBigramModelAD(train_corpus)\n",
        "    else: assert False, 'Invalid model_type'\n",
        "\n",
        "    print(\"--- TEST: generateSentence() ---\")\n",
        "    modelSen = model.generateSentence()\n",
        "    senTestPassed = isinstance(modelSen, list) and len(modelSen) > 1 and isinstance(modelSen[0], str)\n",
        "    if senTestPassed:\n",
        "        print (\"Test generateSentence() passed!\")\n",
        "    else:\n",
        "        print (\"Test generateSentence() failed; did not return a list of strings...\")\n",
        "\n",
        "    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n",
        "    sentences = [sentence, *train_corpus, *test_corpus]\n",
        "    failed = 0\n",
        "    for i in range(len(sentences)):\n",
        "        sen, correct_prob = sentences[i], senprobs[i]\n",
        "        prob = round(model.getSentenceLogProbability(sen), 10)\n",
        "        print(\"Correct log prob.:\", correct_prob, '\\tYour log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n",
        "        if prob != correct_prob: failed+=1\n",
        "\n",
        "    if not failed:\n",
        "        print (\"Test getSentenceProbability(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n",
        "\n",
        "    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n",
        "    train_perp = round(model.getCorpusPerplexity(train_corpus), 10)\n",
        "    test_perp = round(model.getCorpusPerplexity(test_corpus), 10)\n",
        "\n",
        "    print(\"Correct train perp.:\", trainPerp, '\\tYour train perp.:', train_perp, '\\t', 'PASSED' if trainPerp == train_perp else 'FAILED')\n",
        "    print(\"Correct test perp.:\", testPerp, '\\tYour test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n",
        "    train_passed, test_passed = train_perp == trainPerp, test_perp == testPerp\n",
        "    if train_passed and test_passed:\n",
        "        print(\"Test getCorpusPerplexity(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getCorpusPerplexity(...) failed on\", \"the training corpus and the testing corpus...\" if not train_passed and not test_passed else \"the testing corpus...\" if not test_passed else \"the training corpus...\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    sanityCheck('unigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z8h9U63AkiG"
      },
      "source": [
        "Next, we provide you with another <b>sanity check</b> that trains your model on the *entire* training set, and tests your functions on a small corpus (10 sentences) of *real* test data.\n",
        "\n",
        "If your code is inefficient, you will likely see that this cell is taking too long. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mCa6zasN-0j",
        "outputId": "75df42cf-2580-4760-f9f8-db67714ccc76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -80.7782190984 \tYour log prob.: -80.7782190984 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -174.4769654449 \tYour log prob.: -174.4769654449 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -136.455148267 \tYour log prob.: -136.455148267 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -225.5890741503 \tYour log prob.: -225.5890741503 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -719.0142129846 \tYour log prob.: -719.0142129846 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -236.350443633 \tYour log prob.: -236.350443633 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -126.0056604204 \tYour log prob.: -126.0056604204 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -47.3424655612 \tYour log prob.: -47.3424655612 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -47.7775372096 \tYour log prob.: -47.7775372096 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -138.8159941929 \tYour log prob.: -138.8159941929 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: 881.0132848704 \tYour test perp.: 881.0132848704 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ],
      "source": [
        "def sanityCheckFullDataset(model_type):\n",
        "    model = UnigramModel(train_dataset)\n",
        "    idxes = list(range(75,7500, 800))\n",
        "    small_test_corpus = [test_dataset[idx] for idx in idxes]\n",
        "    if model_type == 'unigram':\n",
        "        senprobs = [-80.7782190984, -174.4769654449, -136.455148267, -225.5890741503, -719.0142129846, -236.350443633, -126.0056604204, -47.3424655612, -47.7775372096, -138.8159941929]\n",
        "        testPerp = 881.0132848704\n",
        "        model = UnigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-unigram':\n",
        "        senprobs = [-80.8423009715, -174.5131424172, -136.3181234818, -225.357454098, -719.1543898871, -236.6682968913, -126.1965419509, -47.4369338195, -47.7692144935, -138.542462715]\n",
        "        testPerp = 881.6105352831\n",
        "        model = SmoothedUnigramModel(train_dataset)\n",
        "    elif model_type == 'bigram':\n",
        "        senprobs = [-float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -32.1502020637, -float('inf'), -float('inf')]\n",
        "        testPerp = float ('inf')\n",
        "        model = BigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-bigram':\n",
        "        senprobs = [-61.3754065648, -141.9754903887, -107.0849366076, -168.4944718788, -619.9409055374, -195.8159911677, -86.3762008156, -32.4764801981, -48.124714509, -124.687107856]\n",
        "        testPerp = 261.4247123506\n",
        "        model = SmoothedBigramModelAD(train_dataset)\n",
        "    else: assert False, 'Invalid model_type'\n",
        "    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n",
        "    failed = 0\n",
        "    for i in range(len(small_test_corpus)):\n",
        "        sen, correct_prob = small_test_corpus[i], senprobs[i]\n",
        "        prob = round(model.getSentenceLogProbability(sen), 10)\n",
        "        print(\"Correct log prob.:\", correct_prob, '\\tYour log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n",
        "        if prob != correct_prob: failed+=1\n",
        "\n",
        "    if not failed:\n",
        "        print (\"Test getSentenceProbability(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n",
        "\n",
        "    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n",
        "    test_perp = round(model.getCorpusPerplexity(small_test_corpus), 10)\n",
        "\n",
        "    print(\"Correct test perp.:\", testPerp, '\\tYour test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n",
        "    test_passed = test_perp == testPerp\n",
        "    if test_passed:\n",
        "        print(\"Test getCorpusPerplexity(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getCorpusPerplexity(...) failed on the testing corpus...\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('unigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYJ0x5KpRrtU"
      },
      "source": [
        "Finally, you can train your model on the full WikiText corpus, and evaluate it on the held-out test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1XHIg0xrUIt",
        "outputId": "b27d10da-af7a-4c37-aaea-9af9d4d0ff1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -72.32620311491841 \tSentence: ['<s>', 'the', '<UNK>', 'sweep', 'at', 'bought', 'kind', 'Horus', 'find', 'by', '</s>']\n",
            "Log Probability: -231.66380599171765 \tSentence: ['<s>', 'heavy', 'amino', 'his', 'the', 'India', 'with', 'agreed', 'to', 'being', 'chosen', 'excessive', 'and', 'rights', 'should', ',', '(', 'standard', 'the', 'together', ',', 'and', 'retro', 'more', 'states', 'Matt', 'goals', 'the', 'music', '\"', 'plates', 'grout', '</s>']\n",
            "Log Probability: -90.95206888425095 \tSentence: ['<s>', 'eastward', 'she', 'locks', 'de', 'critical', 'somewhat', 'bound', 'martial', 'produced', '</s>']\n",
            "Log Probability: -145.94142401575797 \tSentence: ['<s>', 'monastery', 'damaged', 'jubilee', 'for', 'equipped', 'Graham', '\"', 'were', ',', '.', 'drive', 'the', 'funerary', 'phosphorus', 'productions', 'specifically', 'and', 'is', '</s>']\n",
            "Log Probability: -50.556990634399895 \tSentence: ['<s>', 'Composition', 'It', 'Portrait', 'of', 'sleeping', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 1101.9435880266938\n",
            "Testing Set: 912.1574385930448\n"
          ]
        }
      ],
      "source": [
        "def runModel(model_type):\n",
        "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
        "    # Read the corpora\n",
        "    if model_type == 'unigram':\n",
        "        model = UnigramModel(train_dataset)\n",
        "    elif model_type == 'bigram':\n",
        "        model = BigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-unigram':\n",
        "        model = SmoothedUnigramModel(train_dataset)\n",
        "    else:\n",
        "        model = SmoothedBigramModelAD(train_dataset)\n",
        "\n",
        "    print(\"--------- 5 sentences from your model ---------\")\n",
        "    model.printSentences(5)\n",
        "\n",
        "    print (\"\\n--------- Corpus Perplexities ---------\")\n",
        "    print (\"Training Set:\", model.getCorpusPerplexity(train_dataset))\n",
        "    print (\"Testing Set:\", model.getCorpusPerplexity(test_dataset))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    runModel('unigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bGyA8vOfvRj"
      },
      "source": [
        "## <font color='red'>TODO:</font> Smoothed Unigram Model [18 points]\n",
        "\n",
        "Here, you will implement each of the 4 functions described above for a <b>unigram</b> model with <b>Laplace (add-one) smoothing</b>. The probability distribution of a word is given by $P_L(w)$. This type of smoothing takes away some of the probability mass for observed events and assigns it to unseen events.\n",
        "\n",
        "In order to smooth your model, you will need the number of words in the corpus, $N$, and the number of word types, $S$. The distinction between these is meaningful: $N$ indicates the number of word instances, where $S$ refers to the size of our vocabulary. For example, the sentence <em>the cat saw the dog</em> has four word types (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>dog</em>), but five word tokens (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>the</em>, <em>dog</em>). The token <em>the</em> appears twice in the sentence, but they share the same type <em>the</em>.\n",
        "\n",
        "If $c(w)$ is the frequency of $w$ in the training data, you can compute $P_L(w)$ as follows:\n",
        "\n",
        "$$P_L(w)=\\frac{c(w)+1}{N+S}$$\n",
        "\n",
        "<font color='green'><b>Hints:</b></font>\n",
        "* <font color='green'>You may find it convenient to make your `SmoothedUnigramModel` inherit your `UnigramModel`, and then override the function(s) that need to be changed.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzX-UZJPfvRn"
      },
      "outputs": [],
      "source": [
        "class SmoothedUnigramModel(UnigramModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "\n",
        "        ### TODO ###\n",
        "        super().__init__(trainCorpus)\n",
        "        token_dict = super().token_dict()\n",
        "        self.p_w = self.token_prob(token_dict)\n",
        "\n",
        "\n",
        "    def token_prob(self, token_dict):\n",
        "\n",
        "        N = sum(token_dict.values())\n",
        "        S= len(token_dict.keys())\n",
        "        pl_w ={}\n",
        "\n",
        "        for key, value in token_dict.items():\n",
        "          pl_w[key] = (value+1)/(N+S)\n",
        "\n",
        "        return pl_w\n",
        "\n",
        "\n",
        "    def generateSentence(self):\n",
        "\n",
        "        ### TODO ###\n",
        "        return super().generateSentence()\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "\n",
        "        ### TODO ###\n",
        "        return super().getSentenceLogProbability(sentence)\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "\n",
        "        ### TODO ###\n",
        "        return super().getCorpusPerplexity(testCorpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBy9QaEdCUbc",
        "outputId": "fac36bc3-cca0-4c98-a2ca-ce984064ade3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -19.0405293515 \tYour log prob.: -19.0405293515 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -115.3479413049 \tYour log prob.: -115.3479413049 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -108.9114348746 \tYour log prob.: -108.9114348746 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -54.8190029616 \tYour log prob.: -54.8190029616 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -55.8122547346 \tYour log prob.: -55.8122547346 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 41.9994393615 \tYour train perp.: 41.9994393615 \t PASSED\n",
            "Correct test perp.: 39.9531928383 \tYour test perp.: 39.9531928383 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheck('smoothed-unigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxeKnWk1TTfF"
      },
      "source": [
        "Since the next sanity check trains your model on the *entire* training set, you will likely see that it is taking too long if you have inefficiences in your code. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "210t2j1GS1Xd",
        "outputId": "c8a6a7dc-1d45-4f26-d0fe-b1d50fb697dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -80.8423009715 \tYour log prob.: -80.8423009715 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -174.5131424172 \tYour log prob.: -174.5131424172 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -136.3181234818 \tYour log prob.: -136.3181234818 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -225.357454098 \tYour log prob.: -225.357454098 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -719.1543898871 \tYour log prob.: -719.1543898871 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -236.6682968913 \tYour log prob.: -236.6682968913 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -126.1965419509 \tYour log prob.: -126.1965419509 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -47.4369338195 \tYour log prob.: -47.4369338195 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -47.7692144935 \tYour log prob.: -47.7692144935 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -138.542462715 \tYour log prob.: -138.542462715 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: 881.6105352831 \tYour test perp.: 881.6105352831 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('smoothed-unigram')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6agWmpjdCWOt",
        "outputId": "b3420b4c-3eaf-4698-d631-0189e70b8f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -79.34895362782214 \tSentence: ['<s>', 'that', 'oldest', 'auxiliary', 'major', 'approximately', 'of', 'career', 'a', '<UNK>', 'the', '.', '(', '</s>']\n",
            "Log Probability: -106.40544725304825 \tSentence: ['<s>', 'throughout', 'at', 'from', 'but', 'the', ',', 'Goose', 'Importance', ',', 'week', 'wall', '.', ',', ',', 'criticized', 'in', 'of', '</s>']\n",
            "Log Probability: -101.00499624055497 \tSentence: ['<s>', 'It', 'Chicago', 'the', 'Corps', 'and', ',', 'source', 'song', '(', 'severe', 'resumed', 'communists', 'place', '</s>']\n",
            "Log Probability: -81.61720152578056 \tSentence: ['<s>', '.', 'final', 'of', 'the', 'Protoceratops', \"'s\", 'been', 'it', 'embark', 'England', 'under', 'the', '</s>']\n",
            "Log Probability: -82.28262606133984 \tSentence: ['<s>', 'in', 'of', 'and', 'mobile', 'fictional', 'limited', 'July', 'kg', 'race', '.', 'years', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 1103.0243317444865\n",
            "Testing Set: 914.4724502283168\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('smoothed-unigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGtcWVMGiEGw"
      },
      "source": [
        "## <font color='red'>TODO:</font> Bigram Model [18 points]\n",
        "\n",
        "Here, you will implement each of the 4 functions described above for an <b>unsmoothed bigram</b> model. The probability distribution of a word is given by $\\hat P(w'|w)$. Thus, the probability of $w_i$ is conditioned on $w_{i-1}$.\n",
        "\n",
        "<font color='green'><b>Hints:</b></font>\n",
        "* <font color='green'>You should use a dictionary of dictionaries to store your bigram counts. That is, the outer dictionary should map $w$ to another dictionary that maps $w'$ to the number of times $w'$ occurs after $w$.</font>\n",
        "* <font color='green'>Do <b>not</b> attempt to iterate over all possible bigrams in your voabulary: <em>only store bigrams that actually occur in your training data.</em> You will run into timeout or out-of-memory issues if you attempt to enumerate all bigrams.</font>\n",
        "* <font color='green'>Similarly, avoid nested loops over the training data.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ojk_q0YiEGx"
      },
      "outputs": [],
      "source": [
        "class BigramModel(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "\n",
        "        ### TODO ###\n",
        "        self.train_corpus = trainCorpus\n",
        "        self.tokens_unigrams = self.token_dict_unigrams()\n",
        "        self.tokens_bigrams = self.token_dict_bigrams()\n",
        "        self.p_ww = self.bigram_prob(self.tokens_unigrams, self.tokens_bigrams)\n",
        "\n",
        "\n",
        "    def token_dict_unigrams(self):\n",
        "      t_d_unigrams = {}\n",
        "\n",
        "      for sentence in self.train_corpus:\n",
        "        for word1 in sentence[:-1]:\n",
        "          super().unigram_count(t_d_unigrams, word1)\n",
        "\n",
        "      return t_d_unigrams\n",
        "\n",
        "    def token_dict_bigrams(self):\n",
        "      t_d_bigrams = {}\n",
        "\n",
        "      for sentence in self.train_corpus:\n",
        "        for word1, word2 in zip(sentence[:-1],sentence[1:]):\n",
        "          super().bigram_count(t_d_bigrams, word1, word2)\n",
        "\n",
        "      return t_d_bigrams\n",
        "\n",
        "    def set_bigram_prob(self, d, word1, word2, prob):\n",
        "      if word1 not in d.keys():\n",
        "        d[word1] = {word2: prob}\n",
        "      else:\n",
        "        if word2 not in d[word1].keys():\n",
        "                d[word1][word2] = prob\n",
        "\n",
        "\n",
        "\n",
        "    def bigram_prob(self, unigrams, bigrams):\n",
        "        p_ww = {}\n",
        "        for word1, word1dict in bigrams.items():\n",
        "            for word2, numbigrams in word1dict.items():\n",
        "              numunigrams = unigrams[word1]\n",
        "              p_w = numbigrams / numunigrams\n",
        "\n",
        "              self.set_bigram_prob(p_ww, word1, word2, p_w)\n",
        "        return p_ww\n",
        "\n",
        "\n",
        "\n",
        "    def generateSentence(self):\n",
        "\n",
        "        ### TODO ###\n",
        "        sentence = []\n",
        "        word1 = START\n",
        "        word2 = ''\n",
        "\n",
        "        while word2 != END:\n",
        "            possible_word2 = list(self.p_ww[word1].keys())\n",
        "            word2_prob = list(self.p_ww[word1].values())\n",
        "            word2 = random.choices(possible_word2, weights =word2_prob, k=1)[0]\n",
        "            sentence.append(word1)\n",
        "            word1 = word2\n",
        "\n",
        "        sentence.append(word2)\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "       ### TODO ###\n",
        "        probarray =[]\n",
        "\n",
        "        for word1, word2 in zip(sentence[:-1], sentence[1:]):\n",
        "            if word1 in self.p_ww.keys() and word2 in self.p_ww[word1].keys():\n",
        "              probarray.append(self.p_ww[word1][word2])\n",
        "            else:\n",
        "              return float('-inf')\n",
        "\n",
        "\n",
        "        logarray = super().naturalLog(probarray)\n",
        "\n",
        "        ### TODO ###\n",
        "        return sum(logarray)\n",
        "\n",
        "\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "\n",
        "        ### TODO ###\n",
        "        logprob = 0\n",
        "        N = 0\n",
        "\n",
        "        for sentence in testCorpus:\n",
        "            N += len(sentence) - 1\n",
        "            logprob += self.getSentenceLogProbability(sentence)\n",
        "\n",
        "        perplexity = math.exp(-1/N* logprob)\n",
        "        return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J8O8mCICZO5",
        "outputId": "ded05fd9-fe1b-4751-a557-19ef1bcb52dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -10.3450917073 \tYour log prob.: -10.3450917073 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -9.2464794186 \tYour log prob.: -9.2464794186 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 1.3861445461 \tYour train perp.: 1.3861445461 \t PASSED\n",
            "Correct test perp.: inf \tYour test perp.: inf \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheck('bigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMcAZYrpUE0W"
      },
      "source": [
        "Since the next sanity check trains your model on the *entire* training set, you will likely see that it is taking too long if you have inefficiences in your code. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIolz_8sS3B9",
        "outputId": "b66ad428-ec3c-470b-9c08-3b39d917974a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -32.1502020637 \tYour log prob.: -32.1502020637 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: inf \tYour test perp.: inf \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('bigram')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKop-qYKCZO5",
        "outputId": "e3eca36f-e082-4e55-cede-f54b827a88a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -97.04902884021536 \tSentence: ['<s>', 'The', 'Cogan', 'Viaduct', ')', 'at', 'the', 'American', 'distribution', 'includes', 'the', '18th', 'century', 'also', 'be', 'told', 'Newsweek', ',', 'Krasnyi', 'Kavkaz', '(', 'Harry', '<UNK>', '.', '</s>']\n",
            "Log Probability: -67.7940185916302 \tSentence: ['<s>', 'The', 'New', 'York', 'Times', 'critic', '<UNK>', 'Giles', ',', 'encouraging', 'him', 'that', 'Corbet', 'within', 'ten', 'games', '.', '</s>']\n",
            "Log Probability: -83.13046918462703 \tSentence: ['<s>', 'He', 'maintained', 'by', 'two', 'guns', \"'\", 'of', '\u00a3', '750', 'kg', '(', '75', 'the', 'reduced', 'the', 'United', 'States', '.', '</s>']\n",
            "Log Probability: -469.87026404396573 \tSentence: ['<s>', 'The', 'newspaper', 'from', 'the', '<UNK>', 'do', 'not', 'as', 'his', 'first', ',', '<UNK>', ')', 'in', '2008', 'Italian', 'champions', 'of', 'the', 'aircraft', ',', 'but', 'Bart', 'is', 'known', 'as', 'a', 'large', 'Goose', 'Pagoda', 'and', 'relief', 'from', 'the', 'perfect', '41', '\u2013', '0', '@.@', '3', ',', 'which', 'is', 'inconclusive', ':', '\"', '...', ']', 'whole', 'of', 'Hellblazer', ',', 'in', 'the', 'BBC', 'anthology', 'series', 'to', '<UNK>', '<UNK>', 'or', 'destroyed', ',', 'and', 'recording', 'time', 'are', 'interviewed', 'by', 'a', 'photo', ',', 'while', 'underground', 'members', 'of', 'risky', '@-@', 'Air', 'Force', '(', 'translucent', ')', 'for', 'his', 'Eve', 'ball', 'to', 'completing', 'a', 'much', 'more', 'common', 'starlings', 'were', 'displaced', ',', 'the', 'Order', 'of', 'Ceres', 'in', 'the', 'end', 'it', 'was', '78', 'million', '.', '</s>']\n",
            "Log Probability: -10.729337053631637 \tSentence: ['<s>', 'The', 'player', '.', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 76.92394608735728\n",
            "Testing Set: inf\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('bigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPBeyKUsfnnW"
      },
      "source": [
        "## <font color='red'>TODO:</font> Smoothed Bigram Model [18 points]\n",
        "\n",
        "Here, you will implement each of the 4 functions described above for a <b>bigram</b> model with <b>absolute discounting</b>. The probability distribution of a word is given by $P_{AD}(w\u2019|w)$.\n",
        "\n",
        "In order to smooth your model, you need to compute a discounting factor $D$. If $n_k$ is the number of bigrams $w_1w_2$ that appear exactly $k$ times, you can compute $D$ as:\n",
        "\n",
        "$$D=\\frac{n_1}{n_1+2n_2}$$\n",
        "\n",
        "For each word $w$, you then need to compute the number of bigram types $ww\u2019$ as follows:\n",
        "\n",
        "$$S(w)=|\\{w\u2019\\mid c(ww\u2019)>0\\}|$$\n",
        "\n",
        "where $c(ww\u2019)$ is the frequency of $ww\u2019$ in the training data. In other words, $S(w)$ is the number of unique words that follow $w$ at least once in the training data.\n",
        "\n",
        "Finally, you can compute $P_{AD}(w\u2019|w)$ as follows:\n",
        "\n",
        "$$P_{AD}(w\u2019|w)=\\frac{\\max \\big (c(ww\u2019)-D,0\\big )}{c(w)}+\\bigg (\\frac{D}{c(w)}\\cdot S(w) \\cdot P_L(w\u2019)\\bigg )$$\n",
        "\n",
        "where $c(w)$ is the frequency of $w$ in the training data and $P_L(w\u2019)$ is the Laplace-smoothed unigram probability of $w\u2019$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1klb00wtVtS"
      },
      "outputs": [],
      "source": [
        "class SmoothedBigramModelAD(BigramModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "\n",
        "        ### TODO ###\n",
        "        super().__init__(trainCorpus)\n",
        "        self.tokens_unigrams = self.completeUnigrams()\n",
        "        n1 = self.n_k(1)\n",
        "        n2 = self.n_k(2)\n",
        "        self.pl_w = SmoothedUnigramModel(trainCorpus).p_w\n",
        "        self.D = (n1)/(n1+2*n2)\n",
        "        self.p_ww = self.smooth_bigram_prob(self.tokens_bigrams)\n",
        "\n",
        "\n",
        "    def completeUnigrams(self):\n",
        "      t_d_unigrams = {}\n",
        "\n",
        "      for sentence in self.train_corpus:\n",
        "        for word1 in sentence:\n",
        "          super().unigram_count(t_d_unigrams, word1)\n",
        "\n",
        "      return t_d_unigrams\n",
        "\n",
        "    def P_AD(self, word1, word2):\n",
        "      c_ww = self.c_ww(word1, word2)\n",
        "      c_w = self.c_w(word1)\n",
        "      PL_W = self.pl_w[word2]\n",
        "\n",
        "      S_w = self.S_w(word1)\n",
        "\n",
        "      P_AD = (max(c_ww - self.D, 0) / c_w) + (self.D/c_w * S_w * PL_W)\n",
        "      return P_AD\n",
        "\n",
        "\n",
        "    def smooth_bigram_prob(self, bigrams):\n",
        "      p_ww ={}\n",
        "\n",
        "      for word1, word1dict in bigrams.items():\n",
        "          for word2, numbigrams in word1dict.items():\n",
        "            P_AD = self.P_AD(word1, word2)\n",
        "            self.set_bigram_prob(p_ww, word1, word2, P_AD)\n",
        "      return p_ww\n",
        "\n",
        "\n",
        "\n",
        "    def n_k(self, k):\n",
        "      n = 0\n",
        "      for word1, word1dict in self.tokens_bigrams.items():\n",
        "        for word2, bigramcount in word1dict.items():\n",
        "          if bigramcount ==k:\n",
        "            n+=1\n",
        "      return n\n",
        "\n",
        "    def S_w(self, word1):\n",
        "      word1dict = self.tokens_bigrams[word1].keys()\n",
        "      return len(word1dict)\n",
        "\n",
        "\n",
        "\n",
        "    def c_ww(self, word1, word2):\n",
        "      if word1 in self.tokens_bigrams.keys() and word2 in self.tokens_bigrams[word1].keys():\n",
        "        return self.tokens_bigrams[word1][word2]\n",
        "      else:\n",
        "        return 0\n",
        "\n",
        "    def c_w(self, word1):\n",
        "      if word1 in self.tokens_bigrams.keys():\n",
        "        return self.tokens_unigrams[word1]\n",
        "      else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def generateSentence(self):\n",
        "\n",
        "        ### TODO ###\n",
        "        #print(super().generateSentence())\n",
        "        return super().generateSentence()\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "\n",
        "        ### TODO ###\n",
        "        probarray =[]\n",
        "\n",
        "        for word1, word2 in zip(sentence[:-1], sentence[1:]):\n",
        "            if word1 in self.p_ww.keys() and word2 in self.p_ww[word1].keys():\n",
        "              probarray.append(self.p_ww[word1][word2])\n",
        "            else:\n",
        "              #print('word probability DNE')\n",
        "              P_AD = self.P_AD(word1, word2)\n",
        "              probarray.append(P_AD)\n",
        "\n",
        "\n",
        "        logarray = super(BigramModel, self).naturalLog(probarray)\n",
        "\n",
        "        ### TODO ###\n",
        "        return sum(logarray)\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "\n",
        "        ### TODO ###\n",
        "        return super().getCorpusPerplexity(testCorpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BLplBkmtfWG",
        "outputId": "ec886574-295f-4d2f-f8b2-60c111d64b47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -16.355820202 \tYour log prob.: -16.355820202 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -76.0026113319 \tYour log prob.: -76.0026113319 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -74.2346475108 \tYour log prob.: -74.2346475108 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -47.2885760372 \tYour log prob.: -47.2885760372 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -51.2730261907 \tYour log prob.: -51.2730261907 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 12.2307627397 \tYour train perp.: 12.2307627397 \t PASSED\n",
            "Correct test perp.: 26.7193157699 \tYour test perp.: 26.7193157699 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheck('smoothed-bigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dgNaW3HUGuU"
      },
      "source": [
        "Since the next sanity check trains your model on the *entire* training set, you will likely see that it is taking too long if you have inefficiences in your code. This cell is expected to run in fewer than <b>10 seconds</b>, so if it takes significantly longer than that, you should probably inspect your code for efficiency issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85sJoiXsS4Zv",
        "outputId": "f0142ea1-14e1-41c3-ddc3-353a49cb96b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -61.3754065648 \tYour log prob.: -61.3754065648 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -141.9754903887 \tYour log prob.: -141.9754903887 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -107.0849366076 \tYour log prob.: -107.0849366076 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -168.4944718788 \tYour log prob.: -168.4944718788 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -619.9409055374 \tYour log prob.: -619.9409055374 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -195.8159911677 \tYour log prob.: -195.8159911677 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -86.3762008156 \tYour log prob.: -86.3762008156 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -32.4764801981 \tYour log prob.: -32.4764801981 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -48.124714509 \tYour log prob.: -48.124714509 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -124.687107856 \tYour log prob.: -124.687107856 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: 261.4247123506 \tYour test perp.: 261.4247123506 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('smoothed-bigram')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GibKGwdXtiUQ",
        "outputId": "bf35bc7d-6ca3-490d-ce2d-250fba3a1495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -43.16946582037758 \tSentence: ['<s>', 'Likewise', ',', 'who', 'was', 'genius', ',', 'her', 'book', '.', '</s>']\n",
            "Log Probability: -293.52634271511556 \tSentence: ['<s>', 'That', 'was', 'finished', 'third', 'and', 'claimed', 'the', 'United', ',', 'the', 'Susquehanna', 'River', 'Avenue', ',', 'Staley', 'broke', 'and', 'star', 'is', 'seen', 'in', 'the', 'Fox', '(', 'before', 'its', 'radical', 'agenda', 'and', 'Kansas', 'City', '<UNK>', ',', 'although', 'most', 'part', 'of', '36', 'crosses', 'for', 'the', 'absence', 'of', 'tribbles', 'kept', 'in', 'the', 'day', ',', 'began', 'to', 'widen', 'the', '2015', ',', 'is', 'so', 'that', 'the', 'final', 'season', ',', 'and', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Log Probability: -40.87629082628197 \tSentence: ['<s>', 'On', '29', 'when', 'young', ',', 'drummer', 'Barrett', '.', '</s>']\n",
            "Log Probability: -312.97162084943824 \tSentence: ['<s>', 'The', 'final', 'storm', ',', 'if', 'they', 'may', 'be', '@-@', 'and', 'a', 'significant', 'challenges', 'that', '\"', 'To', 'promote', 'the', 'reintroduction', 'of', 'larger', 'than', 'the', 'Qedarites', 'were', 'targeted', 'at', '1', 'on', 'blood', 'more', 'famous', 'archery', 'teacher', 'helped', 'to', 'generate', 'high', 'scalers', 'received', 'word', 'spaces', 'leads', 'to', 'rebuild', 'her', 'into', 'the', 'eventual', '2', '%', 'of', 'marriage', 'of', '9', 'December', '2012', 'to', 'write', 'with', 'his', 'first', 'time', 'in', 'Boca', 'Raton', '.', '</s>']\n",
            "Log Probability: -24.168752230511867 \tSentence: ['<s>', 'L\u00fctzow', 'carried', 'out', 'of', 'Ireland', '.', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 98.55812920532598\n",
            "Testing Set: 272.57201979320354\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('smoothed-bigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtMMWXC0Emwq"
      },
      "source": [
        "## Food for Thought\n",
        "We provide you some questions to think about. <b>You do not need to answer these questions, but we encourage you to give them some thought.</b>\n",
        "<ol>\n",
        "<li>When generating sentences with the unigram model, what controls the length of the generated sentences? How does this differ from the sentences produced by the bigram models?\n",
        "<li>Consider the probability of the generated sentences according to your models. Do your models assign drastically different probabilities to the different sets of sentences? Why do you think that is?\n",
        "<li>Look back at the sentences generated using your models. In your opinion, which model produces better / more realistic sentences?\n",
        "<li>For each of the four models, which test corpus has the highest perplexity? Why?\n",
        "<li> Why do you think it might be a bad idea to use Laplace (add-one) smoothing for a bigram model? How does the absolute discounting method help?\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa3qZXUBePFW"
      },
      "source": [
        "# What to Submit\n",
        "\n",
        "To submit the assignment, download this notebook as a <TT>.py</TT> file. You can do this by going to <TT>File > Download > Download .py</TT>. Then submit it to the autograder in Gradescope. <b>Do not try to submit it as a <TT>.ipynb</TT> file!</b>\n",
        "\n",
        "**Reminder: Make sure that you access the Gradescope submission page via the corresponding assignment in Coursera!** Failure to do so may result in the inability to push your grades to Coursera. (The same goes for quizzes!)\n",
        "\n",
        "Note that it should take <b>less than 10 minutes</b> to see your score after you have submitted to Gradescope. If your submission runs significantly longer than that, you probably have inefficiency issues in your code!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}